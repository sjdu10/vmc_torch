{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import ast\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.fermion_utils import *\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "class fTNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ftn):\n",
    "        super().__init__()\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter\n",
    "        self.torch_params = {\n",
    "            tid: nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        }\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "    def product_bra_state(self, config, peps, symmetry='Z2'):\n",
    "        \"\"\"Spinless fermion product bra state.\"\"\"\n",
    "        product_tn = qtn.TensorNetwork()\n",
    "        backend = peps.tensors[0].data.backend\n",
    "        iterable_oddpos = iter(range(2*peps.nsites+1))\n",
    "        for n, site in zip(config, peps.sites):\n",
    "            p_ind = peps.site_ind_id.format(*site)\n",
    "            p_tag = peps.site_tag_id.format(*site)\n",
    "            tid = peps.sites.index(site)\n",
    "            nsites = peps.nsites\n",
    "            # use autoray to ensure the correct backend is used\n",
    "            with ar.backend_like(backend):\n",
    "                if symmetry == 'Z2':\n",
    "                    data = [sr.Z2FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2', charge=0, oddpos=2*tid+1), # It doesn't matter if oddpos is None for even parity tensor.\n",
    "                            sr.Z2FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2',charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "                elif symmetry == 'U1':\n",
    "                    data = [sr.U1FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=0, oddpos=2*tid+1),\n",
    "                            sr.U1FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "            tsr_data = data[int(n)] # BUG: does not fit in jax compilation, a concrete value is needed for traced arrays\n",
    "            tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, 'bra'))\n",
    "            product_tn |= tsr\n",
    "        return product_tn\n",
    "\n",
    "    def get_amp(self, peps, config, inplace=False, symmetry='Z2', conj=True):\n",
    "        \"\"\"Get the amplitude of a configuration in a PEPS.\"\"\"\n",
    "        if not inplace:\n",
    "            peps = peps.copy()\n",
    "        if conj:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry).conj()\n",
    "        else:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry)\n",
    "        for site in peps.sites:\n",
    "            site_tag = peps.site_tag_id.format(*site)\n",
    "            amp.contract_(tags=site_tag)\n",
    "\n",
    "        amp.view_as_(\n",
    "            qtn.PEPS,\n",
    "            site_ind_id=\"k{},{}\",\n",
    "            site_tag_id=\"I{},{}\",\n",
    "            x_tag_id=\"X{}\",\n",
    "            y_tag_id=\"Y{}\",\n",
    "            Lx=peps.Lx,\n",
    "            Ly=peps.Ly,\n",
    "        )\n",
    "        return amp\n",
    "        \n",
    "    def parameters(self):\n",
    "        # Manually yield all parameters from the nested structure\n",
    "        for tid_dict in self.torch_params.values():\n",
    "            for param in tid_dict.values():\n",
    "                yield param\n",
    "    \n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    def from_vec_to_params(self, vec, quimb_format=False):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {}\n",
    "        idx = 0\n",
    "        for tid, blk_array in self.torch_params.items():\n",
    "            params[tid] = {}\n",
    "            for sector, data in blk_array.items():\n",
    "                shape = data.shape\n",
    "                size = data.numel()\n",
    "                if quimb_format:\n",
    "                    params[tid][ast.literal_eval(sector)] = vec[idx:idx+size].view(shape)\n",
    "                else:\n",
    "                    params[tid][sector] = vec[idx:idx+size].view(shape)\n",
    "                idx += size\n",
    "        return params\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        if isinstance(new_params, torch.Tensor):\n",
    "            new_params = self.from_vec_to_params(new_params)\n",
    "        # Update the parameters manually\n",
    "        with torch.no_grad():\n",
    "            for tid, blk_array in new_params.items():\n",
    "                for sector, data in blk_array.items():\n",
    "                    self.torch_params[tid][sector].data = data\n",
    "\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            tid: {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "       # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            amp = self.get_amp(psi, x_i, symmetry=self.symmetry, conj=True)\n",
    "            batch_amps.append(amp.contract())\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)\n",
    "\n",
    "\n",
    "class fTN_NNiso_Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, ftn, max_bond, nn_hidden_dim=64, nn_eta=1e-3):\n",
    "        super().__init__()\n",
    "        self.max_bond = max_bond\n",
    "        self.nn_eta = nn_eta\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "        \n",
    "        self.parity_config = [array.parity for array in ftn.arrays]\n",
    "        self.N_fermion = sum(self.parity_config)\n",
    "        dummy_config = torch.zeros(ftn.nsites)\n",
    "        dummy_config[:self.N_fermion] = 1\n",
    "        dummy_amp = self.get_amp(ftn, dummy_config, inplace=False)\n",
    "        dummy_amp_w_proj = insert_proj_peps(dummy_amp, max_bond=max_bond, yrange=[0, ftn.Ly-2])\n",
    "        dummy_amp_tn, dummy_proj_tn = dummy_amp_w_proj.partition(tags='proj')\n",
    "        dummy_proj_params, dummy_proj_skeleton = qtn.pack(dummy_proj_tn)\n",
    "        dummy_proj_params_vec = flatten_proj_params(dummy_proj_params)\n",
    "        self.proj_params_vec_len = len(dummy_proj_params_vec)\n",
    "\n",
    "        # Define an MLP layer (or any other neural network layers)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(ftn.nsites, nn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nn_hidden_dim, self.proj_params_vec_len)\n",
    "        )\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "        assert self.symmetry == 'Z2', \"Only Z2 symmetry fPEPS is supported for NN insertion now.\"\n",
    "        if self.symmetry == 'Z2':\n",
    "            assert self.N_fermion %2 == sum(self.parity_config) % 2, \"The number of fermions must match the parity of the Z2-TNS.\"\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS (proj inserted)':{'D': ftn.max_bond(), 'chi': self.max_bond, 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry, 'proj_yrange': [0, ftn.Ly-2]},\n",
    "            '2LayerMLP':{'hidden_dim': nn_hidden_dim, 'nn_eta': nn_eta, 'activation': 'ReLU'}\n",
    "        }\n",
    "\n",
    "    def product_bra_state(self, config, peps, symmetry='Z2'):\n",
    "        \"\"\"Spinless fermion product bra state.\"\"\"\n",
    "        product_tn = qtn.TensorNetwork()\n",
    "        backend = peps.tensors[0].data.backend\n",
    "        iterable_oddpos = iter(range(2*peps.nsites+1))\n",
    "        for n, site in zip(config, peps.sites):\n",
    "            p_ind = peps.site_ind_id.format(*site)\n",
    "            p_tag = peps.site_tag_id.format(*site)\n",
    "            tid = peps.sites.index(site)\n",
    "            nsites = peps.nsites\n",
    "            # use autoray to ensure the correct backend is used\n",
    "            with ar.backend_like(backend):\n",
    "                if symmetry == 'Z2':\n",
    "                    data = [sr.Z2FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2', charge=0, oddpos=2*tid+1), # It doesn't matter if oddpos is None for even parity tensor.\n",
    "                            sr.Z2FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2',charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "                elif symmetry == 'U1':\n",
    "                    data = [sr.U1FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=0, oddpos=2*tid+1),\n",
    "                            sr.U1FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "            tsr_data = data[int(n)] # BUG: does not fit in jax compilation, a concrete value is needed for traced arrays\n",
    "            tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, 'bra'))\n",
    "            product_tn |= tsr\n",
    "        return product_tn\n",
    "\n",
    "    def get_amp(self, peps, config, inplace=False, symmetry='Z2', conj=True):\n",
    "        \"\"\"Get the amplitude of a configuration in a PEPS.\"\"\"\n",
    "        if not inplace:\n",
    "            peps = peps.copy()\n",
    "        if conj:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry).conj()\n",
    "        else:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry)\n",
    "        for site in peps.sites:\n",
    "            site_tag = peps.site_tag_id.format(*site)\n",
    "            amp.contract_(tags=site_tag)\n",
    "\n",
    "        amp.view_as_(\n",
    "            qtn.PEPS,\n",
    "            site_ind_id=\"k{},{}\",\n",
    "            site_tag_id=\"I{},{}\",\n",
    "            x_tag_id=\"X{}\",\n",
    "            y_tag_id=\"Y{}\",\n",
    "            Lx=peps.Lx,\n",
    "            Ly=peps.Ly,\n",
    "        )\n",
    "        return amp\n",
    "        \n",
    "    \n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        pointer = 0\n",
    "        for param, shape in zip(self.parameters(), self.param_shapes):\n",
    "            num_param = param.numel()\n",
    "            new_param_values = new_params[pointer:pointer+num_param].view(shape)\n",
    "            with torch.no_grad():\n",
    "                param.copy_(new_param_values)\n",
    "            pointer += num_param\n",
    "\n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            amp = self.get_amp(psi, x_i, symmetry=self.symmetry, conj=True)\n",
    "\n",
    "            # Insert projectors\n",
    "            amp_w_proj = insert_proj_peps(amp, max_bond=self.max_bond, yrange=[0, psi.Ly-2])\n",
    "            amp_tn, proj_tn = amp_w_proj.partition(tags='proj')\n",
    "            proj_params, proj_skeleton = qtn.pack(proj_tn)\n",
    "            proj_params_vec = flatten_proj_params(proj_params)\n",
    "\n",
    "            # Check x_i type\n",
    "            if not type(x_i) == torch.Tensor:\n",
    "                x_i = torch.tensor(x_i, dtype=torch.float32)\n",
    "            # Add NN output\n",
    "            proj_params_vec += self.nn_eta*self.mlp(x_i)\n",
    "            # Reconstruct the proj parameters\n",
    "            new_proj_params = reconstruct_proj_params(proj_params_vec, proj_params)\n",
    "            # Load the new parameters\n",
    "            new_proj_tn = qtn.unpack(new_proj_params, proj_skeleton)\n",
    "            new_amp_w_proj = amp_tn | new_proj_tn\n",
    "\n",
    "            # contract column by column\n",
    "            \n",
    "            # batch_amps.append(torch.tensor(new_amp_w_proj.contract(), dtype=torch.float32, requires_grad=True))\n",
    "            batch_amps.append(new_amp_w_proj.contract())\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n=50, tau=0.3000, energy~-0.402307: 100%|##########| 50/50 [00:10<00:00,  4.77it/s]\n"
     ]
    }
   ],
   "source": [
    "Lx = 6\n",
    "Ly = 6\n",
    "D = 8\n",
    "symmetry = 'Z2'\n",
    "N_f = int(4*4/2)-2\n",
    "# Create a random PEPS\n",
    "peps, parity_config = generate_random_fpeps(Lx, Ly, D, seed=2, symmetry=symmetry, Nf=N_f)\n",
    "\n",
    "# Create a random configuration\n",
    "random_conf = np.zeros(Lx*Ly)\n",
    "random_conf[:N_f] = 1\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(random_conf)\n",
    "\n",
    "t = 1.0\n",
    "V = 4.0\n",
    "mu = 0.0\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "site_info = sr.utils.parse_edges_to_site_info(\n",
    "    edges,\n",
    "    D,\n",
    "    phys_dim=2,\n",
    "    site_ind_id=\"k{},{}\",\n",
    "    site_tag_id=\"I{},{}\",\n",
    ")\n",
    "terms = {\n",
    "    (sitea, siteb): sr.fermi_hubbard_spinless_local_array(\n",
    "        t=t, V=V, mu=mu,\n",
    "        symmetry=symmetry,\n",
    "        coordinations=(\n",
    "            site_info[sitea]['coordination'],\n",
    "            site_info[siteb]['coordination'],\n",
    "        ),\n",
    "    ).fuse((0, 1), (2, 3))\n",
    "    for (sitea, siteb) in peps.gen_bond_coos()\n",
    "}\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "su = qtn.SimpleUpdateGen(peps, ham, compute_energy_per_site=True,D=D, compute_energy_opts={\"max_distance\":1}, gate_opts={'cutoff':1e-10})\n",
    "su.evolve(50, 0.3)\n",
    "# su.evolve(50, 0.1)\n",
    "peps = su.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22117/2079912326.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float32, requires_grad=True))\n",
      "\n",
      "pyinstrument ........................................\n",
      ".\n",
      ".  Block at /tmp/ipykernel_22117/2079912326.py:5\n",
      ".\n",
      ".  0.066 <module>  ../../../../../tmp/ipykernel_22117/2079912326.py:5\n",
      ".  └─ 0.066 PEPS.contract  quimb/tensor/tensor_core.py:8396\n",
      ".        [6 frames hidden]  functools, quimb, cotengra\n",
      ".           0.066 wrapper  functools.py:883\n",
      ".           └─ 0.065 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".              ├─ 0.052 tensordot_abelian  symmray/abelian_core.py:1996\n",
      ".              │  └─ 0.052 _tensordot_via_fused  symmray/abelian_core.py:1941\n",
      ".              │     ├─ 0.031 Z2FermionicArray.fuse  symmray/abelian_core.py:1418\n",
      ".              │     │  ├─ 0.010 <dictcomp>  symmray/abelian_core.py:1524\n",
      ".              │     │  │  └─ 0.010 _recurse_concat  symmray/abelian_core.py:1484\n",
      ".              │     │  │     ├─ 0.005 translated_function  autoray/autoray.py:1310\n",
      ".              │     │  │     │     [1 frames hidden]  <built-in>\n",
      ".              │     │  │     └─ 0.005 <genexpr>  symmray/abelian_core.py:1517\n",
      ".              │     │  │        └─ 0.005 _recurse_concat  symmray/abelian_core.py:1484\n",
      ".              │     │  │           └─ 0.005 translated_function  autoray/autoray.py:1310\n",
      ".              │     │  │                 [2 frames hidden]  <built-in>, autoray\n",
      ".              │     │  ├─ 0.010 _VariableFunctionsClass.reshape  <built-in>\n",
      ".              │     │  ├─ 0.006 torch_transpose  autoray/autoray.py:1974\n",
      ".              │     │  │     [1 frames hidden]  <built-in>\n",
      ".              │     │  ├─ 0.004 [self]  symmray/abelian_core.py\n",
      ".              │     │  └─ 0.001 hasattr  <built-in>\n",
      ".              │     ├─ 0.013 Z2FermionicArray.unfuse  symmray/abelian_core.py:1536\n",
      ".              │     │  ├─ 0.006 [self]  symmray/abelian_core.py\n",
      ".              │     │  ├─ 0.004 _VariableFunctionsClass.reshape  <built-in>\n",
      ".              │     │  ├─ 0.002 <genexpr>  symmray/abelian_core.py:1582\n",
      ".              │     │  │  ├─ 0.001 [self]  symmray/abelian_core.py\n",
      ".              │     │  │  └─ 0.001 BlockIndex.size_of  symmray/abelian_core.py:120\n",
      ".              │     │  └─ 0.001 <dictcomp>  symmray/abelian_core.py:1559\n",
      ".              │     │     └─ 0.001 accum_for_split  symmray/abelian_core.py:298\n",
      ".              │     ├─ 0.005 _tensordot_blockwise  symmray/abelian_core.py:1831\n",
      ".              │     │  └─ 0.005 <genexpr>  symmray/abelian_core.py:1885\n",
      ".              │     │     └─ 0.005 numpy_like  autoray/autoray.py:2034\n",
      ".              │     │           [2 frames hidden]  torch, <built-in>\n",
      ".              │     └─ 0.002 drop_misaligned_sectors  symmray/abelian_core.py:1899\n",
      ".              │        ├─ 0.001 <dictcomp>  symmray/abelian_core.py:1919\n",
      ".              │        └─ 0.001 set.intersection  <built-in>\n",
      ".              ├─ 0.008 Z2FermionicArray.transpose  symmray/fermionic_core.py:232\n",
      ".              │  ├─ 0.003 [self]  symmray/fermionic_core.py\n",
      ".              │  ├─ 0.003 Z2FermionicArray.transpose  symmray/abelian_core.py:1270\n",
      ".              │  │  └─ 0.003 <dictcomp>  symmray/abelian_core.py:1281\n",
      ".              │  │     ├─ 0.002 torch_transpose  autoray/autoray.py:1974\n",
      ".              │  │     │     [1 frames hidden]  <built-in>\n",
      ".              │  │     └─ 0.001 permuted  symmray/abelian_core.py:270\n",
      ".              │  └─ 0.002 <genexpr>  symmray/fermionic_core.py:262\n",
      ".              ├─ 0.004 Z2FermionicArray.phase_sync  symmray/fermionic_core.py:395\n",
      ".              └─ 0.001 [self]  symmray/fermionic_core.py\n",
      ".  \n",
      ".....................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float32, requires_grad=True))\n",
    "# Get the amplitude of the configuration\n",
    "amp = peps.get_amp(random_conf, conj=True)\n",
    "import pyinstrument\n",
    "with pyinstrument.profile():\n",
    "    amp.contract()\n",
    "    # amp.contract_boundary_from_ymin(max_bond=8, yrange=(0, amp.Ly-2), cutoff=0.0).contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.023293644344654e-08 -5.956890913005554e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-5.990340537268858e-08"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the amplitude of the configuration\n",
    "amp = peps.get_amp(random_conf, conj=True)\n",
    "amp.contract()\n",
    "yrange = [0, Lx-2]\n",
    "chi = 8\n",
    "amp_w_proj = insert_proj_peps(amp, max_bond=chi, yrange=yrange)\n",
    "\n",
    "# Phase check\n",
    "print(amp_w_proj.contract(), amp.contract())\n",
    "\n",
    "amp_tn, proj_tn = amp_w_proj.partition(tags='proj')\n",
    "proj_params, proj_skeleton = qtn.pack(proj_tn)\n",
    "\n",
    "# Flatten the proj parameters\n",
    "proj_params_vec = flatten_proj_params(proj_params)\n",
    "\n",
    "# Perturbation\n",
    "perturbation = 1e-4\n",
    "perturbed_params = proj_params_vec + perturbation*np.random.randn(len(proj_params_vec))\n",
    "\n",
    "# Reconstruct the proj parameters\n",
    "perturbed_proj_params = reconstruct_proj_params(perturbed_params, proj_params)\n",
    "\n",
    "# Load the perturbed parameters\n",
    "new_proj_tn = qtn.unpack(perturbed_proj_params, proj_skeleton)\n",
    "new_amp_w_proj = amp_tn | new_proj_tn\n",
    "new_amp_w_proj.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.]])\n",
      "[tensor(-1.9720e-08, grad_fn=<ViewBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "peps = su.state.copy()\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float32, requires_grad=True))\n",
    "np.random.shuffle(random_conf)\n",
    "test_model = fTN_NNiso_Model(peps, max_bond=chi, nn_hidden_dim=8, nn_eta=0.0)\n",
    "# # test_model = fTNModel(peps)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print(torch.tensor([random_conf], dtype=torch.float32))\n",
    "loss = test_model.amplitude(torch.tensor([random_conf], dtype=torch.float32))\n",
    "# amp = peps.get_amp(random_conf, conj=True)\n",
    "# amp_w_proj = insert_proj_peps(amp, max_bond=chi, yrange=[0, peps.Ly-2])\n",
    "# loss = amp_w_proj.contract()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If symmetry is U1, the shape of each block is different, and the numbers of parameters in the projectors for different configurations are also different.\n",
    "\n",
    "Must use NN sturcture that can have dynamic output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0,\n",
       "  0,\n",
       "  0): tensor([[[ 0.4578,  0.2153],\n",
       "          [-0.0884,  0.2482]],\n",
       " \n",
       "         [[-0.1604,  0.4821],\n",
       "          [-0.1731, -0.6656]]]),\n",
       " (1,\n",
       "  1,\n",
       "  0): tensor([[[ -0.1208, -11.7845],\n",
       "          [  0.0700,   7.7760]],\n",
       " \n",
       "         [[  0.0388,   4.7961],\n",
       "          [ -0.0251,  -3.2224]]]),\n",
       " (0,\n",
       "  1,\n",
       "  1): tensor([[[  2.3070, -11.2277],\n",
       "          [ -1.4781,   8.7886]],\n",
       " \n",
       "         [[ -0.5556,  25.5320],\n",
       "          [  0.3850, -17.3789]]]),\n",
       " (1,\n",
       "  0,\n",
       "  1): tensor([[[-6.5736e-05, -2.6272e-02],\n",
       "          [ 1.0638e-01, -3.7179e-01]],\n",
       " \n",
       "         [[ 8.8650e-04,  7.9483e-02],\n",
       "          [-3.2298e-02,  3.0391e-01]]])}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(random_conf)\n",
    "ampx = peps.get_amp(random_conf, conj=True)\n",
    "ampx_w_proj = insert_proj_peps(ampx, max_bond=chi, yrange=[0, peps.Ly-2])\n",
    "ampx_tn, projx_tn = ampx_w_proj.partition(tags='proj')\n",
    "projx_params, projx_skeleton = qtn.pack(projx_tn)\n",
    "projx_params_vec = flatten_proj_params(projx_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symmray_nqs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
