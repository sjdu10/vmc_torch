{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2120b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -56626.0253, -126844.2878,  -27390.6366,   12296.2005,   43295.4322,\n",
       "         -29739.2064,  -13452.7840,  -24764.9240,  -52658.3799,  -45615.5187],\n",
       "       dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vmap_utils import Transformer_fPEPS_Model_batchedAttn, Transformer_fPEPS_Model, random_initial_config, fPEPS_Model, compute_grads\n",
    "\n",
    "import quimb.tensor as qtn\n",
    "\n",
    "import symmray as sr\n",
    "\n",
    "import torch\n",
    "\n",
    "# System parameters\n",
    "Lx = 4\n",
    "Ly = 4\n",
    "nsites = Lx * Ly\n",
    "D = 4\n",
    "seed = 42\n",
    "flat = True\n",
    "\n",
    "# random fPEPS\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 1),  # linear index 1 -> charge 1, offset 1\n",
    "        (1, 0),  # linear index 2 -> charge 1, offset 0\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],  # -> (0, 3), (2, 1)\n",
    "    # put an odd number of odd sites in, for testing\n",
    "    # site_charge=lambda site: int(site in [(0, 0), (0, 1), (1, 0)]),\n",
    "    subsizes=\"equal\",\n",
    "    flat=flat,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Prepare initial samples\n",
    "# batchsize per rank\n",
    "B = 10\n",
    "fxs = []\n",
    "for _ in range(B):\n",
    "    fxs.append(random_initial_config(nsites, nsites))\n",
    "fxs = torch.stack(fxs, dim=0)\n",
    "model = Transformer_fPEPS_Model_batchedAttn(\n",
    "    tn=peps,\n",
    "    max_bond=-1,\n",
    "    nn_eta=1,\n",
    "    nn_hidden_dim=16,\n",
    "    embed_dim=16,\n",
    "    attn_heads=4,\n",
    "    dtype=torch.float64,\n",
    ")\n",
    "# model = fPEPS_Model(\n",
    "#     tn=peps,\n",
    "#     max_bond=D,\n",
    "#     dtype=torch.float64,\n",
    "# )\n",
    "nparams = sum(p.numel() for p in model.parameters())\n",
    "model(fxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2efaff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vmc_torch.experiment.vmap.vmap_utils import flatten_params\n",
    "from torch.utils._pytree import tree_map, tree_flatten\n",
    "\n",
    "def compute_grads_decoupled(fxs, fpeps_model, batch_size=None):\n",
    "    \"\"\"\n",
    "    解耦合梯度计算 (支持 Chunking 以节省内存)：\n",
    "    Step 1: 前向计算 NN, 拿到数值上的 delta_P (backflow correction)\n",
    "    Step 2: 计算 TN 的梯度 (分批 vmap, 拿到 sensitivity vector)\n",
    "    Step 3: 将 sensitivity vector 回传给 NN (VJP, 逐样本循环)\n",
    "    \"\"\"\n",
    "    B = fxs.shape[0]\n",
    "    dtype = fpeps_model.dtype\n",
    "    \n",
    "    # 确定 chunk size，如果未指定则一次性算完\n",
    "    B_grad = batch_size if batch_size is not None else B\n",
    "    \n",
    "    # === 准备参数 ===\n",
    "    ftn_params = list(fpeps_model.ftn_params)\n",
    "    nn_params = list(fpeps_model.nn_backflow.parameters())\n",
    "    nn_params_dict = dict(zip(fpeps_model.nn_param_names, nn_params))\n",
    "\n",
    "    # =================================================================\n",
    "    # Step 1: NN Forward (Native Batch)\n",
    "    # 目的：获取 delta_P 的数值，作为 TN 的输入。\n",
    "    # =================================================================\n",
    "    # 这一步通常内存占用不大（相比 TN），所以我们可以一次性算完。\n",
    "    # 如果显存非常吃紧，也可以把这一步放入下面的循环中，但逻辑会稍微复杂一点。\n",
    "    with torch.no_grad():\n",
    "        batch_delta_p = torch.func.functional_call(\n",
    "            fpeps_model.nn_backflow, nn_params_dict, fxs.to(dtype)\n",
    "        )\n",
    "    # batch_delta_p shape: (B, ftn_params_length)\n",
    "\n",
    "    # =================================================================\n",
    "    # Step 2: TN Backward (Chunked vmap over grad)\n",
    "    # 目的：计算 psi 对 delta_P 的敏感度 (Sensitivity)\n",
    "    # =================================================================\n",
    "    \n",
    "    # 定义纯 TN 收缩函数\n",
    "    def tn_only_func(x_i, ftn_p_list, delta_p_i):\n",
    "        amp = fpeps_model.tn_contraction(x_i, ftn_p_list, delta_p_i)\n",
    "        return amp, amp # (Target, Aux)\n",
    "\n",
    "    # 定义 vmap 函数\n",
    "    tn_grad_vmap_func = torch.vmap(\n",
    "        torch.func.grad(tn_only_func, argnums=(1, 2), has_aux=True), \n",
    "        in_dims=(0, None, 0)\n",
    "    )\n",
    "\n",
    "    # --- 开始 Chunking 循环 ---\n",
    "    g_ftn_chunks = []\n",
    "    g_sensitivity_chunks = []\n",
    "    amps_chunks = []\n",
    "\n",
    "    for b_start in range(0, B, B_grad):\n",
    "        b_end = min(b_start + B_grad, B)\n",
    "        \n",
    "        # 1. 切片 (Slicing)\n",
    "        fxs_chunk = fxs[b_start:b_end]\n",
    "        delta_p_chunk = batch_delta_p[b_start:b_end]\n",
    "        \n",
    "        # 2. 计算当前 chunk 的梯度\n",
    "        # (g_ftn_chunk, g_sensitivity_chunk), amps_chunk\n",
    "        (g_ftn_c, g_sens_c), amps_c = tn_grad_vmap_func(fxs_chunk, ftn_params, delta_p_chunk)\n",
    "        \n",
    "        # 3. 立即 Detach Amps 以释放 TN 计算图 (关键!)\n",
    "        if amps_c.requires_grad:\n",
    "            amps_c = amps_c.detach()\n",
    "            \n",
    "        # 4. 存储结果\n",
    "        g_ftn_chunks.append(g_ftn_c)        # 这是一个 list/tuple of tensors\n",
    "        g_sensitivity_chunks.append(g_sens_c)\n",
    "        amps_chunks.append(amps_c)\n",
    "        \n",
    "        # 显式删除临时变量，辅助 GC\n",
    "        del g_ftn_c, g_sens_c, amps_c\n",
    "\n",
    "    # --- 拼接结果 (Aggregation) ---\n",
    "    \n",
    "    # 1. 拼接 g_sensitivity (B, Out)\n",
    "    g_sensitivity = torch.cat(g_sensitivity_chunks, dim=0)\n",
    "    \n",
    "    # 2. 拼接 amps (B, 1)\n",
    "    amps = torch.cat(amps_chunks, dim=0)\n",
    "    if amps.dim() == 1:\n",
    "        amps = amps.unsqueeze(-1)\n",
    "\n",
    "    # 3. 拼接 g_ftn (PyTree 结构)\n",
    "    # g_ftn_chunks 是一个列表，里面每一个元素都是一个 tuple(tensor_param_1, tensor_param_2, ...)\n",
    "    # 我们需要把它们按照 parameter 的位置，沿着 dim=0 拼起来\n",
    "    # 使用 tree_map 可以优雅地处理这个结构\n",
    "    g_ftn = tree_map(lambda *leaves: torch.cat(leaves, dim=0), *g_ftn_chunks)\n",
    "\n",
    "    # 此时，TN 部分的大内存已经释放完毕\n",
    "\n",
    "    # =================================================================\n",
    "    # Step 3: NN Backward (Sequential Loop)\n",
    "    # 目的：利用 g_sensitivity 计算 NN 参数的梯度\n",
    "    # =================================================================\n",
    "    # 这一步本身就是逐样本的，天然节省内存，直接复用你之前的逻辑即可\n",
    "    \n",
    "    g_nn_params_list = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        x_i = fxs[i].unsqueeze(0) \n",
    "        g_sens_i = g_sensitivity[i].unsqueeze(0) \n",
    "        \n",
    "        fpeps_model.nn_backflow.zero_grad()\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            out_i = torch.func.functional_call(\n",
    "                fpeps_model.nn_backflow, \n",
    "                nn_params_dict, \n",
    "                x_i.to(dtype)\n",
    "            )\n",
    "            target = torch.sum(out_i * g_sens_i.detach())\n",
    "            grads_i = torch.autograd.grad(target, nn_params, retain_graph=False)\n",
    "            \n",
    "        flat_g = flatten_params(grads_i)\n",
    "        g_nn_params_list.append(flat_g)\n",
    "        \n",
    "    g_nn_params_vec = torch.stack(g_nn_params_list)\n",
    "\n",
    "    # =================================================================\n",
    "    # Step 4: 拼装最终结果\n",
    "    # =================================================================\n",
    "    \n",
    "    # Flatten g_ftn\n",
    "    leaves, _ = tree_flatten(g_ftn)\n",
    "    flat_g_ftn_list = [leaf.flatten(start_dim=1) for leaf in leaves]\n",
    "    g_ftn_params_vec = torch.cat(flat_g_ftn_list, dim=1)\n",
    "\n",
    "    g_params_vec = torch.cat([g_ftn_params_vec, g_nn_params_vec], dim=1) # (B, Np_total)\n",
    "    \n",
    "    return g_params_vec, amps\n",
    "\n",
    "g_params_vec, amps = compute_grads_decoupled(fxs, model, batch_size=6)\n",
    "g_params_vec_benchmark, amps_benchmark = compute_grads(fxs, model, vectorize=True)\n",
    "g_params_vec.shape, g_params_vec_benchmark.shape\n",
    "torch.allclose(g_params_vec, g_params_vec_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e703b2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (1): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (2): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (3): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (4): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (5): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (6): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (7): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quimb as qu\n",
    "tn = peps.copy()\n",
    "params, skeleton = qtn.pack(tn)\n",
    "# for torch, further flatten pytree into a single list\n",
    "ftn_params_flat, ftn_params_pytree = qu.utils.tree_flatten(\n",
    "    params, get_ref=True\n",
    ")\n",
    "ftn_params = torch.nn.ParameterList([\n",
    "    torch.as_tensor(x, dtype=torch.float64) for x in ftn_params_flat\n",
    "])\n",
    "ftn_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
