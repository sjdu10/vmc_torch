{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2120b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-10.0554], dtype=torch.float64, grad_fn=<MulBackward0>), 4816)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vmap_utils import Transformer_fPEPS_Model_batchedAttn, Transformer_fPEPS_Model\n",
    "\n",
    "import quimb.tensor as qtn\n",
    "\n",
    "import symmray as sr\n",
    "\n",
    "import torch\n",
    "\n",
    "# System parameters\n",
    "Lx = 4\n",
    "Ly = 2\n",
    "nsites = Lx * Ly\n",
    "D = 4\n",
    "seed = 42\n",
    "flat = True\n",
    "\n",
    "# random fPEPS\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 1),  # linear index 1 -> charge 1, offset 1\n",
    "        (1, 0),  # linear index 2 -> charge 1, offset 0\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],  # -> (0, 3), (2, 1)\n",
    "    # put an odd number of odd sites in, for testing\n",
    "    # site_charge=lambda site: int(site in [(0, 0), (0, 1), (1, 0)]),\n",
    "    subsizes=\"equal\",\n",
    "    flat=flat,\n",
    "    seed=seed,\n",
    ")\n",
    "fx = torch.tensor([1, 2, 1, 2, 1, 2, 1, 2])\n",
    "\n",
    "model = Transformer_fPEPS_Model_batchedAttn(\n",
    "    tn=peps,\n",
    "    max_bond=4*D,\n",
    "    nn_eta=1,\n",
    "    nn_hidden_dim=16,\n",
    "    embed_dim=16,\n",
    "    attn_heads=4,\n",
    "    dtype=torch.float64,\n",
    ")\n",
    "nparams = sum(p.numel() for p in model.parameters())\n",
    "model(fx.unsqueeze(0)), nparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e703b2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParameterList(\n",
       "    (0): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (1): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (2): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (3): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (4): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (5): Parameter containing: [torch.float64 of size 8x2x2x2x2]\n",
       "    (6): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       "    (7): Parameter containing: [torch.float64 of size 4x2x2x2]\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quimb as qu\n",
    "tn = peps.copy()\n",
    "params, skeleton = qtn.pack(tn)\n",
    "# for torch, further flatten pytree into a single list\n",
    "ftn_params_flat, ftn_params_pytree = qu.utils.tree_flatten(\n",
    "    params, get_ref=True\n",
    ")\n",
    "ftn_params = torch.nn.ParameterList([\n",
    "    torch.as_tensor(x, dtype=torch.float64) for x in ftn_params_flat\n",
    "])\n",
    "ftn_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
