{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "@torch.inference_mode()\n",
    "def evaluate_energy_reuse(fxs, fpeps_model, H, current_amps, verbose=False):\n",
    "    # TODO: divide the connected configs into chunks of size fxs.shape[0] to avoid OOM [x]\n",
    "    B = fxs.shape[0]\n",
    "    # get connected configurations and coefficients\n",
    "    conn_eta_num = []\n",
    "    conn_etas = []\n",
    "    conn_eta_coeffs = []\n",
    "    for fx in fxs:\n",
    "        eta, coeffs = H.get_conn(fx)\n",
    "        conn_eta_num.append(len(eta))\n",
    "        conn_etas.append(torch.tensor(eta))\n",
    "        conn_eta_coeffs.append(torch.tensor(coeffs))\n",
    "\n",
    "    conn_etas = torch.cat(conn_etas, dim=0)\n",
    "    conn_eta_coeffs = torch.cat(conn_eta_coeffs, dim=0)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Prepared batched conn_etas and coeffs: {conn_etas.shape}, {conn_eta_coeffs.shape} (batch size {B})')\n",
    "\n",
    "    # calculate amplitudes for connected configs, in the future consider TN reuse to speed up calculation, TN reuse is controlled by a param that is not batched over (control flow?)\n",
    "    conn_amps = fpeps_model(conn_etas)\n",
    "\n",
    "    # Local energy \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "\n",
    "    local_energies = []\n",
    "    offset = 0\n",
    "    for b in range(B):\n",
    "        n_conn = conn_eta_num[b]\n",
    "        amps_ratio = conn_amps[offset:offset+n_conn] / current_amps[b]\n",
    "        energy_b = torch.sum(conn_eta_coeffs[offset:offset+n_conn] * amps_ratio)\n",
    "        local_energies.append(energy_b)\n",
    "        offset += n_conn\n",
    "    local_energies = torch.stack(local_energies, dim=0)\n",
    "    if verbose:\n",
    "        print(f'Batched local energies: {local_energies.shape}')\n",
    "\n",
    "    # Energy: (1/N) * \\sum_s <s|H|psi>/<s|psi> = (1/N) * \\sum_s \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "    energy = torch.mean(local_energies)\n",
    "    if verbose:\n",
    "        print(f'Energy: {energy.item()}')\n",
    "\n",
    "    return energy, local_energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d888b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "class SimpleModel_reuse(nn.Module):\n",
    "    def __init__(self, tn, max_bond, dtype=torch.float64):\n",
    "        import quimb as qu\n",
    "        import quimb.tensor as qtn\n",
    "        super().__init__()\n",
    "        \n",
    "        params, skeleton = qtn.pack(tn)\n",
    "        self.dtype = dtype\n",
    "        self.skeleton = skeleton\n",
    "        self.chi = max_bond\n",
    "        # for torch, further flatten pytree into a single list\n",
    "        params_flat, params_pytree = qu.utils.tree_flatten(\n",
    "            params, get_ref=True\n",
    "        )\n",
    "        self.params_pytree = params_pytree\n",
    "\n",
    "        # register the flat list parameters\n",
    "        self.params = nn.ParameterList([\n",
    "            torch.as_tensor(x, dtype=self.dtype) for x in params_flat\n",
    "        ])\n",
    "    \n",
    "    def amplitude(self, x, params, cache_bmps=False):\n",
    "        tn = qtn.unpack(params, self.skeleton)\n",
    "        # might need to specify the right site ordering here\n",
    "        amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "        if self.chi > 0:\n",
    "            amp.contract_boundary_from_ymin_(max_bond=self.chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "            amp.contract_boundary_from_ymax_(max_bond=self.chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "        if cache_bmps:\n",
    "            env_x = amp.compute_x_environments(max_bond=self.chi, cutoff=0.0)\n",
    "            return amp.contract(), env_x\n",
    "        \n",
    "        return amp.contract()\n",
    "    \n",
    "    def vamp(self, x, params, cache_bmps=False):\n",
    "        params = qu.utils.tree_unflatten(params, self.params_pytree)\n",
    "        return torch.vmap(\n",
    "            self.amplitude,\n",
    "            in_dims=(0, None, None),\n",
    "        )(x, params, cache_bmps)\n",
    "\n",
    "    def forward(self, x, cache_bmps=False):\n",
    "        return self.vamp(x, self.params, cache_bmps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed4cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import symmray as sr\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "Lx = 4\n",
    "Ly = 4\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites  # half-filling\n",
    "D = 4\n",
    "chi = 2*D # BUG: chi=4*D then the contraction becomes input dependent?\n",
    "# only the flat backend is compatible with jax.jit\n",
    "flat = True\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 0),  # linear index 1 -> charge 1, offset 0\n",
    "        (1, 1),  # linear index 2 -> charge 1, offset 1\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],\n",
    "    subsizes=\"equal\",\n",
    "    flat=True,\n",
    "    seed=42,\n",
    ")\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "\n",
    "fpeps_model = SimpleModel_reuse(peps, max_bond=chi, dtype=torch.float64)\n",
    "n_params = sum(p.numel() for p in fpeps_model.parameters())\n",
    "\n",
    "# generate Hamiltonian graph\n",
    "t=1.0\n",
    "U=8.0\n",
    "N_f = int(Lx*Ly) # half-filling\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(\n",
    "    Lx,\n",
    "    Ly,\n",
    "    t,\n",
    "    U,\n",
    "    N_f,\n",
    "    pbc=False,\n",
    "    n_fermions_per_spin=n_fermions_per_spin,\n",
    "    no_u1_symmetry=False,\n",
    ")\n",
    "graph = H.graph\n",
    "\n",
    "fxs = [torch.tensor(H.hilbert.random_state()) for _ in range(10)]\n",
    "fxs = torch.stack(fxs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021d771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21168.4573, dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cache(fx):\n",
    "    amp = peps.isel({peps.site_ind(site): fx[i] for i, site in enumerate(peps.sites)})\n",
    "    env_x = amp.compute_x_environments(max_bond=chi, cutoff=0.0)\n",
    "    return amp.contract()\n",
    "\n",
    "def vcache(fx):\n",
    "    return torch.vmap(cache, in_dims=0, out_dims=0)(fx)\n",
    "\n",
    "vcache(fxs) # BUG: the cache function must only return `pytree` to allow vmap\n",
    "\n",
    "cache(fxs[0])\n",
    "# TODO: do one time non-vmap contraction for cached skeletons of bMPSs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
