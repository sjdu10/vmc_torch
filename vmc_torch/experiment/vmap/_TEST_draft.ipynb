{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98cde5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import numpy as np\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random\n",
    "from vmc_torch.fermion_utils import fPEPS\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, tn, max_bond, dtype=torch.float64):\n",
    "        import quimb as qu\n",
    "        import quimb.tensor as qtn\n",
    "        super().__init__()\n",
    "        \n",
    "        params, skeleton = qtn.pack(tn)\n",
    "        self.dtype = dtype\n",
    "        self.skeleton = skeleton\n",
    "        self.chi = max_bond\n",
    "        # for torch, further flatten pytree into a single list\n",
    "        params_flat, params_pytree = qu.utils.tree_flatten(\n",
    "            params, get_ref=True\n",
    "        )\n",
    "        self.params_pytree = params_pytree\n",
    "\n",
    "        # register the flat list parameters\n",
    "        self.params = torch.nn.ParameterList([\n",
    "            torch.as_tensor(x, dtype=self.dtype) for x in params_flat\n",
    "        ])\n",
    "    \n",
    "    def amplitude(self, x, params):\n",
    "        tn = qtn.unpack(params, self.skeleton)\n",
    "        # might need to specify the right site ordering here\n",
    "        amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "        amp.contract_boundary_from_ymin_(max_bond=self.chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "        amp.contract_boundary_from_ymax_(max_bond=self.chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "        return amp.contract()\n",
    "    \n",
    "    def vamp(self, x, params):\n",
    "        params = qu.utils.tree_unflatten(params, self.params_pytree)\n",
    "        return torch.vmap(\n",
    "            self.amplitude,\n",
    "            in_dims=(0, None),\n",
    "        )(x, params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vamp(x, self.params)\n",
    "\n",
    "\n",
    "class TNAmplitudeModel(torch.nn.Module):\n",
    "    def __init__(self, fn, tn, vmap=False, **kwargs):\n",
    "        import quimb as qu\n",
    "        import quimb.tensor as qtn\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # split into plain arrays and bare tn structure\n",
    "        params, skeleton = qtn.pack(tn)\n",
    "        # for torch, further flatten pytree into a single list\n",
    "        params_flat, params_pytree = qu.utils.tree_flatten(\n",
    "            params, get_ref=True\n",
    "        )\n",
    "        # register the flat list parameters\n",
    "        self.params = torch.nn.ParameterList([\n",
    "            torch.as_tensor(x, dtype=torch.float32) for x in params_flat\n",
    "        ])\n",
    "\n",
    "        def amplitude(x):\n",
    "            params = qu.utils.tree_unflatten(self.params, params_pytree)\n",
    "            tn = qtn.unpack(params, skeleton)\n",
    "            return fn(x, tn, **kwargs)\n",
    "\n",
    "        if vmap:\n",
    "            self.f = torch.vmap(amplitude)\n",
    "        else:\n",
    "            self.f = amplitude\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.f(*args)\n",
    "\n",
    "\n",
    "def propose_exchange_or_hopping(i, j, current_config, hopping_rate=0.25):\n",
    "    ind_n_map = {0: 0, 1: 1, 2: 1, 3: 2}\n",
    "    if current_config[i] == current_config[j]:\n",
    "        return current_config, 0\n",
    "    proposed_config = current_config.clone()\n",
    "    config_i = current_config[i].item()\n",
    "    config_j = current_config[j].item()\n",
    "    if random.random() < 1 - hopping_rate:\n",
    "        # exchange\n",
    "        proposed_config[i] = config_j\n",
    "        proposed_config[j] = config_i\n",
    "    else:\n",
    "        # hopping\n",
    "        n_i = ind_n_map[current_config[i].item()]\n",
    "        n_j = ind_n_map[current_config[j].item()]\n",
    "        delta_n = abs(n_i - n_j)\n",
    "        if delta_n == 1:\n",
    "            # consider only valid hopping: (0, u) -> (u, 0); (d, ud) -> (ud, d)\n",
    "            proposed_config[i] = config_j\n",
    "            proposed_config[j] = config_i\n",
    "        elif delta_n == 0:\n",
    "            # consider only valid hopping: (u, d) -> (0, ud) or (ud, 0)\n",
    "            choices = [(0, 3), (3, 0)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        elif delta_n == 2:\n",
    "            # consider only valid hopping: (0, ud) -> (u, d) or (d, u)\n",
    "            choices = [(1, 2), (2, 1)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid configuration\")\n",
    "    return proposed_config, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a5b92",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df5dac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched Metropolis-Hastings updates\n",
    "import time\n",
    "\n",
    "@torch.inference_mode()\n",
    "def sample_next(fxs, fpeps_model, graph):\n",
    "    current_amps = fpeps_model(fxs)\n",
    "    B = len(fxs)\n",
    "    for row, edges in graph.row_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            # t0 = time.time()\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            # t1 = time.time()\n",
    "            # print(f\"Propose time: {t1 - t0}\")\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "\n",
    "    for col, edges in graph.col_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "    \n",
    "    return fxs, current_amps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "187bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential Metropolis-Hastings updates\n",
    "# current_amps = torch.stack([amplitude(fx, params) for fx in fxs], dim=0)\n",
    "# for row, edges in graph.row_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx, new = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params) if new == 1 else current_amps[k]\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp\n",
    "# for col, edges in graph.col_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params)\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067fc0f",
   "metadata": {},
   "source": [
    "Local Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f44218",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def evaluate_energy(fxs, fpeps_model, H, current_amps):\n",
    "    B = fxs.shape[0]\n",
    "    # get connected configurations and coefficients\n",
    "    conn_eta_num = []\n",
    "    conn_etas = []\n",
    "    conn_eta_coeffs = []\n",
    "    for fx in fxs:\n",
    "        eta, coeffs = H.get_conn(fx)\n",
    "        conn_eta_num.append(len(eta))\n",
    "        conn_etas.append(torch.tensor(eta))\n",
    "        conn_eta_coeffs.append(torch.tensor(coeffs))\n",
    "\n",
    "    conn_etas = torch.cat(conn_etas, dim=0)\n",
    "    conn_eta_coeffs = torch.cat(conn_eta_coeffs, dim=0)\n",
    "\n",
    "    print(f'Prepared batched conn_etas and coeffs: {conn_etas.shape}, {conn_eta_coeffs.shape} (batch size {B})')\n",
    "\n",
    "    # calculate amplitudes for connected configs, in the future consider TN reuse to speed up calculation, TN reuse is controlled by a param that is not batched over (control flow?)\n",
    "    conn_amps = fpeps_model(conn_etas)\n",
    "\n",
    "    # Local energy \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "\n",
    "    local_energies = []\n",
    "    offset = 0\n",
    "    for b in range(B):\n",
    "        n_conn = conn_eta_num[b]\n",
    "        amps_ratio = conn_amps[offset:offset+n_conn] / current_amps[b]\n",
    "        energy_b = torch.sum(conn_eta_coeffs[offset:offset+n_conn] * amps_ratio)\n",
    "        local_energies.append(energy_b)\n",
    "        offset += n_conn\n",
    "    local_energies = torch.stack(local_energies, dim=0)\n",
    "    print(f'Batched local energies: {local_energies.shape}')\n",
    "\n",
    "    # Energy: (1/N) * \\sum_s <s|H|psi>/<s|psi> = (1/N) * \\sum_s \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "    energy = torch.mean(local_energies)\n",
    "    print(f'Energy: {energy.item()}')\n",
    "\n",
    "    return energy, local_energies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d739ba4",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daa6bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(fxs, fpeps_model, vectorize=True):\n",
    "    if vectorize:\n",
    "        # Vectorized gradient calculation\n",
    "        # need per sample gradient of amplitude -- jacobian\n",
    "        params_pytree = (\n",
    "            list(fpeps_model.params)\n",
    "            if type(fpeps_model.params) is torch.nn.ParameterList\n",
    "            else dict(fpeps_model.params)\n",
    "            if type(fpeps_model.params) is torch.nn.ParameterDict\n",
    "            else fpeps_model.params\n",
    "        )\n",
    "        # params is a pytree, fxs has shape (B, nsites)\n",
    "        def g(x, p):\n",
    "            results = fpeps_model.vamp(x, p)\n",
    "            return results, results\n",
    "        t0 = time.time()\n",
    "        jac_pytree, amps = torch.func.jacrev(g, argnums=1, has_aux=True)(fxs, params_pytree)\n",
    "        t1 = time.time()\n",
    "        print(f\"Vectorized jacobian time: {t1 - t0}\")\n",
    "        # jac_pytree has shape same as params_pytree, each leaf has shape (B, )\n",
    "\n",
    "        # Get per-sample batched grads in list of pytree format\n",
    "        batched_grads_vec = []\n",
    "        for b in range(fxs.shape[0]):\n",
    "            if isinstance(jac_pytree, dict):\n",
    "                grad_b_iter = [jac_pytree[k][b] for k in jac_pytree.keys()]\n",
    "            elif isinstance(jac_pytree, list):\n",
    "                grad_b_iter = [jac_pytree[k][b] for k in range(len(jac_pytree))]\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grad_b_iter))\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)  # shape (B, Np), Np is number of parameters\n",
    "        amps.unsqueeze_(1)  # shape (B, 1)\n",
    "        return batched_grads_vec, amps\n",
    "    \n",
    "    else:\n",
    "        # Sequential non-vectorized gradient calculation\n",
    "        amps = []\n",
    "        batched_grads_vec = []\n",
    "        t0 = time.time()\n",
    "        for fx in fxs:\n",
    "            amp = fpeps_model(fx.unsqueeze(0))\n",
    "            amps.append(amp)\n",
    "            amp.backward()\n",
    "            grads = qu.tree_map(lambda x: x.grad, fpeps_model.params_as_dict())\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grads.values()))\n",
    "            qu.tree_map(lambda x: x.grad.zero_(), fpeps_model.params_as_dict())\n",
    "        t1 = time.time()\n",
    "        print(f\"Sequential jacobian time: {t1 - t0}\")\n",
    "        amps = torch.stack(amps, dim=0)\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)\n",
    "        return batched_grads_vec, amps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb6ed9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/home/sijingdu/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "n=25, D=4, tau=0.1, max|dS|=0.0118, energyâ‰ˆ-4.23686: 100%|##########| 25/25 [00:02<00:00, 12.30it/s]\n",
      "/home/sijingdu/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/utils/_device.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# SU\n",
    "Lx = 3\n",
    "Ly = 2\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites  # half-filling\n",
    "D = 4\n",
    "chi = 4*D\n",
    "seed = 42\n",
    "# only the flat backend is compatible with jax.jit\n",
    "flat = False\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap'\n",
    "# params = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_su_params.pkl', 'rb'))\n",
    "# skeleton = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_skeleton.pkl', 'rb'))\n",
    "# peps = qtn.unpack(params, skeleton)\n",
    "\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 0),  # linear index 1 -> charge 1, offset 0\n",
    "        (1, 1),  # linear index 2 -> charge 1, offset 1\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],\n",
    "    subsizes=\"equal\",\n",
    "    flat=False,\n",
    "    seed=seed,\n",
    ")\n",
    "original_peps = peps.copy()\n",
    "\n",
    "\n",
    "# Old sparse fpeps model\n",
    "fpeps_random = peps.copy()\n",
    "fpeps_random.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "\n",
    "# SU\n",
    "terms = sr.ham_fermi_hubbard_from_edges(\n",
    "    symmetry='Z2',\n",
    "    edges=tuple(peps.gen_bond_coos()),\n",
    "    t=1.0,\n",
    "    U=8.0,\n",
    "    mu=4.0,\n",
    ")\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "ham.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "# if flat:\n",
    "#     ham.terms = {k: v.to_flat() for k, v in ham.terms.items()}\n",
    "su = qtn.SimpleUpdateGen(\n",
    "    fpeps_random,\n",
    "    ham,\n",
    "    # setting a cutoff is important to turn on dynamic charge sectors\n",
    "    # cutoff=1e-12,\n",
    "    cutoff=0.0,\n",
    "    second_order_reflect=True,\n",
    "    # SimpleUpdateGen computes cluster energies by default\n",
    "    # which might not be accurate\n",
    "    compute_energy_every=10,\n",
    "    compute_energy_opts=dict(max_distance=1),\n",
    "    compute_energy_per_site=True,\n",
    "    # use a fixed trotterization order\n",
    "    ordering=\"sort\",\n",
    "    # if the gauge difference drops below this, we consider the PEPS converged\n",
    "    tol=1e-9,\n",
    ")\n",
    "# run the evolution, these are reasonable defaults\n",
    "tau = 0.1\n",
    "steps = 25\n",
    "su.evolve(steps, tau=tau)\n",
    "peps = su.get_state()\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "for site in peps.sites:\n",
    "    peps[site].data._label = site\n",
    "fpeps = fPEPS(peps) # old fpeps\n",
    "# turn to flat backend if needed\n",
    "if flat:\n",
    "    for ts in peps.tensors:\n",
    "        ts.modify(data=ts.data.to_flat())\n",
    "\n",
    "def amplitude(x, params, skeleton):\n",
    "    tn = qtn.unpack(params, skeleton)\n",
    "    # might need to specify the right site ordering here\n",
    "    amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "    amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "    amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "    return amp.contract()\n",
    "\n",
    "\n",
    "# torch.set_default_device(\"cuda:0\") # GPU\n",
    "torch.set_default_device(\"cpu\") # CPU\n",
    "\n",
    "if flat:\n",
    "    fpeps_model = SimpleModel(peps, max_bond=chi, dtype=torch.float64)\n",
    "    # fpeps_model(fxs[0].unsqueeze(0))  # warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0847c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "from vmc_torch.fermion_utils import from_quimb_config_to_netket_config, from_netket_config_to_quimb_config, from_netket_config_to_quimb_binary_config, from_quimb_biniary_config_to_netket_config\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "# generate Hamiltonian graph\n",
    "t=1.0\n",
    "U=8.0\n",
    "N_f = int(Lx*Ly) # half-filling\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(\n",
    "    Lx,\n",
    "    Ly,\n",
    "    t,\n",
    "    U,\n",
    "    N_f,\n",
    "    pbc=False,\n",
    "    n_fermions_per_spin=n_fermions_per_spin,\n",
    "    no_u1_symmetry=True,\n",
    ")\n",
    "graph = H.graph\n",
    "print(len(H.hilbert.all_states()))\n",
    "\n",
    "# fxs_nk = from_quimb_config_to_netket_config(fxs)\n",
    "# fxs_quimb_binary = from_netket_config_to_quimb_binary_config(fxs_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "258d759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quimb.experimental.operatorbuilder as qop\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "sites = [(i, j) for i in range(Lx) for j in range(Ly)]\n",
    "H_quimb = qop.fermi_hubbard_from_edges(\n",
    "    edges,\n",
    "    t=t,\n",
    "    U=U,\n",
    "    mu=0,\n",
    "    # this ordering pairs spins together, as with the fermionic TN\n",
    "    order=lambda site: (site[1], site[0]),\n",
    "    sector=int(sum(ary.charge for ary in peps.arrays) % 2),\n",
    "    symmetry=\"Z2\",\n",
    ")\n",
    "# H_quimb.flatconfig_coupling(fxs[0])\n",
    "hs = H_quimb.hilbert_space\n",
    "# eta_quimb, eta_coeff_quimb = H_quimb.flatconfig_coupling(fxs_quimb_binary[0])\n",
    "# from_netket_config_to_quimb_config(from_quimb_biniary_config_to_netket_config(eta_quimb)), eta_coeff_quimb, hs, H.get_conn(fxs[0]), fxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bff4f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, peps = qtn.pack(peps)\n",
    "# peps = qtn.unpack(params, skeleton)\n",
    "def amplitude_binary(x, peps):\n",
    "    x = x[::2] + 2 * x[1::2]\n",
    "    x = torch.tensor(x, dtype=torch.int64)\n",
    "    # might need to specify the right site ordering here\n",
    "    amp = peps.isel({peps.site_ind(site): x[i] for i, site in enumerate(peps.sites)})\n",
    "    amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "    amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "    return amp.contract()\n",
    "\n",
    "# get pytree of initial parameters, and reference tn structure\n",
    "params, skeleton = qtn.pack(peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5db4cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dense energy: 1.671421313329154; Old dense energy: -0.2367327605617049\n"
     ]
    }
   ],
   "source": [
    "if Lx * Ly <= 6:\n",
    "    configs = H.hilbert.all_states()\n",
    "    with torch.no_grad():\n",
    "        H_dense = torch.tensor(H.to_dense(), dtype=torch.float64)\n",
    "        all_states = torch.tensor(configs, dtype=torch.int64)\n",
    "        psi_vec = fpeps_model(all_states) if flat else torch.tensor(\n",
    "            [amplitude(fx, params, skeleton) for fx in all_states], dtype=torch.float64\n",
    "        )\n",
    "        fpsi_vec = torch.tensor([fpeps.get_amp(fx).contract().item() for fx in all_states], dtype=torch.float64)\n",
    "\n",
    "        E = (psi_vec @ H_dense @ psi_vec)/(psi_vec @ psi_vec)\n",
    "        E1 = (fpsi_vec @ H_dense @ fpsi_vec)/(fpsi_vec @ fpsi_vec)\n",
    "    print(f'New dense energy: {E.item() / Lx / Ly}; Old dense energy: {E1.item() / Lx / Ly}')\n",
    "\n",
    "if Lx * Ly <= 4:\n",
    "    # compute the full exact energy at the amplitude level\n",
    "    O = 0.0\n",
    "    O_f = 0.0\n",
    "    p = 0.0\n",
    "    p_f = 0.0\n",
    "\n",
    "    fcs = []\n",
    "    for i in range(hs.size):\n",
    "        fx = hs.rank_to_flatconfig(i)\n",
    "        fx_tn = torch.tensor(fx[::2] + 2*fx[1::2], dtype=torch.int64)\n",
    "        xpsi = amplitude(fx_tn, params, skeleton).item()\n",
    "        # xpsi = amplitude_binary(fx, peps).item()\n",
    "        xpsi_f = fpeps.get_amp(fx_tn).contract().item()\n",
    "\n",
    "        pi = abs(xpsi) ** 2\n",
    "        p += pi\n",
    "\n",
    "        pi_f = abs(xpsi_f) ** 2\n",
    "        p_f += pi_f\n",
    "\n",
    "        Oloc = 0.0\n",
    "        O_floc = 0.0\n",
    "        for fy, hxy in zip(*H_quimb.flatconfig_coupling(fx)):\n",
    "            fy_tn = torch.tensor(fy[::2] +2*fy[1::2], dtype=torch.int64)\n",
    "            if xpsi:\n",
    "                ypsi = amplitude(fy_tn, params, skeleton).item()\n",
    "                # ypsi = amplitude_binary(fy, peps).item()\n",
    "                Oloc = Oloc + hxy * ypsi / xpsi\n",
    "            if xpsi_f:\n",
    "                ypsi_f = fpeps.get_amp(fy_tn).contract().item()\n",
    "                O_floc = O_floc + hxy * ypsi_f / xpsi_f\n",
    "\n",
    "        O += Oloc * pi\n",
    "        O_f += O_floc * pi_f\n",
    "\n",
    "    print(f'New amp MC energy: {O / p / Lx / Ly}. Old amp MC energy: {O_f / p_f / Lx / Ly}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b09f7c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5.4059e-13, -2.3757e-09,  2.0505e-09,  4.3648e-09, -5.4708e-09,\n",
       "         -3.9091e-09, -5.5853e-11, -9.8753e-10,  0.0000e+00,  0.0000e+00],\n",
       "        dtype=torch.float64),\n",
       " tensor([ 5.4059e-13, -2.3757e-09, -5.3833e-09, -2.8864e-09, -5.4708e-09,\n",
       "         -3.9091e-09, -5.5853e-11, -9.8753e-10,  2.0505e-09,  4.3648e-09],\n",
       "        dtype=torch.float64),\n",
       " array([[0, 0, 0, 0, 0, 0],\n",
       "        [2, 2, 0, 0, 0, 0],\n",
       "        [2, 0, 2, 0, 0, 0],\n",
       "        [2, 0, 0, 2, 0, 0],\n",
       "        [2, 0, 0, 0, 2, 0],\n",
       "        [2, 0, 0, 0, 0, 2],\n",
       "        [3, 0, 0, 0, 0, 0],\n",
       "        [2, 1, 0, 0, 0, 0],\n",
       "        [2, 0, 1, 0, 0, 0],\n",
       "        [2, 0, 0, 1, 0, 0]]),\n",
       " array([2, 0, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_vec[:10], fpsi_vec[:10], configs[:10], configs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c600284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_317/1629115161.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  new_peps.apply_to_arrays(lambda x: np.array(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-0.2367327605617048)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact energy via local expectation contraction\n",
    "terms = sr.hamiltonians.ham_fermi_hubbard_from_edges(\n",
    "    \"Z2\",\n",
    "    edges=edges,\n",
    "    U=8,\n",
    "    mu=0.0,\n",
    ")\n",
    "if flat:\n",
    "    terms = {k: v.to_flat() for k, v in terms.items()}\n",
    "new_peps = peps.copy()\n",
    "new_peps.apply_to_arrays(lambda x: np.array(x))\n",
    "new_peps.compute_local_expectation_exact(terms, normalized=True) / nsites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f8ac4",
   "metadata": {},
   "source": [
    "VMC update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling time: 0.318359375 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6212, 8]), torch.Size([6212]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 14.014137655355086\n",
      "Vectorized jacobian time: 3.762491464614868\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6212, 8]), torch.Size([6212]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 9.295374721562009\n",
      "STEP 0 VMC energy after update: 9.295374721562009\n",
      "\n",
      "Sampling time: 0.335552453994751 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6244, 8]), torch.Size([6244]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 12.050172083625418\n",
      "Vectorized jacobian time: 3.8198659420013428\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6244, 8]), torch.Size([6244]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 7.303123989902819\n",
      "STEP 1 VMC energy after update: 7.303123989902819\n",
      "\n",
      "Sampling time: 0.31620097160339355 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6325, 8]), torch.Size([6325]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 9.506577507336324\n",
      "Vectorized jacobian time: 3.7204911708831787\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6325, 8]), torch.Size([6325]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 3.93155785587141\n",
      "STEP 2 VMC energy after update: 3.93155785587141\n",
      "\n",
      "Sampling time: 0.3018338680267334 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6375, 8]), torch.Size([6375]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 7.657345825856288\n",
      "Vectorized jacobian time: 3.172957420349121\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6375, 8]), torch.Size([6375]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 4.107710313657934\n",
      "STEP 3 VMC energy after update: 4.107710313657934\n",
      "\n",
      "Sampling time: 0.307342529296875 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6271, 8]), torch.Size([6271]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 6.381555091857109\n",
      "Vectorized jacobian time: 3.48970365524292\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6271, 8]), torch.Size([6271]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 5.0419788073267195\n",
      "STEP 4 VMC energy after update: 5.0419788073267195\n",
      "\n",
      "Sampling time: 0.3260631561279297 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6288, 8]), torch.Size([6288]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 6.302617534039218\n",
      "Vectorized jacobian time: 3.6733572483062744\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6288, 8]), torch.Size([6288]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 1.7557541661652276\n",
      "STEP 5 VMC energy after update: 1.7557541661652276\n",
      "\n",
      "Sampling time: 0.30208468437194824 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6309, 8]), torch.Size([6309]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 4.886059650840149\n",
      "Vectorized jacobian time: 3.5506486892700195\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6309, 8]), torch.Size([6309]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 1.0881529328026533\n",
      "STEP 6 VMC energy after update: 1.0881529328026533\n",
      "\n",
      "Sampling time: 0.2989084720611572 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6097, 8]), torch.Size([6097]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 5.100715813149625\n",
      "Vectorized jacobian time: 3.686461925506592\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6097, 8]), torch.Size([6097]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 0.05739622224402324\n",
      "STEP 7 VMC energy after update: 0.05739622224402324\n",
      "\n",
      "Sampling time: 0.29415178298950195 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6069, 8]), torch.Size([6069]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 3.4175218589616776\n",
      "Vectorized jacobian time: 3.6194400787353516\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6069, 8]), torch.Size([6069]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 0.6517701844076672\n",
      "STEP 8 VMC energy after update: 0.6517701844076672\n",
      "\n",
      "Sampling time: 0.2982969284057617 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6053, 8]), torch.Size([6053]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 3.025843113887962\n",
      "Vectorized jacobian time: 3.686192274093628\n",
      "Prepared batched conn_etas and coeffs: torch.Size([6053, 8]), torch.Size([6053]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 0.2835981318429384\n",
      "STEP 9 VMC energy after update: 0.2835981318429384\n",
      "\n",
      "Sampling time: 0.295365571975708 seconds for 512 samples\n",
      "Prepared batched conn_etas and coeffs: torch.Size([5970, 8]), torch.Size([5970]) (batch size 512)\n",
      "Batched local energies: torch.Size([512])\n",
      "Energy: 1.7825290819132147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSampling time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m batched_grads_vec, amps = \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpeps_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Now that we have local energies, amps and per-sample gradients, we can compute the energy gradient\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# With the energy gradient, we can further do SR for optimization\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Or we can do minSR, which is simpler here\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcompute_grads\u001b[39m\u001b[34m(fxs, fpeps_model, vectorize)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results, results\n\u001b[32m     16\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m jac_pytree, amps = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_pytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m t1 = time.time()\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVectorized jacobian time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt1\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:690\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    688\u001b[39m     flat_jacobians_per_input = compute_jacobian_preallocate_and_copy()\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     flat_jacobians_per_input = \u001b[43mcompute_jacobian_stacked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;66;03m# Step 2: The returned jacobian is one big tensor per input. In this step,\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# we split each Tensor by output.\u001b[39;00m\n\u001b[32m    694\u001b[39m flat_jacobians_per_input = [\n\u001b[32m    695\u001b[39m     result.split(flat_output_numels, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m flat_jacobians_per_input\n\u001b[32m    697\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:612\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn.<locals>.compute_jacobian_stacked\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    610\u001b[39m     chunked_result = vjp_fn(basis)\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# chunk_size is None or chunk_size != 1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     chunked_result = \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m flat_results = pytree.tree_leaves(chunked_result)\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/apis.py:208\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:282\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    272\u001b[39m         func,\n\u001b[32m    273\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         **kwargs,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:432\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    429\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    430\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    431\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:395\u001b[39m, in \u001b[36m_vjp_with_argnums.<locals>.wrapper\u001b[39m\u001b[34m(cotangents, retain_graph, create_graph)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m primals_out_spec != cotangents_spec:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    390\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected pytree structure of cotangents to be the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    391\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mas pytree structure of outputs to the function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    392\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcotangents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec_pprint(cotangents_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprimal output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec_pprint(primals_out_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m result = \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_primals_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_diff_primals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_cotangents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(result, primals_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:142\u001b[39m, in \u001b[36m_autograd_grad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(torch.zeros_like(inp) \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch._dynamo.compiled_autograd._disable():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     grad_inputs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiff_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m grad_inputs = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    151\u001b[39m     torch.zeros_like(inp) \u001b[38;5;28;01mif\u001b[39;00m gi \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gi\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gi, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_inputs, inputs)\n\u001b[32m    153\u001b[39m )\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad_inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/__init__.py:452\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    450\u001b[39m overridable_args = t_outputs + t_inputs\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverridable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_inputs:\n\u001b[32m    467\u001b[39m     warnings.warn(\n\u001b[32m    468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    473\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/overrides.py:1728\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1725\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1726\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1728\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1730\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# generate half-filling configs\n",
    "# batchsize\n",
    "B = 512\n",
    "rng = np.random.default_rng(seed)\n",
    "xs_u = np.concatenate(\n",
    "    [\n",
    "        np.zeros((B, nsites // 2), dtype=np.int32),\n",
    "        np.ones((B, nsites // 2), dtype=np.int32),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "xs_d = xs_u.copy()\n",
    "xs_u = rng.permuted(xs_u, axis=1)\n",
    "xs_d = rng.permuted(xs_d, axis=1)\n",
    "xs = np.concatenate([xs_u[:, :, None], xs_d[:, :, None]], axis=2).reshape(B, -1)\n",
    "fxs = 2 * xs[:, ::2] + xs[:, 1::2]  # map to 0,1,2,3\n",
    "fxs = torch.tensor(fxs, dtype=torch.int32)\n",
    "\n",
    "for _ in range(50):\n",
    "    t0 = time.time()\n",
    "    fxs, current_amps = sample_next(fxs, fpeps_model, graph)\n",
    "    t1 = time.time()\n",
    "    print(f'Sampling time: {t1 - t0} seconds for {B} samples')\n",
    "    energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "    batched_grads_vec, amps = compute_grads(fxs, fpeps_model, vectorize=True)\n",
    "    # Now that we have local energies, amps and per-sample gradients, we can compute the energy gradient\n",
    "    # With the energy gradient, we can further do SR for optimization\n",
    "    # Or we can do minSR, which is simpler here\n",
    "    with torch.no_grad():\n",
    "        local_energies # shape (B,)\n",
    "        local_energies_mean = torch.mean(local_energies)\n",
    "        amps # shape (B,)\n",
    "        params # pytree with each leaf of shape (param_shape...)\n",
    "\n",
    "        # flatten the model params into a 1d vector of shape (Np,)\n",
    "        params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "\n",
    "        # compute log-derivative grads\n",
    "        batched_log_grads_vec = batched_grads_vec / amps  # shape (B, Np)\n",
    "        log_grads_vec_mean = torch.mean(batched_log_grads_vec, dim=0)  # shape (Np,)\n",
    "\n",
    "        O_sk = (batched_log_grads_vec - log_grads_vec_mean[None, :]) / (B**0.5)  # shape (B, Np)\n",
    "        T = (O_sk @ O_sk.T.conj())  # shape (B, B)\n",
    "        E_s = (local_energies - local_energies_mean) / (B**0.5)  # shape (B,)\n",
    "\n",
    "        # minSR: need to solve O_sk * dp = E_s in the least square sense, using the pseudo-inverse of O_sk to get the minimum norm solution\n",
    "        T_inv = torch.linalg.pinv(T,  rtol=1e-12, atol=0, hermitian=True)\n",
    "        dp = O_sk.conj().T @ (T_inv @ E_s)  # shape (Np,)\n",
    "        # update params\n",
    "        learning_rate = 0.1\n",
    "        new_params_vec = params_vec - learning_rate * dp\n",
    "\n",
    "    # load the new params back to the model\n",
    "    torch.nn.utils.vector_to_parameters(new_params_vec, fpeps_model.parameters())\n",
    "\n",
    "    energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "    print(f'STEP {_} VMC energy after update: {energy.item()}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
