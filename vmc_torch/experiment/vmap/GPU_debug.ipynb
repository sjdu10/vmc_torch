{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b18d1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not using torchrun. Defaulting to single device.\n",
      "Model parameters: 128 | World Size: 1 | Device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:   2%|▏         | 1/50 [00:02<02:09,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: E = 0.342344, Var = 8.92e-01, Std of E_mean = 4.22e-02\n",
      "SR dp mean: 0.1402525148382699, std: 4.339575989193563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:   4%|▍         | 2/50 [00:05<01:59,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: E = 0.264746, Var = 1.49e+00, Std of E_mean = 5.46e-02\n",
      "SR dp mean: -0.25565849697787224, std: 3.2501419288451547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:   6%|▌         | 3/50 [00:07<01:59,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: E = 0.080028, Var = 3.04e-01, Std of E_mean = 2.47e-02\n",
      "SR dp mean: -0.5029354180332347, std: 6.642014631556633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:   8%|▊         | 4/50 [00:10<02:01,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: E = -0.060726, Var = 1.14e-01, Std of E_mean = 1.51e-02\n",
      "SR dp mean: 0.029852509806195547, std: 3.3284047160087624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  10%|█         | 5/50 [00:13<02:03,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: E = -0.071733, Var = 6.14e-02, Std of E_mean = 1.11e-02\n",
      "SR dp mean: -0.015150399601216329, std: 1.21282559461489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  12%|█▏        | 6/50 [00:16<02:02,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: E = -0.103638, Var = 7.02e-03, Std of E_mean = 3.75e-03\n",
      "SR dp mean: -0.07416685104231913, std: 0.5103943355049855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  14%|█▍        | 7/50 [00:19<02:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: E = -0.097767, Var = 2.39e-02, Std of E_mean = 6.91e-03\n",
      "SR dp mean: 0.05111357004638722, std: 1.318456989465329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  16%|█▌        | 8/50 [00:21<01:59,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: E = -0.107848, Var = 3.92e-03, Std of E_mean = 2.80e-03\n",
      "SR dp mean: 0.0011944513957517708, std: 0.33725259860144086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  18%|█▊        | 9/50 [00:24<01:56,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8: E = -0.113258, Var = 4.38e-03, Std of E_mean = 2.96e-03\n",
      "SR dp mean: -0.00960746043527783, std: 0.5876895939502549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  20%|██        | 10/50 [00:27<01:54,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9: E = -0.110830, Var = 7.51e-03, Std of E_mean = 3.88e-03\n",
      "SR dp mean: 0.0650330782163315, std: 0.49646564280259686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  22%|██▏       | 11/50 [00:30<01:51,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: E = -0.142724, Var = 1.86e-01, Std of E_mean = 1.93e-02\n",
      "SR dp mean: 0.09775656889323286, std: 2.039713887063871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  24%|██▍       | 12/50 [00:33<01:49,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 11: E = -0.126652, Var = 6.60e-03, Std of E_mean = 3.63e-03\n",
      "SR dp mean: -0.1302555064743034, std: 9.05015897467296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  26%|██▌       | 13/50 [00:36<01:48,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12: E = -0.062193, Var = 9.77e-02, Std of E_mean = 1.40e-02\n",
      "SR dp mean: -0.025087843810528435, std: 2.4652350269034087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  28%|██▊       | 14/50 [00:40<01:51,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 13: E = -0.048701, Var = 1.44e-01, Std of E_mean = 1.70e-02\n",
      "SR dp mean: 0.29512474419635126, std: 3.11598471532187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  30%|███       | 15/50 [00:43<01:54,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 14: E = -0.098231, Var = 5.41e-02, Std of E_mean = 1.04e-02\n",
      "SR dp mean: -0.22168260342843, std: 1.9394327816510644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  32%|███▏      | 16/50 [00:47<01:52,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 15: E = -0.105112, Var = 4.16e-02, Std of E_mean = 9.12e-03\n",
      "SR dp mean: -0.06374344731524788, std: 1.9403349855157184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  34%|███▍      | 17/50 [00:50<01:54,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 16: E = -0.113849, Var = 6.00e-02, Std of E_mean = 1.10e-02\n",
      "SR dp mean: -0.8930259886747504, std: 4.5347204553487765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  36%|███▌      | 18/50 [00:55<01:57,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17: E = -0.123439, Var = 4.12e-02, Std of E_mean = 9.08e-03\n",
      "SR dp mean: 0.2498581863896141, std: 2.039108596699125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  38%|███▊      | 19/50 [00:58<01:47,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 18: E = -0.123508, Var = 3.55e-03, Std of E_mean = 2.66e-03\n",
      "SR dp mean: 0.040718757554239704, std: 0.28438599925931624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  40%|████      | 20/50 [01:01<01:42,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 19: E = -0.049759, Var = 1.85e+00, Std of E_mean = 6.08e-02\n",
      "SR dp mean: 0.005335274036637583, std: 5.831408819309945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  42%|████▏     | 21/50 [01:05<01:42,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: E = -0.071748, Var = 2.65e-01, Std of E_mean = 2.30e-02\n",
      "SR dp mean: 0.3580838958382103, std: 3.390476063743327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC Steps:  44%|████▍     | 22/50 [01:08<01:37,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 21: E = -0.168455, Var = 1.81e-01, Std of E_mean = 1.90e-02\n",
      "SR dp mean: -0.10796654117810331, std: 2.367569393039268\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 224\u001b[39m\n\u001b[32m    222\u001b[39m T = O_sk @ O_sk.conj().T\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# Pseudo-inverse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m T_inv = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhermitian\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# dp = O^dagger * T_inv * E_s\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# (Np, Ns) @ (Ns, Ns) @ (Ns, ) -> (Np, )\u001b[39;00m\n\u001b[32m    227\u001b[39m dp = O_sk.conj().T @ (T_inv @ E_s)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np # 仅用于非计算的简单统计或IO\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设这些是你现有的工具库\n",
    "from vmc_torch.experiment.vmap.GPU_vmap_utils import sample_next, evaluate_energy, compute_grads, random_initial_config\n",
    "from vmc_torch.experiment.vmap.GPU_vmap_utils import Transformer_fPEPS_Model_batchedAttn, fPEPS_Model\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "from vmc_torch.experiment.tn_model import init_weights_to_zero\n",
    "import quimb.tensor as qtn\n",
    "\n",
    "# ==========================================\n",
    "# 1. 初始化 Distributed 环境 (GPU Native)\n",
    "# ==========================================\n",
    "def setup_distributed():\n",
    "    if \"RANK\" not in os.environ:\n",
    "        # 调试模式：如果没有用 torchrun 启动，默认单卡运行\n",
    "        print(\"Warning: Not using torchrun. Defaulting to single device.\")\n",
    "        os.environ[\"RANK\"] = \"0\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "        os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    \n",
    "    # 核心：设置当前进程使用的 GPU\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(f\"cuda:{local_rank}\")\n",
    "    return rank, world_size, device\n",
    "\n",
    "RANK, WORLD_SIZE, device = setup_distributed()\n",
    "\n",
    "# 设置默认精度\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# 不同 Rank 设置不同随机种子，保证采样独立\n",
    "torch.manual_seed(42 + RANK)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 参数设置与模型加载\n",
    "# ==========================================\n",
    "Lx, Ly = 2, 2\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites\n",
    "D = 4\n",
    "chi = -1\n",
    "\n",
    "# 路径配置 (保持你的原样)\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap'\n",
    "u1z2 = True\n",
    "appendix = '_U1SU' if u1z2 else ''\n",
    "\n",
    "# 加载骨架 (这部分很快，可以在 CPU 做完再转 GPU)\n",
    "# 注意：pickle load 最好只在 Rank 0 做然后广播，或者大家各自读文件(如果文件系统支持并发)\n",
    "# 这里假设大家各自读文件没问题\n",
    "params_pkl = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_su_params{appendix}.pkl', 'rb'))\n",
    "skeleton = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_skeleton{appendix}.pkl', 'rb'))\n",
    "peps = qtn.unpack(params_pkl, skeleton)\n",
    "\n",
    "# 预处理 (CPU)\n",
    "for ts in peps.tensors:\n",
    "    ts.modify(data=ts.data.to_flat() * 10)\n",
    "for site in peps.sites:\n",
    "    peps[site].data._label = site\n",
    "    peps[site].data.indices[-1]._linearmap = ((0, 0), (1, 0), (1, 1), (0, 1))\n",
    "\n",
    "# 初始化模型并移动到 GPU\n",
    "# fpeps_model = Transformer_fPEPS_Model_batchedAttn(\n",
    "#     tn=peps, max_bond=chi, embed_dim=8, attn_heads=4, nn_hidden_dim=16, nn_eta=1, dtype=torch.float64,\n",
    "# )\n",
    "fpeps_model = fPEPS_Model(\n",
    "    tn=peps, max_bond=chi, dtype=torch.float64,\n",
    ")\n",
    "fpeps_model.to(device) # <--- 关键：模型全在 GPU\n",
    "\n",
    "# 初始化权重\n",
    "model_params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "init_std = float(model_params_vec.std().item()) * 0.1\n",
    "fpeps_model.apply(lambda x: init_weights_to_zero(x, std=init_std))\n",
    "\n",
    "n_params = sum(p.numel() for p in fpeps_model.parameters())\n",
    "if RANK == 0:\n",
    "    print(f'Model parameters: {n_params} | World Size: {WORLD_SIZE} | Device: {device}')\n",
    "\n",
    "# Hamiltonians\n",
    "t, U = 1.0, 8.0\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(\n",
    "    Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin, no_u1_symmetry=False,gpu=True\n",
    ")\n",
    "graph = H.graph\n",
    "\n",
    "# ==========================================\n",
    "# 3. 采样配置\n",
    "# ==========================================\n",
    "Total_Ns = int(5e2)  # 总样本数\n",
    "# 确保每个 Rank 分到的样本数是整数\n",
    "assert Total_Ns % WORLD_SIZE == 0, f\"Total samples {Total_Ns} must be divisible by World Size {WORLD_SIZE}\"\n",
    "samples_per_rank = Total_Ns // WORLD_SIZE\n",
    "\n",
    "# 并行运行的 Chain 数量 (Batch Size)\n",
    "# 如果显存够，可以直接设为 samples_per_rank，这样一步到位\n",
    "# 如果显存不够，可以设小一点，循环多次累积\n",
    "batch_size_per_rank = 64\n",
    "# 确保初始化 walkers 在 GPU 上\n",
    "fxs_list = [random_initial_config(N_f, nsites, seed=None) for _ in range(batch_size_per_rank)]\n",
    "fxs = torch.stack(fxs_list).to(device)\n",
    "\n",
    "# Burn-in (Warmup)\n",
    "for _ in range(2): # 调整你的 burn-in 步数\n",
    "    fxs, current_amps = sample_next(fxs, fpeps_model, graph, seed=None)\n",
    "\n",
    "# VMC Settings\n",
    "vmc_steps = 50\n",
    "minSR = True # 推荐用 minSR，因为全在 GPU 上很快\n",
    "learning_rate = 0.1\n",
    "save_state_every = 10000\n",
    "stats_file = pwd + f'/stats_{fpeps_model._get_name()}.json'\n",
    "stats = {'mean': [], 'error': [], 'variance': []}\n",
    "\n",
    "# ==========================================\n",
    "# 4. VMC 主循环 (All on GPU)\n",
    "# ==========================================\n",
    "if RANK == 0:\n",
    "    vmc_pbar = tqdm(total=vmc_steps, desc=\"VMC Steps\")\n",
    "\n",
    "for step in range(vmc_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # --- A. 本地采样与梯度计算 (Local Sampling & Compute) ---\n",
    "    # 我们需要在本地累积 samples_per_rank 这么多数据\n",
    "    local_energies_acc = []\n",
    "    local_grads_acc = []\n",
    "    local_amps_acc = []\n",
    "    \n",
    "    ##############################################################################\n",
    "    # debug\n",
    "    current_count = 0\n",
    "    while current_count < samples_per_rank:\n",
    "        # 1. 采样\n",
    "        fxs, current_amps = sample_next(fxs, fpeps_model, graph, seed=None)\n",
    "        \n",
    "        # 2. 计算能量\n",
    "        # 注意：evaluate_energy 内部需要确保返回 GPU tensor\n",
    "        _, local_E = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "        \n",
    "        # 3. 计算梯度\n",
    "        # batch_size=batch_size_per_rank 表示一次处理完，避免 OOM\n",
    "        local_grads, local_amps = compute_grads(fxs, fpeps_model, vectorize=True)\n",
    "        \n",
    "        # 4. 收集 (还是 GPU tensor)\n",
    "        # 裁剪掉多余的样本 (如果 batch_size 不整除 samples_per_rank)\n",
    "        needed = min(batch_size_per_rank, samples_per_rank - current_count)\n",
    "        \n",
    "        local_energies_acc.append(local_E[:needed])\n",
    "        local_grads_acc.append(local_grads[:needed])\n",
    "        local_amps_acc.append(local_amps[:needed])\n",
    "        \n",
    "        current_count += needed\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    # 拼接本地数据\n",
    "    my_energies = torch.cat(local_energies_acc) # (samples_per_rank, )\n",
    "    my_grads = torch.cat(local_grads_acc)       # (samples_per_rank, Np)\n",
    "    my_amps = torch.cat(local_amps_acc)         # (samples_per_rank, )\n",
    "    \n",
    "    # 确保内存连续 (通信必须)\n",
    "    my_energies = my_energies.contiguous()\n",
    "    my_grads = my_grads.contiguous()\n",
    "    my_amps = my_amps.contiguous()\n",
    "\n",
    "    # --- B. 全局聚合 (Global Gather) ---\n",
    "    # 准备接收容器\n",
    "    def gather_tensor(tensor):\n",
    "        gather_list = [torch.zeros_like(tensor) for _ in range(WORLD_SIZE)]\n",
    "        dist.all_gather(gather_list, tensor)\n",
    "        return torch.cat(gather_list)\n",
    "\n",
    "    total_energies = gather_tensor(my_energies) # (Total_Ns, )\n",
    "    total_amps = gather_tensor(my_amps)         # (Total_Ns, )\n",
    "    # 如果 Np 很大，gather grads 可能会显存爆炸。如果炸了需要换策略 (reduce_scatter)。\n",
    "    # 对于 Transformer fPEPS (Np ~ 10k-100k)，完全没问题。\n",
    "    total_grads = gather_tensor(my_grads)       # (Total_Ns, Np)\n",
    "\n",
    "    # --- C. 优化步 (Optimization) ---\n",
    "    # 为了数值稳定和计算，我们在 Rank 0 上做 SR 的矩阵求逆\n",
    "    # 其他 Rank 等待广播\n",
    "    \n",
    "    # 1. 计算全局能量平均\n",
    "    E_mean = torch.mean(total_energies)\n",
    "    E_var = torch.var(total_energies)\n",
    "    \n",
    "    # 准备 update 向量容器\n",
    "    dp = torch.zeros(n_params, device=device, dtype=torch.float64)\n",
    "\n",
    "    if RANK == 0:\n",
    "        # SR / MinSR Logic (All GPU)\n",
    "        # log_psi gradients\n",
    "        log_grads = total_grads / total_amps # (Total_Ns, Np)\n",
    "        log_grads_mean = torch.mean(log_grads, dim=0)\n",
    "        \n",
    "        # Centering\n",
    "        O_sk = (log_grads - log_grads_mean.unsqueeze(0)) / np.sqrt(Total_Ns)\n",
    "        E_s = (total_energies - E_mean) / np.sqrt(Total_Ns)\n",
    "        \n",
    "        # SR Matrix T = O * O^dagger\n",
    "        # (Total_Ns, Np) @ (Np, Total_Ns) -> (Total_Ns, Total_Ns)\n",
    "        # 如果 Np < Total_Ns (参数少，样本多)，应该算 (Np, Np) 的协方差矩阵\n",
    "        # 如果 Np > Total_Ns (参数多，样本少)，minSR 算 (Ns, Ns) 的 Gram Matrix\n",
    "        \n",
    "        if minSR:\n",
    "            # T is (Ns, Ns) - usually small\n",
    "            T = O_sk @ O_sk.conj().T\n",
    "            # Pseudo-inverse\n",
    "            T_inv = torch.linalg.pinv(T, rtol=1e-12, hermitian=True)\n",
    "            # dp = O^dagger * T_inv * E_s\n",
    "            # (Np, Ns) @ (Ns, Ns) @ (Ns, ) -> (Np, )\n",
    "            dp = O_sk.conj().T @ (T_inv @ E_s)\n",
    "        else:\n",
    "            # 也可以在这里实现 Iterative Solver (CG/MinRes) using torch.linalg\n",
    "            # 简单起见，这里假设用 minSR\n",
    "            pass\n",
    "\n",
    "        # # For debug gradient, use raw gradient\n",
    "        # dp = torch.einsum('si,s->i', log_grads, total_energies) / Total_Ns - E_mean * torch.mean(log_grads, dim=0)\n",
    "\n",
    "        # 打印信息\n",
    "        print(f\"Step {step}: E = {E_mean.item()/nsites:.6f}, Var = {E_var.item()/nsites**2:.2e}, Std of E_mean = {(E_var.item()/(Total_Ns*nsites**2))**0.5:.2e}\")\n",
    "        print(f'SR dp mean: {dp.mean()}, std: {dp.std()}')\n",
    "\n",
    "    # --- D. 广播更新量 (Broadcast Update) ---\n",
    "    dist.broadcast(dp, src=0)\n",
    "\n",
    "    # --- E. 更新模型参数 ---\n",
    "    # 小技巧：先把 param vector 拿出来，减去 dp，再放回去\n",
    "    current_params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "    new_params_vec = current_params_vec - learning_rate * dp\n",
    "    torch.nn.utils.vector_to_parameters(new_params_vec, fpeps_model.parameters())\n",
    "\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # --- F. Logging (Rank 0 only) ---\n",
    "    if RANK == 0:\n",
    "        # stats['mean'].append(E_mean.item()/nsites)\n",
    "        # stats['error'].append(torch.sqrt(E_var).item()/nsites)\n",
    "        # stats['variance'].append(E_var.item())\n",
    "        \n",
    "        # with open(stats_file, 'w') as f:\n",
    "        #     json.dump(stats, f)\n",
    "            \n",
    "        # if (step + 1) % save_state_every == 0:\n",
    "        #     ckpt_path = pwd + f'/checkpoint_{step+1}.pt'\n",
    "        #     torch.save(fpeps_model.state_dict(), ckpt_path)\n",
    "        \n",
    "        vmc_pbar.update(1)\n",
    "\n",
    "if RANK == 0:\n",
    "    vmc_pbar.close()\n",
    "\n",
    "# 销毁进程组\n",
    "dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5d398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fPEPS-based model number of parameters: 128\n",
      "Exact ground state energy: -0.33005873956798376\n",
      "SU variational energy: 0.6509558133972732\n",
      "Double layer energy: 0.5043600730329295\n",
      "Burn-in sampling time: 1.4283 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VMC steps:  28%|██▊       | 14/50 [00:46<01:59,  3.32s/it]\n",
      "Sampling starts...: 100%|██████████| 1024/1024 [00:27<00:00, 37.66it/s] \n",
      "VMC steps:   2%|▏         | 1/50 [00:01<01:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STEP 0 VMC energy: 0.563015872939573\n",
      "Total sample size: 1024\n",
      "SR dp mean: -0.007137715565171081, std: 0.09208547255690996\n",
      "STEP 0:\n",
      "Energy per site: 0.563015872939573\n",
      "Energy variance square root: 0.040940264297686546\n",
      "Sample size: 1024\n",
      "Time elapsed: 1.3636891200000036 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling starts...: 100%|██████████| 1024/1024 [00:01<00:00, 751.32it/s]\n",
      "\n",
      "VMC steps:   4%|▍         | 2/50 [00:02<01:03,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STEP 1 VMC energy: 0.5598735873216727\n",
      "Total sample size: 1024\n",
      "SR dp mean: 0.0017383671039250574, std: 0.0695445824606592\n",
      "STEP 1:\n",
      "Energy per site: 0.5598735873216727\n",
      "Energy variance square root: 0.03347178868674712\n",
      "Sample size: 1024\n",
      "Time elapsed: 1.3098883119999982 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling starts...: 100%|██████████| 1024/1024 [00:01<00:00, 781.47it/s]\n",
      "VMC steps:   6%|▌         | 3/50 [00:03<01:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "STEP 2 VMC energy: 0.5454575718193018\n",
      "Total sample size: 1024\n",
      "SR dp mean: 0.0015265783079231022, std: 0.06896169801579798\n",
      "STEP 2:\n",
      "Energy per site: 0.5454575718193018\n",
      "Energy variance square root: 0.032293804246041974\n",
      "Sample size: 1024\n",
      "Time elapsed: 1.3127004859999971 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampling starts...: 100%|██████████| 1024/1024 [00:01<00:00, 779.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    144\u001b[39m fxs, current_amps = sample_next(fxs, fpeps_model, graph, seed=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    145\u001b[39m energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m grads_vec, amps = \u001b[43mcompute_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpeps_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m E_loc_vec.append(local_energies.detach().numpy())\n\u001b[32m    150\u001b[39m amps_vec.append(amps.detach().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap/vmap_utils.py:645\u001b[39m, in \u001b[36mcompute_grads\u001b[39m\u001b[34m(fxs, fpeps_model, vectorize, batch_size, verbose)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    644\u001b[39m     t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m     jac_pytree, amps = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_pytree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m     t1 = time.time()\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:690\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    688\u001b[39m     flat_jacobians_per_input = compute_jacobian_preallocate_and_copy()\n\u001b[32m    689\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m     flat_jacobians_per_input = \u001b[43mcompute_jacobian_stacked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;66;03m# Step 2: The returned jacobian is one big tensor per input. In this step,\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# we split each Tensor by output.\u001b[39;00m\n\u001b[32m    694\u001b[39m flat_jacobians_per_input = [\n\u001b[32m    695\u001b[39m     result.split(flat_output_numels, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m flat_jacobians_per_input\n\u001b[32m    697\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:612\u001b[39m, in \u001b[36mjacrev.<locals>.wrapper_fn.<locals>.compute_jacobian_stacked\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    610\u001b[39m     chunked_result = vjp_fn(basis)\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# chunk_size is None or chunk_size != 1\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     chunked_result = \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvjp_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m flat_results = pytree.tree_leaves(chunked_result)\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk_size == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/apis.py:208\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:282\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    272\u001b[39m         func,\n\u001b[32m    273\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         **kwargs,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:432\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    429\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    430\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    431\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:395\u001b[39m, in \u001b[36m_vjp_with_argnums.<locals>.wrapper\u001b[39m\u001b[34m(cotangents, retain_graph, create_graph)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m primals_out_spec != cotangents_spec:\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    390\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected pytree structure of cotangents to be the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    391\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mas pytree structure of outputs to the function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    392\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcotangents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec_pprint(cotangents_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    393\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprimal output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtreespec_pprint(primals_out_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    394\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m result = \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_primals_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_diff_primals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_cotangents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(result, primals_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:142\u001b[39m, in \u001b[36m_autograd_grad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(torch.zeros_like(inp) \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m inputs)\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch._dynamo.compiled_autograd._disable():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     grad_inputs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiff_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m grad_inputs = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m    151\u001b[39m     torch.zeros_like(inp) \u001b[38;5;28;01mif\u001b[39;00m gi \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gi\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m gi, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grad_inputs, inputs)\n\u001b[32m    153\u001b[39m )\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m grad_inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/__init__.py:452\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    450\u001b[39m overridable_args = t_outputs + t_inputs\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(overridable_args):\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43moverridable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_inputs:\n\u001b[32m    467\u001b[39m     warnings.warn(\n\u001b[32m    468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    473\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/overrides.py:1728\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1725\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1726\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1728\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1730\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/__init__.py:503\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    499\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    500\u001b[39m         grad_outputs_\n\u001b[32m    501\u001b[39m     )\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    514\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    515\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    516\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '2'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import symmray as sr\n",
    "import quimb.tensor as qtn\n",
    "import pickle\n",
    "from autoray import do\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from vmap_utils import sample_next, evaluate_energy, compute_grads, random_initial_config\n",
    "from vmap_utils import fPEPS_Model\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "RANK = COMM.Get_rank()\n",
    "SIZE = COMM.Get_size()\n",
    "\n",
    "# torch.set_default_device(\"cuda:0\") # GPU\n",
    "torch.set_default_device(\"cpu\") # CPU\n",
    "torch.random.manual_seed(42 + RANK)\n",
    "\n",
    "Lx = 2\n",
    "Ly = 2\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites  # filling\n",
    "D = 4\n",
    "chi = -1\n",
    "seed = RANK + 42\n",
    "# only the flat backend is compatible with jax.jit\n",
    "flat = True\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap'\n",
    "u1z2 = True\n",
    "appendix = '_U1SU' if u1z2 else ''\n",
    "params = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_su_params{appendix}.pkl', 'rb'))\n",
    "skeleton = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_skeleton{appendix}.pkl', 'rb'))\n",
    "peps = qtn.unpack(params, skeleton)\n",
    "for ts in peps.tensors:\n",
    "    # print(ts.data)\n",
    "    ts.modify(data=ts.data.to_flat()*10)\n",
    "for site in peps.sites:\n",
    "    peps[site].data._label = site\n",
    "    peps[site].data.indices[-1]._linearmap = ((0, 0), (1, 0), (1, 1), (0, 1)) # Important for U1->Z2 fPEPS\n",
    "\n",
    "fpeps_model = fPEPS_Model(\n",
    "    peps, max_bond=chi, dtype=torch.float64\n",
    ")\n",
    "n_params = sum(p.numel() for p in fpeps_model.parameters())\n",
    "if RANK == 0:\n",
    "    # print model size\n",
    "    print(f'fPEPS-based model number of parameters: {n_params}')\n",
    "\n",
    "# generate Hamiltonian graph\n",
    "t=1.0\n",
    "U=8.0\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(\n",
    "    Lx,\n",
    "    Ly,\n",
    "    t,\n",
    "    U,\n",
    "    N_f,\n",
    "    pbc=False,\n",
    "    n_fermions_per_spin=n_fermions_per_spin,\n",
    "    no_u1_symmetry=False,\n",
    ")\n",
    "graph = H.graph\n",
    "\n",
    "if Lx*Ly <= 6 and RANK == 0:\n",
    "    H_dense = torch.tensor(H.to_dense())\n",
    "    psi_vec = fpeps_model(torch.tensor(H.hilbert.all_states(), dtype=torch.int32))\n",
    "    energies_exact, states_exact = torch.linalg.eigh(H_dense)\n",
    "    print(f'Exact ground state energy: {energies_exact[0].item()/nsites}')\n",
    "    SU_E = (psi_vec.conj().T @ H_dense @ psi_vec) / (psi_vec.conj().T @ psi_vec)\n",
    "    print(f'SU variational energy: {SU_E.item()/nsites}')\n",
    "\n",
    "    terms = sr.hamiltonians.ham_fermi_hubbard_from_edges(\n",
    "        \"Z2\",\n",
    "        edges=tuple(peps.gen_bond_coos()),\n",
    "        U=8,\n",
    "        mu=0.0,\n",
    "    )\n",
    "    terms = {k: v.to_flat() for k, v in terms.items()}\n",
    "    new_peps = peps.copy()\n",
    "    new_peps.apply_to_arrays(lambda x: np.array(x))\n",
    "    E_double = new_peps.compute_local_expectation_exact(terms, normalized=True)\n",
    "    print(f'Double layer energy: {E_double/nsites}')\n",
    "\n",
    "\n",
    "# Prepare initial samples\n",
    "Ns = int(1024) # total sample size\n",
    "# batchsize per rank\n",
    "B = 1024\n",
    "B_grad = 10\n",
    "fxs = []\n",
    "for _ in range(B):\n",
    "    fxs.append(random_initial_config(N_f, nsites, seed=None))\n",
    "fxs = torch.stack(fxs)\n",
    "# burn-in for each rank\n",
    "t0 = MPI.Wtime()\n",
    "for _ in range(10):\n",
    "    fxs, current_amps = sample_next(fxs, fpeps_model, graph)\n",
    "t1 = MPI.Wtime()\n",
    "if RANK == 0:\n",
    "    print(f'Burn-in sampling time: {t1-t0:.4f} s')\n",
    "\n",
    "vmc_steps = 50\n",
    "TAG_OFFSET = 424242\n",
    "vmc_pbar = tqdm(total=vmc_steps, desc=\"VMC steps\")\n",
    "minSR=False\n",
    "learning_rate = 0.1\n",
    "\n",
    "stats_file = pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/vmc_mpi_stats_{fpeps_model._get_name()}.json'\n",
    "stats = {\n",
    "    'Np': n_params,\n",
    "    'sample size': Ns,\n",
    "    'mean': [],\n",
    "    'error': [],\n",
    "    'variance': [],\n",
    "}\n",
    "save_state_every = 10\n",
    "\n",
    "for _ in range(vmc_steps):\n",
    "    sample_time = 0\n",
    "    local_energy_time = 0\n",
    "    grad_time = 0\n",
    "    t0 = MPI.Wtime()\n",
    "    message_tag = _\n",
    "    # rank 0 is the master process, receives data and send out signal for stopping\n",
    "    \n",
    "    E_loc_vec = []\n",
    "    amps_vec = []\n",
    "    grads_vec_list = []\n",
    "\n",
    "    n = 0\n",
    "    n_total = 0\n",
    "    # terminate = False\n",
    "    terminate = np.array([0], dtype=np.int32)\n",
    "    if RANK == 0:\n",
    "        pbar = tqdm(total=Ns, desc=\"Sampling starts...\")\n",
    "        fxs, current_amps = sample_next(fxs, fpeps_model, graph, seed=None)\n",
    "        energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "        grads_vec, amps = compute_grads(fxs, fpeps_model, vectorize=True)\n",
    "\n",
    "        \n",
    "        E_loc_vec.append(local_energies.detach().numpy())\n",
    "        amps_vec.append(amps.detach().numpy())\n",
    "        grads_vec_list.append(grads_vec.detach().numpy())\n",
    "\n",
    "        n += fxs.shape[0]\n",
    "        n_total += fxs.shape[0]\n",
    "        pbar.update(fxs.shape[0])\n",
    "\n",
    "    COMM.Barrier()  \n",
    "    # if RANK == 1:\n",
    "    #     print(f'Rank {RANK} B={B}, n_sample={n}\\nSampling time: {sample_time:.4f} s, local energy time: {local_energy_time:.4f} s, grad time: {grad_time:.4f} s')\n",
    "\n",
    "    local_energies = np.concatenate(E_loc_vec)\n",
    "    grads_vec = np.concatenate(grads_vec_list)\n",
    "    amps = np.concatenate(amps_vec)\n",
    "\n",
    "    # use MPI to gather energies and grads from all ranks\n",
    "    all_energies = COMM.allgather(local_energies)\n",
    "    all_energies = np.concatenate(all_energies)\n",
    "    energy = np.mean(all_energies)\n",
    "    energy_var = np.var(all_energies) / all_energies.shape[0]\n",
    "\n",
    "    if RANK == 0:\n",
    "        print(f'\\n\\nSTEP {_} VMC energy: {energy/nsites}')\n",
    "        N_total = all_energies.shape[0]\n",
    "        print(f'Total sample size: {N_total}')\n",
    "\n",
    "    # SR to compute parameter update\n",
    "    if minSR:\n",
    "        all_grads = COMM.gather(grads_vec, root=0) # shape (N_total, Np)\n",
    "        all_amps = COMM.gather(amps, root=0)\n",
    "        if RANK == 0:\n",
    "            all_grads = np.concatenate(all_grads)\n",
    "            all_amps = np.concatenate(all_amps)\n",
    "            all_energies = torch.tensor(all_energies, dtype=torch.float64)\n",
    "            grads_vec = torch.tensor(all_grads, dtype=torch.float64)\n",
    "            amps = torch.tensor(all_amps, dtype=torch.float64)\n",
    "            # Now that we have local energies, amps and per-sample gradients, we can compute the energy gradient\n",
    "            # With the energy gradient, we can further do SR for optimization\n",
    "            # Or we can do minSR, which is simpler here\n",
    "            t0_sr = time.time()\n",
    "            with torch.no_grad():\n",
    "                all_energies_mean = torch.mean(all_energies)\n",
    "                # compute log-derivative grads\n",
    "                all_logamp_grads_vec = grads_vec / amps  # shape (B, Np)\n",
    "                log_grads_vec_mean = torch.mean(all_logamp_grads_vec, dim=0)  # shape (Np,)\n",
    "\n",
    "                O_sk = (all_logamp_grads_vec - log_grads_vec_mean[None, :]) / (N_total**0.5)  # shape (N_total, Np)\n",
    "                T = (O_sk @ O_sk.T.conj())  # shape (N_total, N_total)\n",
    "                E_s = (all_energies - all_energies_mean) / (N_total**0.5)  # shape (N_total,)\n",
    "\n",
    "                # minSR: need to solve O_sk * dp = E_s in the least square sense, using the pseudo-inverse of O_sk to get the minimum norm solution\n",
    "                T_inv = torch.linalg.pinv(T,  rtol=1e-12, atol=0, hermitian=True)\n",
    "                dp = O_sk.conj().T @ (T_inv @ E_s)  # shape (Np,)\n",
    "            print(f'MinSR dp mean: {dp.mean()}, std: {dp.std()}')\n",
    "        \n",
    "    else:\n",
    "        # SR with iterative minres solver\n",
    "        local_logamp_grads_vec = grads_vec / amps  # shape (n, Np)\n",
    "        local_logamp_grads_vec_sum = np.sum(local_logamp_grads_vec, axis=0)  # shape (Np,)\n",
    "        local_E_logamp_grads_vec_sum = np.dot(local_energies, local_logamp_grads_vec)  # shape (Np,)\n",
    "        n_local = local_energies.shape[0]\n",
    "        N_total = COMM.allreduce(n_local, op=MPI.SUM)\n",
    "\n",
    "        logamp_grads_vec_sum = COMM.allgather(local_logamp_grads_vec_sum)\n",
    "        E_logamp_grads_vec_sum = COMM.allgather(local_E_logamp_grads_vec_sum)\n",
    "\n",
    "        logamp_grads_vec_sum = np.array(logamp_grads_vec_sum)  # shape (SIZE, Np)\n",
    "        E_logamp_grads_vec_sum = np.array(E_logamp_grads_vec_sum)\n",
    "\n",
    "        logamp_grads_vec_mean = np.sum(logamp_grads_vec_sum, axis=0) / N_total # shape (Np,)\n",
    "        E_logamp_grads_vec_mean = np.sum(E_logamp_grads_vec_sum, axis=0) / N_total  # shape (Np,)\n",
    "        \n",
    "        energy_grad = E_logamp_grads_vec_mean - energy * logamp_grads_vec_mean  # shape (Np,)\n",
    "        \n",
    "        def R_dot_x(x, eta=1e-6):\n",
    "            x_out_local = np.zeros_like(x)\n",
    "            # use matrix multiplication for speedup\n",
    "            x_out_local = do('dot', local_logamp_grads_vec.T, do('dot', local_logamp_grads_vec, x))\n",
    "            # synchronize the result\n",
    "            x_out = COMM.allreduce(x_out_local, op=MPI.SUM)/N_total\n",
    "            x_out -= do('dot', logamp_grads_vec_mean, x)*logamp_grads_vec_mean\n",
    "            return x_out + eta*x\n",
    "        \n",
    "        import scipy.sparse.linalg as spla\n",
    "        def matvec(x):\n",
    "            return R_dot_x(x, 1e-4)\n",
    "        A = spla.LinearOperator((n_params, n_params), matvec=matvec)\n",
    "        b = energy_grad\n",
    "        dp, info = spla.minres(A, b, rtol=1e-4, maxiter=100)\n",
    "        dp = energy_grad  # simple gradient descent without SR for debug\n",
    "        print(f'SR dp mean: {np.mean(dp)}, std: {np.std(dp)}')\n",
    "        \n",
    "    if RANK == 0:\n",
    "        # update params\n",
    "        params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "\n",
    "        new_params_vec = params_vec - learning_rate * torch.tensor(dp, dtype=torch.float64)\n",
    "    \n",
    "    COMM.Barrier()\n",
    "    \n",
    "    # broadcast the new params to all ranks\n",
    "    new_params_vec = COMM.bcast(new_params_vec if RANK == 0 else None, root=0)\n",
    "    # print(f'Rank {RANK} received new params vector of shape: {new_params_vec.shape}')\n",
    "\n",
    "    # load the new params back to the model\n",
    "    torch.nn.utils.vector_to_parameters(new_params_vec, fpeps_model.parameters())\n",
    "\n",
    "    vmc_pbar.update(1)\n",
    "    t1 = MPI.Wtime()\n",
    "    if RANK == 0:\n",
    "        # save step, energy, energy variance to a file (if exists, delete and create a new one)\n",
    "        log_file = pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/vmc_mpi_log_{fpeps_model._get_name()}.txt'\n",
    "        print(f'STEP {_}:\\nEnergy per site: {energy/nsites}\\nEnergy variance square root: {np.sqrt(energy_var)/nsites}\\nSample size: {N_total}\\nTime elapsed: {t1 - t0} seconds\\n\\n')\n",
    "\n",
    "\n",
    "vmc_pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
