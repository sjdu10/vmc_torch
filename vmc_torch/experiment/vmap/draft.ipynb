{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "98cde5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import numpy as np\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "Lx = 4\n",
    "Ly = 2\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites  # half-filling\n",
    "D = 4\n",
    "chi = 2*D\n",
    "seed = 42\n",
    "# only the flat backend is compatible with jax.jit\n",
    "flat = True\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap'\n",
    "\n",
    "# params = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_su_params.pkl', 'rb'))\n",
    "# skeleton = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_skeleton.pkl', 'rb'))\n",
    "# peps = qtn.unpack(params, skeleton)\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 1),  # linear index 1 -> charge 1, offset 1\n",
    "        (1, 0),  # linear index 2 -> charge 1, offset 0\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],\n",
    "    subsizes=\"equal\",\n",
    "    flat=flat,\n",
    "    seed=seed,\n",
    ")\n",
    "# peps.set_params(params)\n",
    "\n",
    "# get pytree of initial parameters, and reference tn structure\n",
    "params, skeleton = qtn.pack(peps)\n",
    "\n",
    "\n",
    "def amplitude(x, params):\n",
    "    tn = qtn.unpack(params, skeleton)\n",
    "    # might need to specify the right site ordering here\n",
    "    amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "    amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "    amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "    return amp.contract()\n",
    "\n",
    "\n",
    "# generate half-filling configs\n",
    "# batchsize\n",
    "B = 256\n",
    "rng = np.random.default_rng(seed)\n",
    "xs_u = np.concatenate(\n",
    "    [\n",
    "        np.zeros((B, nsites // 2), dtype=np.int32),\n",
    "        np.ones((B, nsites // 2), dtype=np.int32),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "xs_d = xs_u.copy()\n",
    "xs_u = rng.permuted(xs_u, axis=1)\n",
    "xs_d = rng.permuted(xs_d, axis=1)\n",
    "xs = np.concatenate([xs_u[:, :, None], xs_d[:, :, None]], axis=2).reshape(B, -1)\n",
    "fxs = 2 * xs[:, ::2] + xs[:, 1::2]  # map to 0,1,2,3\n",
    "\n",
    "# torch.set_default_device(\"cuda:0\") # GPU\n",
    "torch.set_default_device(\"cpu\") # CPU\n",
    "\n",
    "# convert bitstrings and arrays to torch\n",
    "fxs = torch.tensor(fxs)\n",
    "params = qu.tree_map(\n",
    "    lambda x: torch.tensor(x, dtype=torch.float64),\n",
    "    params,\n",
    ")\n",
    "\n",
    "vamp = torch.vmap(\n",
    "    amplitude,\n",
    "    # batch on configs, not parameters\n",
    "    in_dims=(0, None),\n",
    ")\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, tn, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        params, skeleton = qtn.pack(tn)\n",
    "        self.dtype = dtype\n",
    "        self.skeleton = skeleton\n",
    "        self.params = nn.ParameterDict(\n",
    "            {\n",
    "                str(k): nn.Parameter(torch.tensor(v, dtype=self.dtype))\n",
    "                for k, v in params.items()\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def params_as_dict(self):\n",
    "        return {int(k): v for k, v in self.params.items()}\n",
    "    \n",
    "    def amplitude(self, x, params):\n",
    "        tn = qtn.unpack(params, self.skeleton)\n",
    "        # might need to specify the right site ordering here\n",
    "        amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "        amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "        amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "        return amp.contract()\n",
    "    \n",
    "    def vamp(self, x, params):\n",
    "        params = {int(k): v for k, v in params.items()}\n",
    "        return torch.vmap(\n",
    "            self.amplitude,\n",
    "            in_dims=(0, None),\n",
    "        )(x, params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vamp(x, self.params)\n",
    "\n",
    "fpeps_model = SimpleModel(peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "1ace5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def propose_exchange_or_hopping(i, j, current_config, hopping_rate=0.25):\n",
    "    ind_n_map = {0: 0, 1: 1, 2: 1, 3: 2}\n",
    "    if current_config[i] == current_config[j]:\n",
    "        return current_config, 0\n",
    "    proposed_config = current_config.clone()\n",
    "    config_i = current_config[i].item()\n",
    "    config_j = current_config[j].item()\n",
    "    if random.random() < 1 - hopping_rate:\n",
    "        # exchange\n",
    "        proposed_config[i] = config_j\n",
    "        proposed_config[j] = config_i\n",
    "    else:\n",
    "        # hopping\n",
    "        n_i = ind_n_map[current_config[i].item()]\n",
    "        n_j = ind_n_map[current_config[j].item()]\n",
    "        delta_n = abs(n_i - n_j)\n",
    "        if delta_n == 1:\n",
    "            # consider only valid hopping: (0, u) -> (u, 0); (d, ud) -> (ud, d)\n",
    "            proposed_config[i] = config_j\n",
    "            proposed_config[j] = config_i\n",
    "        elif delta_n == 0:\n",
    "            # consider only valid hopping: (u, d) -> (0, ud) or (ud, 0)\n",
    "            choices = [(0, 3), (3, 0)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        elif delta_n == 2:\n",
    "            # consider only valid hopping: (0, ud) -> (u, d) or (d, u)\n",
    "            choices = [(1, 2), (2, 1)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid configuration\")\n",
    "    return proposed_config, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "bf9cf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "# generate Hamiltonian graph\n",
    "t=1.0\n",
    "U=8.0\n",
    "N_f = int(Lx*Ly) # half-filling\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a5b92",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "df5dac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched Metropolis-Hastings updates\n",
    "import time\n",
    "\n",
    "def sample_next(fxs, fpeps_model, graph):\n",
    "    current_amps = fpeps_model(fxs)\n",
    "    for row, edges in graph.row_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            # t0 = time.time()\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            # t1 = time.time()\n",
    "            # print(f\"Propose time: {t1 - t0}\")\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "\n",
    "    for col, edges in graph.col_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "    \n",
    "    return fxs, current_amps\n",
    "\n",
    "fxs, current_amps = sample_next(fxs, fpeps_model, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "187bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential Metropolis-Hastings updates\n",
    "# current_amps = torch.stack([amplitude(fx, params) for fx in fxs], dim=0)\n",
    "# for row, edges in graph.row_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx, new = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params) if new == 1 else current_amps[k]\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp\n",
    "# for col, edges in graph.col_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params)\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067fc0f",
   "metadata": {},
   "source": [
    "Local Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "a4f44218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared batched conn_etas and coeffs: torch.Size([3481, 8]), torch.Size([3481]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.97394069216591\n"
     ]
    }
   ],
   "source": [
    "def evaluate_energy(fxs, fpeps_model, H, current_amps):\n",
    "    B = fxs.shape[0]\n",
    "    # get connected configurations and coefficients\n",
    "    conn_eta_num = []\n",
    "    conn_etas = []\n",
    "    conn_eta_coeffs = []\n",
    "    for fx in fxs:\n",
    "        eta, coeffs = H.get_conn(fx)\n",
    "        conn_eta_num.append(len(eta))\n",
    "        conn_etas.append(torch.tensor(eta))\n",
    "        conn_eta_coeffs.append(torch.tensor(coeffs))\n",
    "\n",
    "    conn_etas = torch.cat(conn_etas, dim=0)\n",
    "    conn_eta_coeffs = torch.cat(conn_eta_coeffs, dim=0)\n",
    "\n",
    "    print(f'Prepared batched conn_etas and coeffs: {conn_etas.shape}, {conn_eta_coeffs.shape} (batch size {B})')\n",
    "\n",
    "    # calculate amplitudes for connected configs, in the future consider TN reuse to speed up calculation, TN reuse is controlled by a param that is not batched over (control flow?)\n",
    "    conn_amps = fpeps_model(conn_etas)\n",
    "\n",
    "    # Local energy \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "\n",
    "    local_energies = []\n",
    "    offset = 0\n",
    "    for b in range(B):\n",
    "        n_conn = conn_eta_num[b]\n",
    "        amps_ratio = conn_amps[offset:offset+n_conn] / current_amps[b]\n",
    "        energy_b = torch.sum(conn_eta_coeffs[offset:offset+n_conn] * amps_ratio)\n",
    "        local_energies.append(energy_b)\n",
    "        offset += n_conn\n",
    "    local_energies = torch.stack(local_energies, dim=0)\n",
    "    print(f'Batched local energies: {local_energies.shape}')\n",
    "\n",
    "    # Energy: (1/N) * \\sum_s <s|H|psi>/<s|psi> = (1/N) * \\sum_s \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "    energy = torch.mean(local_energies)\n",
    "    print(f'Energy: {energy.item()}')\n",
    "\n",
    "    return energy, local_energies\n",
    "\n",
    "energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d739ba4",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "daa6bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential jacobian time: 3.262303113937378\n",
      "torch.Size([256, 640]) torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "def compute_grads(fxs, fpeps_model, vectorize=True):\n",
    "    if vectorize:\n",
    "        # Vectorized gradient calculation\n",
    "        # need per sample gradient of amplitude -- jacobian\n",
    "        params_pytree = dict(fpeps_model.params)\n",
    "        # params is a pytree, fxs has shape (B, nsites)\n",
    "        def g(x, p):\n",
    "            results = fpeps_model.vamp(x, p)\n",
    "            return results, results\n",
    "        t0 = time.time()\n",
    "        jac_pytree, amps = torch.func.jacrev(g, argnums=1, has_aux=True)(fxs, params_pytree)\n",
    "        t1 = time.time()\n",
    "        print(f\"Vectorized jacobian time: {t1 - t0}\")\n",
    "        # jac_pytree has shape same as params_pytree, each leaf has shape (B, )\n",
    "\n",
    "        # Get per-sample batched grads in list of dicts format\n",
    "        batched_grads_vec = []\n",
    "        for b in range(B):\n",
    "            grad_b_iter = [jac_pytree[k][b] for k in jac_pytree.keys()]\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grad_b_iter))\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)  # shape (B, Np), Np is number of parameters\n",
    "\n",
    "        return batched_grads_vec, amps\n",
    "    \n",
    "    else:\n",
    "        # Sequential non-vectorized gradient calculation\n",
    "        amps = []\n",
    "        batched_grads_vec = []\n",
    "        t0 = time.time()\n",
    "        for fx in fxs:\n",
    "            amp = fpeps_model(fx.unsqueeze(0))\n",
    "            amps.append(amp)\n",
    "            amp.backward()\n",
    "            grads = qu.tree_map(lambda x: x.grad, fpeps_model.params_as_dict())\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grads.values()))\n",
    "            qu.tree_map(lambda x: x.grad.zero_(), fpeps_model.params_as_dict())\n",
    "        t1 = time.time()\n",
    "        print(f\"Sequential jacobian time: {t1 - t0}\")\n",
    "        amps = torch.stack(amps, dim=0)\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)\n",
    "        return batched_grads_vec, amps\n",
    "\n",
    "            \n",
    "batched_grads_vec, amps = compute_grads(fxs, fpeps_model, vectorize=False)\n",
    "print(batched_grads_vec.shape, amps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f8ac4",
   "metadata": {},
   "source": [
    "VMC update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "a396056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared batched conn_etas and coeffs: torch.Size([3481, 8]), torch.Size([3481]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.585925775616275\n"
     ]
    }
   ],
   "source": [
    "# Now that we have local energies, amps and per-sample gradients, we can compute the energy gradient\n",
    "# With the energy gradient, we can further do SR for optimization\n",
    "# Or we can do minSR, which is simpler here\n",
    "with torch.no_grad():\n",
    "    local_energies # shape (B,)\n",
    "    local_energies_mean = torch.mean(local_energies)\n",
    "    amps # shape (B,)\n",
    "    params # pytree with each leaf of shape (param_shape...)\n",
    "\n",
    "    # flatten the model params into a 1d vector of shape (Np,)\n",
    "    params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "\n",
    "    # compute log-derivative grads\n",
    "    batched_log_grads_vec = batched_grads_vec / amps  # shape (B, Np)\n",
    "    log_grads_vec_mean = torch.mean(batched_log_grads_vec, dim=0)  # shape (Np,)\n",
    "\n",
    "    O_sk = (batched_log_grads_vec - log_grads_vec_mean[None, :]) / (B**0.5)  # shape (B, Np)\n",
    "    T = (O_sk @ O_sk.T.conj())  # shape (B, B)\n",
    "    E_s = (local_energies - local_energies_mean) / (B**0.5)  # shape (B,)\n",
    "\n",
    "    # minSR: need to solve O_sk * dp = E_s in the least square sense, using the pseudo-inverse of O_sk to get the minimum norm solution\n",
    "    T_inv = torch.linalg.pinv(T,  rtol=1e-12, atol=0, hermitian=True)\n",
    "    dp = O_sk.conj().T @ (T_inv @ E_s)  # shape (Np,)\n",
    "    # update params\n",
    "    learning_rate = 0.01\n",
    "    new_params_vec = params_vec - learning_rate * dp\n",
    "\n",
    "# load the new params back to the model\n",
    "torch.nn.utils.vector_to_parameters(new_params_vec, fpeps_model.parameters())\n",
    "\n",
    "energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
