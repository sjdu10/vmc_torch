{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98cde5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n=20, D=4, tau=0.1, max|dS|=0.0136, energyâ‰ˆ-4.2385: 100%|##########| 20/20 [00:00<00:00, 33.66it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "import numpy as np\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from vmc_torch.fermion_utils import fPEPS\n",
    "\n",
    "Lx = 2\n",
    "Ly = 2\n",
    "nsites = Lx * Ly\n",
    "N_f = nsites  # half-filling\n",
    "D = 4\n",
    "chi = 2*D\n",
    "seed = 42\n",
    "# only the flat backend is compatible with jax.jit\n",
    "flat = True\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/vmc_torch/experiment/vmap'\n",
    "# params = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_su_params.pkl', 'rb'))\n",
    "# skeleton = pickle.load(open(pwd+f'/{Lx}x{Ly}/t=1.0_U=8.0/N={N_f}/Z2/D={D}/peps_skeleton.pkl', 'rb'))\n",
    "# peps = qtn.unpack(params, skeleton)\n",
    "peps = sr.networks.PEPS_fermionic_rand(\n",
    "    \"Z2\",\n",
    "    Lx,\n",
    "    Ly,\n",
    "    D,\n",
    "    phys_dim=[\n",
    "        (0, 0),  # linear index 0 -> charge 0, offset 0\n",
    "        (1, 1),  # linear index 1 -> charge 1, offset 1\n",
    "        (1, 0),  # linear index 2 -> charge 1, offset 0\n",
    "        (0, 1),  # linear index 3 -> charge 0, offset 1\n",
    "    ],\n",
    "    subsizes=\"equal\",\n",
    "    flat=flat,\n",
    "    seed=seed,\n",
    ")\n",
    "original_peps = peps.copy()\n",
    "\n",
    "# SU\n",
    "terms = sr.ham_fermi_hubbard_from_edges(\n",
    "    symmetry='Z2',\n",
    "    edges=tuple(peps.gen_bond_coos()),\n",
    "    t=1.0,\n",
    "    U=8.0,\n",
    "    mu=4.0,\n",
    ")\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "ham.apply_to_arrays(lambda A: A.to_flat())\n",
    "su = qtn.SimpleUpdateGen(\n",
    "    peps,\n",
    "    ham,\n",
    "    # setting a cutoff is important to turn on dynamic charge sectors\n",
    "    # cutoff=1e-12,\n",
    "    cutoff=0.0,\n",
    "    second_order_reflect=True,\n",
    "    # SimpleUpdateGen computes cluster energies by default\n",
    "    # which might not be accurate\n",
    "    compute_energy_every=10,\n",
    "    compute_energy_opts=dict(max_distance=1),\n",
    "    compute_energy_per_site=True,\n",
    "    # use a fixed trotterization order\n",
    "    ordering=\"sort\",\n",
    "    # if the gauge difference drops below this, we consider the PEPS converged\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "# run the evolution, these are reasonable defaults\n",
    "tau = 0.1\n",
    "steps = 20\n",
    "su.evolve(steps, tau=tau)\n",
    "peps = su.get_state()\n",
    "for i in range(len(peps.arrays)):\n",
    "    peps.arrays[i]._label = original_peps.arrays[i].label\n",
    "\n",
    "# get pytree of initial parameters, and reference tn structure\n",
    "params, skeleton = qtn.pack(peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cf6b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpeps = peps.copy()\n",
    "for ts in fpeps.tensors:\n",
    "    ts.modify(data=ts.data.to_blocksparse())\n",
    "\n",
    "fpeps = fPEPS(fpeps)\n",
    "fpeps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb6ed9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.4924e-06], dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def amplitude(x, params):\n",
    "    tn = qtn.unpack(params, skeleton)\n",
    "    # might need to specify the right site ordering here\n",
    "    amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "    amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "    amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "    return amp.contract()\n",
    "\n",
    "\n",
    "# generate half-filling configs\n",
    "# batchsize\n",
    "B = 256\n",
    "rng = np.random.default_rng(seed)\n",
    "xs_u = np.concatenate(\n",
    "    [\n",
    "        np.zeros((B, nsites // 2), dtype=np.int32),\n",
    "        np.ones((B, nsites // 2), dtype=np.int32),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "xs_d = xs_u.copy()\n",
    "xs_u = rng.permuted(xs_u, axis=1)\n",
    "xs_d = rng.permuted(xs_d, axis=1)\n",
    "xs = np.concatenate([xs_u[:, :, None], xs_d[:, :, None]], axis=2).reshape(B, -1)\n",
    "fxs = 2 * xs[:, ::2] + xs[:, 1::2]  # map to 0,1,2,3\n",
    "\n",
    "# torch.set_default_device(\"cuda:0\") # GPU\n",
    "torch.set_default_device(\"cpu\") # CPU\n",
    "\n",
    "# convert bitstrings and arrays to torch\n",
    "fxs = torch.tensor(fxs)\n",
    "params = qu.tree_map(\n",
    "    lambda x: torch.tensor(x, dtype=torch.float64),\n",
    "    params,\n",
    ")\n",
    "\n",
    "vamp = torch.vmap(\n",
    "    amplitude,\n",
    "    # batch on configs, not parameters\n",
    "    in_dims=(0, None),\n",
    ")\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, tn, dtype=torch.float64):\n",
    "        super().__init__()\n",
    "        params, skeleton = qtn.pack(tn)\n",
    "        self.dtype = dtype\n",
    "        self.skeleton = skeleton\n",
    "        self.params = nn.ParameterDict(\n",
    "            {\n",
    "                str(k): nn.Parameter(torch.tensor(v['blocks'], dtype=self.dtype))\n",
    "                for k, v in params.items()\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def params_as_dict(self):\n",
    "        return {int(k): v for k, v in self.params.items()}\n",
    "    \n",
    "    def amplitude(self, x, params):\n",
    "        tn = qtn.unpack(params, self.skeleton)\n",
    "        # might need to specify the right site ordering here\n",
    "        amp = tn.isel({tn.site_ind(site): x[i] for i, site in enumerate(tn.sites)})\n",
    "        amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "        amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "        return amp.contract()\n",
    "    \n",
    "    def vamp(self, x, params):\n",
    "        params = {int(k): {'blocks':v} for k, v in params.items()}\n",
    "        return torch.vmap(\n",
    "            self.amplitude,\n",
    "            in_dims=(0, None),\n",
    "        )(x, params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vamp(x, self.params)\n",
    "\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float64))\n",
    "fpeps_model = SimpleModel(peps)\n",
    "fpeps_model(fxs[0].unsqueeze(0))  # warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ace5dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def propose_exchange_or_hopping(i, j, current_config, hopping_rate=0.25):\n",
    "    ind_n_map = {0: 0, 1: 1, 2: 1, 3: 2}\n",
    "    if current_config[i] == current_config[j]:\n",
    "        return current_config, 0\n",
    "    proposed_config = current_config.clone()\n",
    "    config_i = current_config[i].item()\n",
    "    config_j = current_config[j].item()\n",
    "    if random.random() < 1 - hopping_rate:\n",
    "        # exchange\n",
    "        proposed_config[i] = config_j\n",
    "        proposed_config[j] = config_i\n",
    "    else:\n",
    "        # hopping\n",
    "        n_i = ind_n_map[current_config[i].item()]\n",
    "        n_j = ind_n_map[current_config[j].item()]\n",
    "        delta_n = abs(n_i - n_j)\n",
    "        if delta_n == 1:\n",
    "            # consider only valid hopping: (0, u) -> (u, 0); (d, ud) -> (ud, d)\n",
    "            proposed_config[i] = config_j\n",
    "            proposed_config[j] = config_i\n",
    "        elif delta_n == 0:\n",
    "            # consider only valid hopping: (u, d) -> (0, ud) or (ud, 0)\n",
    "            choices = [(0, 3), (3, 0)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        elif delta_n == 2:\n",
    "            # consider only valid hopping: (0, ud) -> (u, d) or (d, u)\n",
    "            choices = [(1, 2), (2, 1)]\n",
    "            choice = random.choice(choices)\n",
    "            proposed_config[i] = choice[0]\n",
    "            proposed_config[j] = choice[1]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid configuration\")\n",
    "    return proposed_config, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a5b92",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df5dac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched Metropolis-Hastings updates\n",
    "import time\n",
    "\n",
    "def sample_next(fxs, fpeps_model, graph):\n",
    "    current_amps = fpeps_model(fxs)\n",
    "    for row, edges in graph.row_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            # t0 = time.time()\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            # t1 = time.time()\n",
    "            # print(f\"Propose time: {t1 - t0}\")\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "\n",
    "    for col, edges in graph.col_edges.items():\n",
    "        for edge in edges:\n",
    "            i, j = edge\n",
    "            proposed_fxs = []\n",
    "            new_flags = []\n",
    "            for fx in fxs:\n",
    "                proposed_fx, new = propose_exchange_or_hopping(i, j, fx)\n",
    "                proposed_fxs.append(proposed_fx)\n",
    "                new_flags.append(new)\n",
    "            proposed_fxs = torch.stack(proposed_fxs, dim=0)\n",
    "            # only compute amplitudes for newly proposed configs\n",
    "            new_proposed_fxs = proposed_fxs[torch.tensor(new_flags, dtype=torch.bool)]\n",
    "            new_proposed_amps = fpeps_model(new_proposed_fxs)\n",
    "            # print(f\"Number of new proposals: {new_proposed_amps.shape[0]} ({B})\" )\n",
    "            proposed_amps = current_amps.clone()\n",
    "            proposed_amps[torch.tensor(new_flags, dtype=torch.bool)] = new_proposed_amps\n",
    "            ratio = proposed_amps**2 / current_amps**2\n",
    "            accept_prob = torch.minimum(ratio, torch.ones_like(ratio))\n",
    "            for k in range(B):\n",
    "                if random.random() < accept_prob[k].item():\n",
    "                    fxs[k] = proposed_fxs[k]\n",
    "                    current_amps[k] = proposed_amps[k]\n",
    "    \n",
    "    return fxs, current_amps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "187bcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sequential Metropolis-Hastings updates\n",
    "# current_amps = torch.stack([amplitude(fx, params) for fx in fxs], dim=0)\n",
    "# for row, edges in graph.row_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx, new = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params) if new == 1 else current_amps[k]\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp\n",
    "# for col, edges in graph.col_edges.items():\n",
    "#     for edge in edges:\n",
    "#         i, j = edge\n",
    "#         for k in range(B):\n",
    "#             proposed_fx = propose_exchange_or_hopping(i, j, fxs[k])\n",
    "#             proposed_amp = amplitude(proposed_fx, params)\n",
    "#             ratio = (proposed_amp**2) / (current_amps[k]**2)\n",
    "#             accept_prob = min(ratio.item(), 1.0)\n",
    "#             if random.random() < accept_prob:\n",
    "#                 fxs[k] = proposed_fx\n",
    "#                 current_amps[k] = proposed_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2067fc0f",
   "metadata": {},
   "source": [
    "Local Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a4f44218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_energy(fxs, fpeps_model, H, current_amps):\n",
    "    B = fxs.shape[0]\n",
    "    # get connected configurations and coefficients\n",
    "    conn_eta_num = []\n",
    "    conn_etas = []\n",
    "    conn_eta_coeffs = []\n",
    "    for fx in fxs:\n",
    "        eta, coeffs = H.get_conn(fx)\n",
    "        conn_eta_num.append(len(eta))\n",
    "        conn_etas.append(torch.tensor(eta))\n",
    "        conn_eta_coeffs.append(torch.tensor(coeffs))\n",
    "\n",
    "    conn_etas = torch.cat(conn_etas, dim=0)\n",
    "    conn_eta_coeffs = torch.cat(conn_eta_coeffs, dim=0)\n",
    "\n",
    "    print(f'Prepared batched conn_etas and coeffs: {conn_etas.shape}, {conn_eta_coeffs.shape} (batch size {B})')\n",
    "\n",
    "    # calculate amplitudes for connected configs, in the future consider TN reuse to speed up calculation, TN reuse is controlled by a param that is not batched over (control flow?)\n",
    "    conn_amps = fpeps_model(conn_etas)\n",
    "\n",
    "    # Local energy \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "\n",
    "    local_energies = []\n",
    "    offset = 0\n",
    "    for b in range(B):\n",
    "        n_conn = conn_eta_num[b]\n",
    "        amps_ratio = conn_amps[offset:offset+n_conn] / current_amps[b]\n",
    "        energy_b = torch.sum(conn_eta_coeffs[offset:offset+n_conn] * amps_ratio)\n",
    "        local_energies.append(energy_b)\n",
    "        offset += n_conn\n",
    "    local_energies = torch.stack(local_energies, dim=0)\n",
    "    print(f'Batched local energies: {local_energies.shape}')\n",
    "\n",
    "    # Energy: (1/N) * \\sum_s <s|H|psi>/<s|psi> = (1/N) * \\sum_s \\sum_{s'} H_{s,s'} <s'|psi>/<s|psi>\n",
    "    energy = torch.mean(local_energies)\n",
    "    print(f'Energy: {energy.item()}')\n",
    "\n",
    "    return energy, local_energies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d739ba4",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "daa6bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(fxs, fpeps_model, vectorize=True):\n",
    "    if vectorize:\n",
    "        # Vectorized gradient calculation\n",
    "        # need per sample gradient of amplitude -- jacobian\n",
    "        params_pytree = dict(fpeps_model.params)\n",
    "        # params is a pytree, fxs has shape (B, nsites)\n",
    "        def g(x, p):\n",
    "            results = fpeps_model.vamp(x, p)\n",
    "            return results, results\n",
    "        t0 = time.time()\n",
    "        jac_pytree, amps = torch.func.jacrev(g, argnums=1, has_aux=True)(fxs, params_pytree)\n",
    "        t1 = time.time()\n",
    "        print(f\"Vectorized jacobian time: {t1 - t0}\")\n",
    "        # jac_pytree has shape same as params_pytree, each leaf has shape (B, )\n",
    "\n",
    "        # Get per-sample batched grads in list of dicts format\n",
    "        batched_grads_vec = []\n",
    "        for b in range(B):\n",
    "            grad_b_iter = [jac_pytree[k][b] for k in jac_pytree.keys()]\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grad_b_iter))\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)  # shape (B, Np), Np is number of parameters\n",
    "        amps.unsqueeze_(1)  # shape (B, 1)\n",
    "        return batched_grads_vec, amps\n",
    "    \n",
    "    else:\n",
    "        # Sequential non-vectorized gradient calculation\n",
    "        amps = []\n",
    "        batched_grads_vec = []\n",
    "        t0 = time.time()\n",
    "        for fx in fxs:\n",
    "            amp = fpeps_model(fx.unsqueeze(0))\n",
    "            amps.append(amp)\n",
    "            amp.backward()\n",
    "            grads = qu.tree_map(lambda x: x.grad, fpeps_model.params_as_dict())\n",
    "            batched_grads_vec.append(torch.nn.utils.parameters_to_vector(grads.values()))\n",
    "            qu.tree_map(lambda x: x.grad.zero_(), fpeps_model.params_as_dict())\n",
    "        t1 = time.time()\n",
    "        print(f\"Sequential jacobian time: {t1 - t0}\")\n",
    "        amps = torch.stack(amps, dim=0)\n",
    "        batched_grads_vec = torch.stack(batched_grads_vec, dim=0)\n",
    "        return batched_grads_vec, amps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0847c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "from vmc_torch.fermion_utils import from_quimb_config_to_netket_config, from_netket_config_to_quimb_config, from_netket_config_to_quimb_binary_config, from_quimb_biniary_config_to_netket_config\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "# generate Hamiltonian graph\n",
    "t=1.0\n",
    "U=8.0\n",
    "N_f = int(Lx*Ly) # half-filling\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=None, no_u1_symmetry=True)\n",
    "graph = H.graph\n",
    "print(len(H.hilbert.all_states()))\n",
    "\n",
    "fxs_nk = from_quimb_config_to_netket_config(fxs)\n",
    "fxs_quimb_binary = from_netket_config_to_quimb_binary_config(fxs_nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "258d759a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3, 2, 0, 1],\n",
       "        [0, 2, 3, 1],\n",
       "        [2, 3, 1, 0],\n",
       "        [2, 0, 1, 3]]),\n",
       " array([-1., -1., -1., -1.]),\n",
       " HilbertSpace(nsites=8, total_size=128, symmetry=Z2, sector=0),\n",
       " (array([[0, 2, 3, 1],\n",
       "         [3, 2, 0, 1],\n",
       "         [2, 0, 1, 3],\n",
       "         [2, 3, 1, 0]]),\n",
       "  array([-1., -1., -1., -1.])),\n",
       " tensor([2, 2, 1, 1], dtype=torch.int32))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quimb.experimental.operatorbuilder as qop\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "sites = [(i, j) for i in range(Lx) for j in range(Ly)]\n",
    "H_quimb = qop.fermi_hubbard_from_edges(\n",
    "    edges,\n",
    "    U=8,\n",
    "    mu=0,\n",
    "    # this ordering pairs spins together, as with the fermionic TN\n",
    "    order=lambda site: (site[1], site[0]),\n",
    "    sector=int(sum(ary.charge for ary in peps.arrays) % 2),\n",
    "    symmetry=\"Z2\",\n",
    ")\n",
    "# H_quimb.flatconfig_coupling(fxs[0])\n",
    "hs = H_quimb.hilbert_space\n",
    "eta_quimb, eta_coeff_quimb = H_quimb.flatconfig_coupling(fxs_quimb_binary[0])\n",
    "from_netket_config_to_quimb_config(from_quimb_biniary_config_to_netket_config(eta_quimb)), eta_coeff_quimb, hs, H.get_conn(fxs[0]), fxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bff4f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params, peps = qtn.pack(peps)\n",
    "# peps = qtn.unpack(params, skeleton)\n",
    "def amplitude_binary(x):\n",
    "    x = 2 * x[::2] + x[1::2]\n",
    "    x = torch.tensor(x, dtype=torch.int64)\n",
    "    # might need to specify the right site ordering here\n",
    "    amp = peps.isel({peps.site_ind(site): x[i] for i, site in enumerate(peps.sites)})\n",
    "    amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=0.0, yrange=[0, amp.Ly//2-1])\n",
    "    amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=0.0, yrange=[amp.Ly//2, amp.Ly-1])\n",
    "    return amp.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5db4cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense energy: 3.648023928988014, -0.23850422302929083\n",
      "MC energy: 3.648023392066006\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    H_dense = torch.tensor(H.to_dense())\n",
    "    all_states = torch.tensor(H.hilbert.all_states())\n",
    "    psi_vec = fpeps_model(all_states)\n",
    "    fpsi_vec = torch.tensor([fpeps.get_amp(fx).contract().item() for fx in all_states], dtype=torch.float64)\n",
    "\n",
    "    E = (psi_vec @ H_dense @ psi_vec)/(psi_vec @ psi_vec)\n",
    "    E1 = (fpsi_vec @ H_dense @ fpsi_vec)/(fpsi_vec @ fpsi_vec)\n",
    "print(f'Dense energy: {E.item() / Lx / Ly}, {E1.item() / Lx / Ly}')\n",
    "\n",
    "# compute the full exact energy at the amplitude level\n",
    "O = 0.0\n",
    "p = 0.0\n",
    "\n",
    "fcs = []\n",
    "for i in range(hs.size):\n",
    "    fx = hs.rank_to_flatconfig(i)\n",
    "    # fx_tn = torch.tensor(2*fx[::2] + fx[1::2], dtype=torch.int64)\n",
    "    # xpsi = amplitude(fx_tn, params).item()\n",
    "    xpsi = amplitude_binary(fx).item()\n",
    "    if not xpsi:\n",
    "        continue\n",
    "\n",
    "    pi = abs(xpsi) ** 2\n",
    "    p += pi\n",
    "\n",
    "    Oloc = 0.0\n",
    "    for fy, hxy in zip(*H_quimb.flatconfig_coupling(fx)):\n",
    "        # fy_tn = torch.tensor(2*fy[::2] + fy[1::2], dtype=torch.int64)\n",
    "        # ypsi = amplitude(fy_tn, params).item()\n",
    "        ypsi = amplitude_binary(fy).item()\n",
    "        Oloc = Oloc + hxy * ypsi / xpsi\n",
    "\n",
    "    O += Oloc * pi\n",
    "\n",
    "print(f'MC energy: {O / p / Lx / Ly}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c600284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_609/3012806822.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  new_peps.apply_to_arrays(lambda x: np.array(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-0.23850422302929086)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exact energy via local expectation contraction\n",
    "terms = sr.hamiltonians.ham_fermi_hubbard_from_edges(\n",
    "    \"Z2\",\n",
    "    edges=edges,\n",
    "    U=8,\n",
    "    mu=0.0,\n",
    ")\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "ham.apply_to_arrays(lambda A: A.to_flat())\n",
    "new_peps = peps.copy()\n",
    "new_peps.apply_to_arrays(lambda x: np.array(x))\n",
    "new_peps.compute_local_expectation_exact(ham, normalized=True) / nsites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f8ac4",
   "metadata": {},
   "source": [
    "VMC update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a396056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared batched conn_etas and coeffs: torch.Size([3142, 8]), torch.Size([3142]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.21894879507688\n",
      "Vectorized jacobian time: 1.1292724609375\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3142, 8]), torch.Size([3142]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 15.995233761048851\n",
      "STEP 0 VMC energy after update: 15.995233761048851\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3133, 8]), torch.Size([3133]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 17.37750064260142\n",
      "Vectorized jacobian time: 0.9365081787109375\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3133, 8]), torch.Size([3133]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.87564648459265\n",
      "STEP 1 VMC energy after update: 16.87564648459265\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3156, 8]), torch.Size([3156]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.525543433417216\n",
      "Vectorized jacobian time: 0.4972374439239502\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3156, 8]), torch.Size([3156]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.15509710465271\n",
      "STEP 2 VMC energy after update: 16.15509710465271\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3160, 8]), torch.Size([3160]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 17.166228158049567\n",
      "Vectorized jacobian time: 0.5890846252441406\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3160, 8]), torch.Size([3160]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.669677312875937\n",
      "STEP 3 VMC energy after update: 16.669677312875937\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3130, 8]), torch.Size([3130]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.488698104505115\n",
      "Vectorized jacobian time: 0.5490400791168213\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3130, 8]), torch.Size([3130]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 15.914658108211523\n",
      "STEP 4 VMC energy after update: 15.914658108211523\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3084, 8]), torch.Size([3084]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.820008300241636\n",
      "Vectorized jacobian time: 0.6791033744812012\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3084, 8]), torch.Size([3084]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.19541764288165\n",
      "STEP 5 VMC energy after update: 16.19541764288165\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3071, 8]), torch.Size([3071]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 17.090305295188994\n",
      "Vectorized jacobian time: 0.5793662071228027\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3071, 8]), torch.Size([3071]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.280597098395923\n",
      "STEP 6 VMC energy after update: 16.280597098395923\n",
      "\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3090, 8]), torch.Size([3090]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 16.484053256146197\n",
      "Vectorized jacobian time: 0.4556698799133301\n",
      "Prepared batched conn_etas and coeffs: torch.Size([3090, 8]), torch.Size([3090]) (batch size 256)\n",
      "Batched local energies: torch.Size([256])\n",
      "Energy: 15.758870518338336\n",
      "STEP 7 VMC energy after update: 15.758870518338336\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     fxs, current_amps = \u001b[43msample_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpeps_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n\u001b[32m      4\u001b[39m     batched_grads_vec, amps = compute_grads(fxs, fpeps_model, vectorize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36msample_next\u001b[39m\u001b[34m(fxs, fpeps_model, graph)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_next\u001b[39m(fxs, fpeps_model, graph):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     current_amps = \u001b[43mfpeps_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row, edges \u001b[38;5;129;01min\u001b[39;00m graph.row_edges.items():\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m edges:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mSimpleModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mSimpleModel.vamp\u001b[39m\u001b[34m(self, x, params)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvamp\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, params):\n\u001b[32m     68\u001b[39m     params = {\u001b[38;5;28mint\u001b[39m(k): {\u001b[33m'\u001b[39m\u001b[33mblocks\u001b[39m\u001b[33m'\u001b[39m:v} \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params.items()}\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mamplitude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/apis.py:208\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:282\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    272\u001b[39m         func,\n\u001b[32m    273\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m         **kwargs,\n\u001b[32m    279\u001b[39m     )\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/_functorch/vmap.py:432\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    429\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    430\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    431\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSimpleModel.amplitude\u001b[39m\u001b[34m(self, x, params)\u001b[39m\n\u001b[32m     63\u001b[39m amp.contract_boundary_from_ymin_(max_bond=chi, cutoff=\u001b[32m0.0\u001b[39m, yrange=[\u001b[32m0\u001b[39m, amp.Ly//\u001b[32m2\u001b[39m-\u001b[32m1\u001b[39m])\n\u001b[32m     64\u001b[39m amp.contract_boundary_from_ymax_(max_bond=chi, cutoff=\u001b[32m0.0\u001b[39m, yrange=[amp.Ly//\u001b[32m2\u001b[39m, amp.Ly-\u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mamp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/quimb/quimb/tensor/tensor_core.py:9315\u001b[39m, in \u001b[36mTensorNetwork.contract\u001b[39m\u001b[34m(self, tags, output_inds, optimize, get, max_bond, strip_exponent, preserve_tensor, backend, inplace, **kwargs)\u001b[39m\n\u001b[32m   9313\u001b[39m \u001b[38;5;66;03m# contracting everything to single output\u001b[39;00m\n\u001b[32m   9314\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_tags \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m9315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_contract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   9316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensor_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrip_exponent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrip_exponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexponent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9319\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   9320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   9322\u001b[39m \u001b[38;5;66;03m# contract some or all tensors, but keeping tensor network\u001b[39;00m\n\u001b[32m   9323\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.contract_tags(\n\u001b[32m   9324\u001b[39m     tags, strip_exponent=strip_exponent, inplace=inplace, **kwargs\n\u001b[32m   9325\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/functools.py:912\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    910\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/quimb/quimb/tensor/tensor_core.py:309\u001b[39m, in \u001b[36mtensor_contract\u001b[39m\u001b[34m(output_inds, optimize, get, backend, preserve_tensor, drop_tags, strip_exponent, exponent, *tensors, **contract_opts)\u001b[39m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_contract_get_other(\n\u001b[32m    299\u001b[39m         arrays=arrays,\n\u001b[32m    300\u001b[39m         inds=inds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    305\u001b[39m         **contract_opts,\n\u001b[32m    306\u001b[39m     )\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# perform the contraction!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m data_out = \u001b[43marray_contract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43minds_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrip_exponent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrip_exponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcontract_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strip_exponent:\n\u001b[32m    320\u001b[39m     \u001b[38;5;66;03m# mantissa and exponent returned separately\u001b[39;00m\n\u001b[32m    321\u001b[39m     data_out, result_exponent = data_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/quimb/quimb/tensor/contraction.py:285\u001b[39m, in \u001b[36marray_contract\u001b[39m\u001b[34m(arrays, inputs, output, optimize, backend, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    284\u001b[39m     backend = get_contract_backend()\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctg\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_contract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/cotengra/interface.py:864\u001b[39m, in \u001b[36marray_contract\u001b[39m\u001b[34m(arrays, inputs, output, optimize, strip_exponent, cache_expression, backend, **kwargs)\u001b[39m\n\u001b[32m    854\u001b[39m shapes = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(ar.shape, arrays))\n\u001b[32m    855\u001b[39m expr = array_contract_expression(\n\u001b[32m    856\u001b[39m     inputs,\n\u001b[32m    857\u001b[39m     output,\n\u001b[32m   (...)\u001b[39m\u001b[32m    862\u001b[39m     **kwargs,\n\u001b[32m    863\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/cotengra/contract.py:784\u001b[39m, in \u001b[36mContractor.__call__\u001b[39m\u001b[34m(self, *arrays, **kwargs)\u001b[39m\n\u001b[32m    781\u001b[39m r_array = temps.pop(r)\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tdot:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m     p_array = \u001b[43m_tensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m perm:\n\u001b[32m    786\u001b[39m         p_array = do(\u001b[33m\"\u001b[39m\u001b[33mtranspose\u001b[39m\u001b[33m\"\u001b[39m, p_array, perm, like=backend)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/symmray/symmray/interface.py:162\u001b[39m, in \u001b[36mtensordot\u001b[39m\u001b[34m(a, b, axes, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Contract two `symmray` arrays along the specified axes.\u001b[39;00m\n\u001b[32m    150\u001b[39m \n\u001b[32m    151\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m    to contract. Default is 2.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(a, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) == \u001b[32m0\u001b[39m:\n\u001b[32m    165\u001b[39m         \u001b[38;5;66;03m# likely called as effective scalar multiplication of block array\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/symmray/symmray/fermionic_common.py:268\u001b[39m, in \u001b[36mFermionicCommon.tensordot\u001b[39m\u001b[34m(self, other, axes, preserve_array, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m a, b, new_axes_a, new_axes_b = \u001b[38;5;28mself\u001b[39m._prepare_for_tensordot_fermionic(\n\u001b[32m    264\u001b[39m     other, axes\n\u001b[32m    265\u001b[39m )\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# perform blocked contraction!\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m c = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tensordot_abelian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_axes_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_axes_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# preserve array for resolving oddposs\u001b[39;49;00m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_array\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m c._resolve_oddpos_combine(a, b)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (c.ndim == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m preserve_array):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/symmray/symmray/flat/flat_array_common.py:1287\u001b[39m, in \u001b[36mFlatArrayCommon._tensordot_abelian\u001b[39m\u001b[34m(self, other, axes, mode, preserve_array)\u001b[39m\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_tensordot_abelian\u001b[39m(\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1282\u001b[39m     other,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     preserve_array=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1286\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensordot_abelian_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1293\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/symmray/symmray/flat/flat_array_common.py:1989\u001b[39m, in \u001b[36mtensordot_abelian_flat\u001b[39m\u001b[34m(a, b, axes, mode, preserve_array)\u001b[39m\n\u001b[32m   1986\u001b[39m         mode = \u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mdirect\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1989\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensordot_flat_direct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxes_a\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxes_b\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxes_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreserve_array\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1999\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensordot_flat_fused(\n\u001b[32m   2000\u001b[39m         a,\n\u001b[32m   2001\u001b[39m         b,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2006\u001b[39m         preserve_array=preserve_array,\n\u001b[32m   2007\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/symmray/symmray/flat/flat_array_common.py:1864\u001b[39m, in \u001b[36mtensordot_flat_direct\u001b[39m\u001b[34m(a, b, left_axes, axes_a, axes_b, right_axes, preserve_array)\u001b[39m\n\u001b[32m   1860\u001b[39m \u001b[38;5;66;03m# sort and reshape left blocks\u001b[39;00m\n\u001b[32m   1861\u001b[39m d0 = a.duals[axes_a[\u001b[32m0\u001b[39m]]\n\u001b[32m   1862\u001b[39m lcon_sectors = zn_combine(\n\u001b[32m   1863\u001b[39m     a.order,\n\u001b[32m-> \u001b[39m\u001b[32m1864\u001b[39m     \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43msectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_a\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m   1865\u001b[39m     duals=[a.duals[ax] != d0 \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axes_a],\n\u001b[32m   1866\u001b[39m )\n\u001b[32m   1867\u001b[39m lkord = lexsort_sectors(\n\u001b[32m   1868\u001b[39m     (\n\u001b[32m   1869\u001b[39m         lcon_sectors,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1872\u001b[39m     )\n\u001b[32m   1873\u001b[39m )\n\u001b[32m   1874\u001b[39m larray = _reshape(a.blocks[lkord], (a.order, -\u001b[32m1\u001b[39m, dc, *a.shape_block))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/clean_symmray/lib/python3.12/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    fxs, current_amps = sample_next(fxs, fpeps_model, graph)\n",
    "    energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "    batched_grads_vec, amps = compute_grads(fxs, fpeps_model, vectorize=True)\n",
    "    # Now that we have local energies, amps and per-sample gradients, we can compute the energy gradient\n",
    "    # With the energy gradient, we can further do SR for optimization\n",
    "    # Or we can do minSR, which is simpler here\n",
    "    with torch.no_grad():\n",
    "        local_energies # shape (B,)\n",
    "        local_energies_mean = torch.mean(local_energies)\n",
    "        amps # shape (B,)\n",
    "        params # pytree with each leaf of shape (param_shape...)\n",
    "\n",
    "        # flatten the model params into a 1d vector of shape (Np,)\n",
    "        params_vec = torch.nn.utils.parameters_to_vector(fpeps_model.parameters())\n",
    "\n",
    "        # compute log-derivative grads\n",
    "        batched_log_grads_vec = batched_grads_vec / amps  # shape (B, Np)\n",
    "        log_grads_vec_mean = torch.mean(batched_log_grads_vec, dim=0)  # shape (Np,)\n",
    "\n",
    "        O_sk = (batched_log_grads_vec - log_grads_vec_mean[None, :]) / (B**0.5)  # shape (B, Np)\n",
    "        T = (O_sk @ O_sk.T.conj())  # shape (B, B)\n",
    "        E_s = (local_energies - local_energies_mean) / (B**0.5)  # shape (B,)\n",
    "\n",
    "        # minSR: need to solve O_sk * dp = E_s in the least square sense, using the pseudo-inverse of O_sk to get the minimum norm solution\n",
    "        T_inv = torch.linalg.pinv(T,  rtol=1e-12, atol=0, hermitian=True)\n",
    "        dp = O_sk.conj().T @ (T_inv @ E_s)  # shape (Np,)\n",
    "        # update params\n",
    "        learning_rate = 0.01\n",
    "        new_params_vec = params_vec - learning_rate * dp\n",
    "\n",
    "    # load the new params back to the model\n",
    "    torch.nn.utils.vector_to_parameters(new_params_vec, fpeps_model.parameters())\n",
    "\n",
    "    energy, local_energies = evaluate_energy(fxs, fpeps_model, H, current_amps)\n",
    "    print(f'STEP {_} VMC energy after update: {energy.item()}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_symmray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
