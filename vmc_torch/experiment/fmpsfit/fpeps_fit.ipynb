{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d99ad4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import cotengra as ctg\n",
    "import quimb.tensor as qtn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vmc_torch.experiment.tn_model import wavefunctionModel\n",
    "\n",
    "\n",
    "class fTNModel_reuse_fitting(wavefunctionModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ftn,\n",
    "        max_bond=None,\n",
    "        dtype=torch.float32,\n",
    "        functional=False,\n",
    "        debug=False,\n",
    "        contraction_kwargs={},\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        self.functional = functional\n",
    "        self.debug = debug\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "        # self.skeleton.exponent = 0\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        # NOTE: pytorch nn.ParameterDict automatically sorts the keys (as sorted(dict))\n",
    "        self.torch_tn_params = nn.ModuleDict(\n",
    "            {\n",
    "                str(tid): nn.ParameterDict(\n",
    "                    {\n",
    "                        str(sector): nn.Parameter(data)\n",
    "                        for sector, data in blk_array.items()\n",
    "                    }\n",
    "                )\n",
    "                for tid, blk_array in params.items()\n",
    "            }\n",
    "        )\n",
    "        self.tn_param_key_id = \"torch_tn_params.{}.{}\"\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            f\"fPEPS (chi={max_bond})\": {\n",
    "                \"D\": ftn.max_bond(),\n",
    "                \"Lx\": ftn.Lx,\n",
    "                \"Ly\": ftn.Ly,\n",
    "                \"symmetry\": self.symmetry,\n",
    "            },\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        self.tree = None\n",
    "        self.Lx = ftn.Lx\n",
    "        self.Ly = ftn.Ly\n",
    "        self._env_x_cache = {}\n",
    "        self._env_y_cache = {}\n",
    "        self.config_ref = None\n",
    "        self.amp_ref = None\n",
    "        self.debug_amp_cache = []\n",
    "        self.contraction_kwargs = contraction_kwargs\n",
    "\n",
    "    def from_1d_to_2d(self, config, ordering=\"snake\"):\n",
    "        if ordering == \"snake\":\n",
    "            config_2d = config.reshape((self.Lx, self.Ly))\n",
    "            return config_2d\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Ordering {ordering} is not implemented.\")\n",
    "\n",
    "    def from_1dsite_to_2dsite(self, site, ordering=\"snake\"):\n",
    "        \"\"\"\n",
    "        Convert a 1d site index to a 2d site index.\n",
    "        site: 1d site index\n",
    "        \"\"\"\n",
    "        if ordering == \"snake\":\n",
    "            return (site // self.Ly, site % self.Ly)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ordering: {ordering}\")\n",
    "\n",
    "    def from_2dsite_to_1dsite(self, site, ordering=\"snake\"):\n",
    "        \"\"\"\n",
    "        Convert a 2d site index to a 1d site index.\n",
    "        site: (row, col) tuple\n",
    "        \"\"\"\n",
    "        if ordering == \"snake\":\n",
    "            return site[0] * self.Ly + site[1]\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ordering: {ordering}\")\n",
    "\n",
    "    def transform_quimb_env_x_key_to_config_key(self, env_x, config):\n",
    "        \"\"\"\n",
    "        Return a dictionary with the keys of of the config rows\n",
    "        \"\"\"\n",
    "        config_2d = self.from_1d_to_2d(config)\n",
    "        env_x_row_config = {}\n",
    "        for key in env_x.keys():\n",
    "            if key[0] == \"xmax\":  # from bottom to top\n",
    "                row_n = key[1]\n",
    "                if row_n != self.Lx - 1:\n",
    "                    rows_config = tuple(\n",
    "                        torch.cat(tuple(config_2d[row_n + 1 :].to(torch.int))).tolist()\n",
    "                    )\n",
    "                    env_x_row_config[(\"xmax\", rows_config)] = env_x[key]\n",
    "            elif key[0] == \"xmin\":  # from top to bottom\n",
    "                row_n = key[1]\n",
    "                if row_n != 0:\n",
    "                    rows_config = tuple(\n",
    "                        torch.cat(tuple(config_2d[:row_n].to(torch.int))).tolist()\n",
    "                    )\n",
    "                    env_x_row_config[(\"xmin\", rows_config)] = env_x[key]\n",
    "        return env_x_row_config\n",
    "\n",
    "    def transform_quimb_env_y_key_to_config_key(self, env_y, config):\n",
    "        \"\"\"\n",
    "        Return a dictionary with the keys of of the config rows\n",
    "        \"\"\"\n",
    "        config_2d = self.from_1d_to_2d(config)\n",
    "        env_y_row_config = {}\n",
    "        for key in env_y.keys():\n",
    "            if key[0] == \"ymax\":\n",
    "                col_n = key[1]\n",
    "                if col_n != self.Ly - 1:\n",
    "                    cols_config = tuple(\n",
    "                        torch.cat(\n",
    "                            tuple(config_2d[:, col_n + 1 :].to(torch.int))\n",
    "                        ).tolist()\n",
    "                    )\n",
    "                    env_y_row_config[(\"ymax\", cols_config)] = env_y[key]\n",
    "            elif key[0] == \"ymin\":\n",
    "                col_n = key[1]\n",
    "                if col_n != 0:\n",
    "                    cols_config = tuple(\n",
    "                        torch.cat(tuple(config_2d[:, :col_n].to(torch.int))).tolist()\n",
    "                    )\n",
    "                    env_y_row_config[(\"ymin\", cols_config)] = env_y[key]\n",
    "        return env_y_row_config\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cache_env_x(self, amp, config):\n",
    "        \"\"\"\n",
    "        Cache the environment x for the given configuration\n",
    "        \"\"\"\n",
    "        env_x = amp.compute_x_environments(\n",
    "            max_bond=self.max_bond, cutoff=0.0, **self.contraction_kwargs\n",
    "        )\n",
    "        env_x_cache = self.transform_quimb_env_x_key_to_config_key(env_x, config)\n",
    "        self._env_x_cache = env_x_cache\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cache_env_y(self, amp, config):\n",
    "        \"\"\"\n",
    "        Cache the environment y for the given configuration\n",
    "        \"\"\"\n",
    "        env_y = amp.compute_y_environments(\n",
    "            max_bond=self.max_bond, cutoff=0.0, **self.contraction_kwargs\n",
    "        )\n",
    "        env_y_cache = self.transform_quimb_env_y_key_to_config_key(env_y, config)\n",
    "        self._env_y_cache = env_y_cache\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp\n",
    "\n",
    "    def cache_env(self, amp, config):\n",
    "        \"\"\"\n",
    "        Cache the environment x and y for the given configuration\n",
    "        \"\"\"\n",
    "        self.cache_env_x(amp, config)\n",
    "        self.cache_env_y(amp, config)\n",
    "\n",
    "    @property\n",
    "    def env_x_cache(self):\n",
    "        \"\"\"\n",
    "        Return the cached environment x\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"_env_x_cache\"):\n",
    "            return self._env_x_cache\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def env_y_cache(self):\n",
    "        \"\"\"\n",
    "        Return the cached environment y\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"_env_y_cache\"):\n",
    "            return self._env_y_cache\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def clear_env_x_cache(self, from_which=None):\n",
    "        \"\"\"\n",
    "        Clear the cached environment x\n",
    "        \"\"\"\n",
    "        if from_which is None:\n",
    "            self._env_x_cache = {}\n",
    "        else:\n",
    "            assert from_which in [\"xmax\", \"xmin\"], \"from_which must be 'xmax' or 'xmin'\"\n",
    "            for key in list(self._env_x_cache.keys()):\n",
    "                if key[0] == from_which:\n",
    "                    del self._env_x_cache[key]\n",
    "\n",
    "    def clear_env_y_cache(self, from_which=None):\n",
    "        \"\"\"\n",
    "        Clear the cached environment y\n",
    "        \"\"\"\n",
    "        if from_which is None:\n",
    "            self._env_y_cache = {}\n",
    "        else:\n",
    "            assert from_which in [\"ymax\", \"ymin\"], \"from_which must be 'ymax' or 'ymin'\"\n",
    "            for key in list(self._env_y_cache.keys()):\n",
    "                if key[0] == from_which:\n",
    "                    del self._env_y_cache[key]\n",
    "\n",
    "    def clear_wavefunction_env_cache(self):\n",
    "        self.clear_env_x_cache()\n",
    "        self.clear_env_y_cache()\n",
    "        self.config_ref = None\n",
    "        self.amp_ref = None\n",
    "\n",
    "    def detect_changed_sites(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "        Detect the sites that have changed in the new configuration,\n",
    "        written in 1d coordinate format.\n",
    "        \"\"\"\n",
    "        changed_sites = set()\n",
    "        unchanged_sites = set()\n",
    "        for i in range(self.Lx * self.Ly):\n",
    "            if config_ref[i] != new_config[i]:\n",
    "                changed_sites.add(i)\n",
    "            else:\n",
    "                unchanged_sites.add(i)\n",
    "        changed_sites = sorted(changed_sites)\n",
    "        unchanged_sites = sorted(unchanged_sites)\n",
    "        if len(changed_sites) == 0:\n",
    "            return [], []\n",
    "        return changed_sites, unchanged_sites\n",
    "\n",
    "    def from_1d_sites_to_tids(self, sites):\n",
    "        \"\"\"\n",
    "        Convert a list of 1d site indices to a list of tensor ids.\n",
    "        \"\"\"\n",
    "        tids_list = list(self.skeleton.tensor_map.keys())\n",
    "        return [tids_list[site] for site in sites]\n",
    "\n",
    "    def detect_changed_rows(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "        Detect the rows that have changed in the new configuration\n",
    "        \"\"\"\n",
    "        config_ref_2d = self.from_1d_to_2d(config_ref)\n",
    "        new_config_2d = self.from_1d_to_2d(new_config)\n",
    "        changed_rows = []\n",
    "        for i in range(self.Lx):\n",
    "            if not torch.equal(config_ref_2d[i], new_config_2d[i]):\n",
    "                changed_rows.append(i)\n",
    "        if len(changed_rows) == 0:\n",
    "            return [], [], []\n",
    "        unchanged_rows_above = list(range(changed_rows[0]))\n",
    "        unchanged_rows_below = list(range(changed_rows[-1] + 1, self.Lx))\n",
    "        return changed_rows, unchanged_rows_above, unchanged_rows_below\n",
    "\n",
    "    def detect_changed_cols(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "        Detect the columns that have changed in the new configuration\n",
    "        \"\"\"\n",
    "        config_ref_2d = self.from_1d_to_2d(config_ref)\n",
    "        new_config_2d = self.from_1d_to_2d(new_config)\n",
    "        changed_cols = []\n",
    "        for i in range(self.Ly):\n",
    "            if not torch.equal(config_ref_2d[:, i], new_config_2d[:, i]):\n",
    "                changed_cols.append(i)\n",
    "        if len(changed_cols) == 0:\n",
    "            return [], [], []\n",
    "        unchanged_cols_left = list(range(changed_cols[0]))\n",
    "        unchanged_cols_right = list(range(changed_cols[-1] + 1, self.Ly))\n",
    "        return changed_cols, unchanged_cols_left, unchanged_cols_right\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_env_x_cache(self, config):\n",
    "        \"\"\"\n",
    "        Update the cached environment x for the given configuration\n",
    "        \"\"\"\n",
    "        if self.env_x_cache:\n",
    "            self.clear_env_x_cache()\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.cache_env_x(amp_tn, config, **self.contraction_kwargs)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "\n",
    "    def get_cache_key(self, config, from_which, row_id=None, col_id=None):\n",
    "        if row_id is None and col_id is None:\n",
    "            raise ValueError(\"Either row_id or col_id must be provided\")\n",
    "        if row_id is not None:\n",
    "            assert from_which in [\"xmax\", \"xmin\"], (\n",
    "                \"from_which must be 'xmax' or 'xmin' when row_id is provided\"\n",
    "            )\n",
    "            config_2d = self.from_1d_to_2d(config)\n",
    "            rows_config = (\n",
    "                tuple(torch.cat(tuple(config_2d[row_id + 1 :].to(torch.int))).tolist())\n",
    "                if from_which == \"xmax\"\n",
    "                else tuple(torch.cat(tuple(config_2d[:row_id].to(torch.int))).tolist())\n",
    "            )\n",
    "            return (from_which, rows_config)\n",
    "        if col_id is not None:\n",
    "            assert from_which in [\"ymax\", \"ymin\"], (\n",
    "                \"from_which must be 'ymax' or 'ymin' when col_id is provided\"\n",
    "            )\n",
    "            config_2d = self.from_1d_to_2d(config)\n",
    "            cols_config = (\n",
    "                tuple(\n",
    "                    torch.cat(tuple(config_2d[:, col_id + 1 :].to(torch.int))).tolist()\n",
    "                )\n",
    "                if from_which == \"ymax\"\n",
    "                else tuple(\n",
    "                    torch.cat(tuple(config_2d[:, :col_id].to(torch.int))).tolist()\n",
    "                )\n",
    "            )\n",
    "            return (from_which, cols_config)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_env_x_cache_to_row(\n",
    "        self, config, row_id, from_which=\"xmin\", mode=\"reuse\"\n",
    "    ):\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "\n",
    "        # add the new env_x to the cache\n",
    "        if not self.env_x_cache:\n",
    "            new_env_x = amp_tn.compute_environments(\n",
    "                max_bond=self.max_bond,\n",
    "                cutoff=0.0,\n",
    "                xrange=(0, row_id + 1)\n",
    "                if from_which == \"xmin\"\n",
    "                else (row_id - 1, self.Lx - 1),\n",
    "                from_which=from_which,\n",
    "                **self.contraction_kwargs,\n",
    "            )\n",
    "            new_env_x_cache = self.transform_quimb_env_x_key_to_config_key(\n",
    "                new_env_x, config\n",
    "            )\n",
    "            self._env_x_cache.update(new_env_x_cache)\n",
    "            return\n",
    "        else:\n",
    "            if (\n",
    "                (from_which == \"xmin\" and row_id == 0)\n",
    "                or (from_which == \"xmax\" and row_id == self.Lx - 1)\n",
    "                or mode == \"force\"\n",
    "            ):\n",
    "                new_env_x = amp_tn.compute_environments(\n",
    "                    max_bond=self.max_bond,\n",
    "                    cutoff=0.0,\n",
    "                    xrange=(0, row_id + 1)\n",
    "                    if from_which == \"xmin\"\n",
    "                    else (row_id - 1, self.Lx - 1),\n",
    "                    from_which=from_which,\n",
    "                    **self.contraction_kwargs,\n",
    "                )\n",
    "                new_env_x_cache = self.transform_quimb_env_x_key_to_config_key(\n",
    "                    new_env_x, config\n",
    "                )\n",
    "                self._env_x_cache.update(new_env_x_cache)\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                assert mode == \"reuse\"\n",
    "                row_tn = amp_tn.select(amp_tn.x_tag(row_id))\n",
    "                cache_mps = self.env_x_cache[\n",
    "                    self.get_cache_key(config, from_which, row_id=row_id)\n",
    "                ]\n",
    "                new_cache_key = self.get_cache_key(\n",
    "                    config,\n",
    "                    from_which,\n",
    "                    row_id=row_id + 1 if from_which == \"xmin\" else row_id - 1,\n",
    "                )\n",
    "                new_cache_mps = row_tn | cache_mps\n",
    "                new_cache_mps.contract_boundary_from_(\n",
    "                    from_which=from_which,\n",
    "                    xrange=(0, self.Lx - 1),\n",
    "                    yrange=(0, self.Ly - 1),\n",
    "                    max_bond=self.max_bond,\n",
    "                    cutoff=0.0,\n",
    "                    **self.contraction_kwargs,\n",
    "                )\n",
    "                new_env_x_cache = {new_cache_key: new_cache_mps}\n",
    "                self._env_x_cache.update(new_env_x_cache)\n",
    "                return\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_env_y_cache(self, config):\n",
    "        \"\"\"\n",
    "        Update the cached environment y for the given configuration\n",
    "        \"\"\"\n",
    "        if self.env_y_cache:\n",
    "            self.clear_env_y_cache()\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.cache_env_y(amp_tn, config)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_env_y_cache_to_col(\n",
    "        self, config, col_id, from_which=\"ymin\", mode=\"reuse\"\n",
    "    ):\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "        # add the new env_y to the cache\n",
    "        if not self.env_y_cache:\n",
    "            new_env_y = amp_tn.compute_environments(\n",
    "                max_bond=self.max_bond,\n",
    "                cutoff=0.0,\n",
    "                yrange=(0, col_id + 1)\n",
    "                if from_which == \"ymin\"\n",
    "                else (col_id - 1, self.Ly - 1),\n",
    "                from_which=from_which,\n",
    "                **self.contraction_kwargs,\n",
    "            )\n",
    "            new_env_y_cache = self.transform_quimb_env_y_key_to_config_key(\n",
    "                new_env_y, config\n",
    "            )\n",
    "            self._env_y_cache.update(new_env_y_cache)\n",
    "            return\n",
    "        else:\n",
    "            if (\n",
    "                (from_which == \"ymin\" and col_id == 0)\n",
    "                or (from_which == \"ymax\" and col_id == self.Ly - 1)\n",
    "                or mode == \"force\"\n",
    "            ):\n",
    "                new_env_y = amp_tn.compute_environments(\n",
    "                    max_bond=self.max_bond,\n",
    "                    cutoff=0.0,\n",
    "                    yrange=(0, col_id + 1)\n",
    "                    if from_which == \"ymin\"\n",
    "                    else (col_id - 1, self.Ly - 1),\n",
    "                    from_which=from_which,\n",
    "                    **self.contraction_kwargs,\n",
    "                )\n",
    "                new_env_y_cache = self.transform_quimb_env_y_key_to_config_key(\n",
    "                    new_env_y, config\n",
    "                )\n",
    "                self._env_y_cache.update(new_env_y_cache)\n",
    "                return\n",
    "            else:\n",
    "                assert mode == \"reuse\"\n",
    "                col_tn = amp_tn.select(amp_tn.y_tag(col_id))\n",
    "                cache_mps = self.env_y_cache[\n",
    "                    self.get_cache_key(config, from_which, col_id=col_id)\n",
    "                ]\n",
    "                new_cache_key = self.get_cache_key(\n",
    "                    config,\n",
    "                    from_which,\n",
    "                    col_id=col_id + 1 if from_which == \"ymin\" else col_id - 1,\n",
    "                )\n",
    "                new_cache_mps = col_tn | cache_mps\n",
    "                new_cache_mps.contract_boundary_from_(\n",
    "                    from_which=from_which,\n",
    "                    xrange=(0, self.Lx - 1),\n",
    "                    yrange=(0, self.Ly - 1),\n",
    "                    max_bond=self.max_bond,\n",
    "                    cutoff=0.0,\n",
    "                    **self.contraction_kwargs,\n",
    "                )\n",
    "                new_env_y_cache = {new_cache_key: new_cache_mps}\n",
    "                self._env_y_cache.update(new_env_y_cache)\n",
    "                return\n",
    "\n",
    "    def psi(self):\n",
    "        \"\"\"\n",
    "        Return the wavefunction (fPEPS)\n",
    "        \"\"\"\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        return psi\n",
    "\n",
    "    def get_local_amp_tensors(self, sites: list, config: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Get the local tensors for the given tensor ids and configuration.\n",
    "        config: the input configuration.\n",
    "        sites: a list of 1d site/2d site indices.\n",
    "        \"\"\"\n",
    "        # first pick out the tensor parameters and form the local tn parameters vector\n",
    "        local_ts_params = {}\n",
    "        # tids = self.from_1d_sites_to_tids(sites)\n",
    "        tids = (\n",
    "            self.from_1d_sites_to_tids(\n",
    "                [self.from_2dsite_to_1dsite(site) for site in sites]\n",
    "            )\n",
    "            if isinstance(sites[0], tuple)\n",
    "            else self.from_1d_sites_to_tids(sites)\n",
    "        )\n",
    "        for tid in tids:\n",
    "            local_ts_params[tid] = {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in self.torch_tn_params[str(tid)].items()\n",
    "            }\n",
    "\n",
    "        # Get sites corresponding to the tids\n",
    "        sites_1d = (\n",
    "            [self.from_2dsite_to_1dsite(site) for site in sites]\n",
    "            if isinstance(sites[0], tuple)\n",
    "            else sites\n",
    "        )\n",
    "        sites_2d = (\n",
    "            sites\n",
    "            if isinstance(sites[0], tuple)\n",
    "            else [self.from_1dsite_to_2dsite(site) for site in sites]\n",
    "        )\n",
    "\n",
    "        # Select the corresponding tensor skeleton\n",
    "        local_ts_skeleton = self.skeleton.select(\n",
    "            [self.skeleton.site_tag_id.format(*site) for site in sites_2d], which=\"any\"\n",
    "        )\n",
    "\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        local_ftn = qtn.unpack(local_ts_params, local_ts_skeleton)\n",
    "\n",
    "        # Fix the physical indices\n",
    "        return local_ftn.fix_phys_inds(sites_2d, config[sites_1d])\n",
    "\n",
    "    def get_amp_tn(self, config, reconstruct=False):\n",
    "        if self.amp_ref is None or reconstruct:\n",
    "            psi = self.psi()\n",
    "            # Check config type\n",
    "            if not isinstance(config, torch.Tensor):\n",
    "                config = torch.tensor(\n",
    "                    config, dtype=torch.int if self.functional else self.param_dtype\n",
    "                )\n",
    "            else:\n",
    "                if config.dtype != self.param_dtype:\n",
    "                    config = config.to(\n",
    "                        torch.int if self.functional else self.param_dtype\n",
    "                    )\n",
    "            # Get the amplitude\n",
    "            amp_tn = psi.get_amp(config, conj=True, functional=self.functional)\n",
    "            # if self.debug:\n",
    "            #     print(f'Efficient amp tn construction (full), amp_tn exponent: {amp_tn.exponent}')\n",
    "            return amp_tn\n",
    "\n",
    "        else:\n",
    "            # detect the sites that have changed\n",
    "            changed_sites, unchanged_sites = self.detect_changed_sites(\n",
    "                self.config_ref, config\n",
    "            )\n",
    "\n",
    "            if len(changed_sites) == 0:\n",
    "                return self.amp_ref\n",
    "            else:\n",
    "                # substitute the changed sites tensors\n",
    "                local_amp_tn = self.get_local_amp_tensors(changed_sites, config)\n",
    "                unchanged_sites_2d = [\n",
    "                    self.from_1dsite_to_2dsite(site) for site in unchanged_sites\n",
    "                ]\n",
    "                unchanged_sites_tags = [\n",
    "                    self.skeleton.site_tag_id.format(*site)\n",
    "                    for site in unchanged_sites_2d\n",
    "                ]\n",
    "                unchanged_amp_tn = self.amp_ref.select(\n",
    "                    unchanged_sites_tags, which=\"any\"\n",
    "                )\n",
    "                # merge the local_amp_tn and unchanged_amp_tn\n",
    "                amp_tn = local_amp_tn | unchanged_amp_tn\n",
    "                amp_tn.exponent = self.skeleton.exponent\n",
    "                # if self.debug:\n",
    "                #     print(f'Efficient amp tn construction (local), amp_tn exponent: {amp_tn.exponent}')\n",
    "                return amp_tn\n",
    "\n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Check x_i type\n",
    "            if not isinstance(x_i, torch.Tensor):\n",
    "                x_i = torch.tensor(\n",
    "                    x_i, dtype=torch.int if self.functional else self.param_dtype\n",
    "                )\n",
    "            else:\n",
    "                if x_i.dtype != self.param_dtype:\n",
    "                    x_i = x_i.to(torch.int if self.functional else self.param_dtype)\n",
    "            # Get the amplitude\n",
    "            # amp = psi.get_amp(x_i, conj=True, functional=self.functional)\n",
    "            amp_tn = self.get_amp_tn(x_i)\n",
    "\n",
    "            if self.max_bond is None:\n",
    "                amp = amp_tn\n",
    "                if self.tree is None:\n",
    "                    opt = ctg.HyperOptimizer(\n",
    "                        progbar=True, max_repeats=10, parallel=True\n",
    "                    )\n",
    "                    self.tree = amp.contraction_tree(optimize=opt)\n",
    "                amp_val = amp.contract(\n",
    "                    optimize=self.tree\n",
    "                )  # quimb will address the cached exponent automatically\n",
    "\n",
    "            else:\n",
    "                if self.cache_env_mode:\n",
    "                    self.cache_env_x(amp_tn, x_i)\n",
    "                    # self.cache_env_y(amp, x_i)\n",
    "                    self.config_ref = x_i\n",
    "                    config_2d = self.from_1d_to_2d(x_i)\n",
    "                    key_bot = (\n",
    "                        \"xmax\",\n",
    "                        tuple(\n",
    "                            torch.cat(\n",
    "                                tuple(config_2d[self.Lx // 2 :].to(torch.int))\n",
    "                            ).tolist()\n",
    "                        ),\n",
    "                    )\n",
    "                    key_top = (\n",
    "                        \"xmin\",\n",
    "                        tuple(\n",
    "                            torch.cat(\n",
    "                                tuple(config_2d[: self.Lx // 2].to(torch.int))\n",
    "                            ).tolist()\n",
    "                        ),\n",
    "                    )\n",
    "                    amp_bot = self.env_x_cache[key_bot]\n",
    "                    amp_top = self.env_x_cache[key_top]\n",
    "                    amp_val = (\n",
    "                        (amp_bot | amp_top).contract() * 10 ** (self.skeleton.exponent)\n",
    "                    )  # quimb cannot address the cached exponent automatically when TN reuses the cached environment, so we need to multiply it manually\n",
    "                    if self.debug:\n",
    "                        amp_val1 = psi.get_amp(x_i).contract()\n",
    "                        print(\n",
    "                            f\"Reused Amp val: {amp_val}, Exact Amp val: {amp_val1}, Rel error: {torch.abs(amp_val1 - amp_val) / torch.abs(amp_val1)}\"\n",
    "                        )\n",
    "                        amp_val = amp_val1\n",
    "\n",
    "                else:\n",
    "                    if not self.env_x_cache and not self.env_y_cache:\n",
    "                        # check whether we can reuse the cached environment\n",
    "                        amp = amp_tn.contract_boundary_from_ymin(\n",
    "                            max_bond=self.max_bond,\n",
    "                            cutoff=0.0,\n",
    "                            yrange=[0, psi.Ly // 2 - 1],\n",
    "                        )\n",
    "                        amp = amp.contract_boundary_from_ymax(\n",
    "                            max_bond=self.max_bond,\n",
    "                            cutoff=0.0,\n",
    "                            yrange=[psi.Ly // 2, psi.Ly - 1],\n",
    "                        )\n",
    "                        amp_val = (\n",
    "                            amp.contract()\n",
    "                        )  # quimb will address the cached exponent automatically\n",
    "                    else:\n",
    "                        config_2d = self.from_1d_to_2d(x_i)\n",
    "                        # detect the rows that have changed\n",
    "                        changed_rows, unchanged_rows_above, unchanged_rows_below = (\n",
    "                            self.detect_changed_rows(self.config_ref, x_i)\n",
    "                        )\n",
    "                        # detect the columns that have changed\n",
    "                        changed_cols, unchanged_cols_left, unchanged_cols_right = (\n",
    "                            self.detect_changed_cols(self.config_ref, x_i)\n",
    "                        )\n",
    "                        if len(changed_rows) == 0:\n",
    "                            key_bot = (\n",
    "                                \"xmax\",\n",
    "                                tuple(\n",
    "                                    torch.cat(\n",
    "                                        tuple(config_2d[self.Lx // 2 :].to(torch.int))\n",
    "                                    ).tolist()\n",
    "                                ),\n",
    "                            )\n",
    "                            key_top = (\n",
    "                                \"xmin\",\n",
    "                                tuple(\n",
    "                                    torch.cat(\n",
    "                                        tuple(config_2d[: self.Lx // 2].to(torch.int))\n",
    "                                    ).tolist()\n",
    "                                ),\n",
    "                            )\n",
    "                            amp_bot = self.env_x_cache[key_bot]\n",
    "                            amp_top = self.env_x_cache[key_top]\n",
    "                            amp_val = (amp_bot | amp_top).contract() * 10 ** (\n",
    "                                self.skeleton.exponent\n",
    "                            )\n",
    "                        else:\n",
    "                            if len(changed_rows) <= len(changed_cols):\n",
    "                                # for bottom envs, until the last row in the changed rows, we can reuse the env\n",
    "                                # for top envs, until the first row in the changed rows, we can reuse the env\n",
    "                                amp_changed_rows = qtn.TensorNetwork(\n",
    "                                    [\n",
    "                                        amp_tn.select(amp_tn.x_tag_id.format(row_n))\n",
    "                                        for row_n in changed_rows\n",
    "                                    ]\n",
    "                                )\n",
    "                                amp_unchanged_bottom_env = qtn.TensorNetwork()\n",
    "                                amp_unchanged_top_env = qtn.TensorNetwork()\n",
    "                                if len(unchanged_rows_below) != 0:\n",
    "                                    amp_unchanged_bottom_env = self.env_x_cache[\n",
    "                                        (\n",
    "                                            \"xmax\",\n",
    "                                            tuple(\n",
    "                                                torch.cat(\n",
    "                                                    tuple(\n",
    "                                                        config_2d[\n",
    "                                                            unchanged_rows_below\n",
    "                                                        ].to(torch.int)\n",
    "                                                    )\n",
    "                                                ).tolist()\n",
    "                                            ),\n",
    "                                        )\n",
    "                                    ]\n",
    "                                if len(unchanged_rows_above) != 0:\n",
    "                                    amp_unchanged_top_env = self.env_x_cache[\n",
    "                                        (\n",
    "                                            \"xmin\",\n",
    "                                            tuple(\n",
    "                                                torch.cat(\n",
    "                                                    tuple(\n",
    "                                                        config_2d[\n",
    "                                                            unchanged_rows_above\n",
    "                                                        ].to(torch.int)\n",
    "                                                    )\n",
    "                                                ).tolist()\n",
    "                                            ),\n",
    "                                        )\n",
    "                                    ]\n",
    "                                amp_val = (\n",
    "                                    amp_unchanged_bottom_env\n",
    "                                    | amp_unchanged_top_env\n",
    "                                    | amp_changed_rows\n",
    "                                ).contract() * 10 ** (self.skeleton.exponent)\n",
    "                            else:\n",
    "                                # for left envs, until the first column in the changed columns, we can reuse the env\n",
    "                                # for right envs, until the last column in the changed columns, we can reuse the env\n",
    "                                amp_changed_cols = qtn.TensorNetwork(\n",
    "                                    [\n",
    "                                        amp_tn.select(amp_tn.y_tag_id.format(col_n))\n",
    "                                        for col_n in changed_cols\n",
    "                                    ]\n",
    "                                )\n",
    "                                amp_unchanged_left_env = qtn.TensorNetwork()\n",
    "                                amp_unchanged_right_env = qtn.TensorNetwork()\n",
    "                                if len(unchanged_cols_left) != 0:\n",
    "                                    amp_unchanged_left_env = self.env_y_cache[\n",
    "                                        (\n",
    "                                            \"ymin\",\n",
    "                                            tuple(\n",
    "                                                torch.cat(\n",
    "                                                    tuple(\n",
    "                                                        config_2d[\n",
    "                                                            :, unchanged_cols_left\n",
    "                                                        ].to(torch.int)\n",
    "                                                    )\n",
    "                                                ).tolist()\n",
    "                                            ),\n",
    "                                        )\n",
    "                                    ]\n",
    "                                if len(unchanged_cols_right) != 0:\n",
    "                                    amp_unchanged_right_env = self.env_y_cache[\n",
    "                                        (\n",
    "                                            \"ymax\",\n",
    "                                            tuple(\n",
    "                                                torch.cat(\n",
    "                                                    tuple(\n",
    "                                                        config_2d[\n",
    "                                                            :, unchanged_cols_right\n",
    "                                                        ].to(torch.int)\n",
    "                                                    )\n",
    "                                                ).tolist()\n",
    "                                            ),\n",
    "                                        )\n",
    "                                    ]\n",
    "                                amp_val = (\n",
    "                                    amp_unchanged_left_env\n",
    "                                    | amp_unchanged_right_env\n",
    "                                    | amp_changed_cols\n",
    "                                ).contract() * 10 ** (self.skeleton.exponent)\n",
    "\n",
    "            if amp_val == 0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "\n",
    "            # if self.debug:\n",
    "            #     amp_val_exact = psi.get_amp(x_i).contract()\n",
    "            #     if (amp_val - amp_val_exact).abs() > 1e-4:\n",
    "            #         print(f'Warning: Reused Amp val and Exact Amp val differ significantly! Reused Amp val: {amp_val}, Exact Amp val: {amp_val_exact}, Rel error: {torch.abs(amp_val_exact - amp_val) / torch.abs(amp_val_exact)}')\n",
    "\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "\n",
    "    def get_grad(self):\n",
    "        \"\"\" \"Compute the amplitude gradient by contracting TN without the on-site tensor.\"\"\"\n",
    "        if self.debug:\n",
    "            print(\n",
    "                \"Computing the amplitude gradient by contracting TN without the on-site tensor.\"\n",
    "            )\n",
    "        self.zero_grad()\n",
    "        sampled_x = self.config_ref\n",
    "        amp_tn = self.amp_ref\n",
    "        index_map = {0: 0, 1: 1, 2: 1, 3: 0}\n",
    "        array_map = {\n",
    "            0: torch.tensor([1.0, 0.0]),\n",
    "            1: torch.tensor([1.0, 0.0]),\n",
    "            2: torch.tensor([0.0, 1.0]),\n",
    "            3: torch.tensor([0.0, 1.0]),\n",
    "        }\n",
    "        config_2d = self.from_1d_to_2d(sampled_x)\n",
    "        for tid, model_ts_params in self.torch_tn_params.items():\n",
    "            with torch.no_grad():\n",
    "                # utils numbers\n",
    "                site_2d = self.from_1dsite_to_2dsite(int(tid))\n",
    "\n",
    "                p_ind = self.skeleton.site_ind_id.format(*site_2d)\n",
    "                p_ind_order = self.skeleton.tensor_map[int(tid)].inds.index(p_ind)\n",
    "                on_site_config = int(sampled_x[int(tid)])\n",
    "                on_site_config_parity = index_map[on_site_config]\n",
    "                site_tag = self.skeleton.site_tag_id.format(*site_2d)\n",
    "                input_vec = array_map[on_site_config]\n",
    "\n",
    "                ts0 = amp_tn.select(site_tag).contract()\n",
    "                ts_params = ts0.get_params()\n",
    "\n",
    "                # Reuse cached environment to compute grad_ts\n",
    "                row_id = site_2d[0]\n",
    "                # select the cached_env on both sides of the site tensor along the row\n",
    "                rows_above = list(range(row_id + 1, self.Lx))\n",
    "                rows_below = list(range(0, row_id))\n",
    "                cached_amp_tn_above = (\n",
    "                    self.env_x_cache[\n",
    "                        (\n",
    "                            \"xmax\",\n",
    "                            tuple(\n",
    "                                torch.cat(\n",
    "                                    tuple(config_2d[rows_above].to(torch.int))\n",
    "                                ).tolist()\n",
    "                            ),\n",
    "                        )\n",
    "                    ]\n",
    "                    if rows_above\n",
    "                    else qtn.TensorNetwork([])\n",
    "                )\n",
    "                cached_amp_tn_below = (\n",
    "                    self.env_x_cache[\n",
    "                        (\n",
    "                            \"xmin\",\n",
    "                            tuple(\n",
    "                                torch.cat(\n",
    "                                    tuple(config_2d[rows_below].to(torch.int))\n",
    "                                ).tolist()\n",
    "                            ),\n",
    "                        )\n",
    "                    ]\n",
    "                    if rows_below\n",
    "                    else qtn.TensorNetwork([])\n",
    "                )\n",
    "                within_row_sites = list(\n",
    "                    (site_2d[0], col_id)\n",
    "                    for col_id in range(self.Ly)\n",
    "                    if col_id != site_2d[1]\n",
    "                )\n",
    "                within_row_hole_tn = self.get_local_amp_tensors(\n",
    "                    within_row_sites, config=sampled_x\n",
    "                )\n",
    "                grad_ts = (\n",
    "                    within_row_hole_tn | cached_amp_tn_above | cached_amp_tn_below\n",
    "                ).contract()\n",
    "                grad_ts.data.phase_sync(inplace=True)\n",
    "\n",
    "            # # Exact contraction of the TN without the site tensor, naive calculation, expensive baseline.\n",
    "            # ts_left = [ts for ts in amp_tn.tensors if site_tag not in ts.tags]\n",
    "            # tn_left = qtn.TensorNetwork(ts_left)\n",
    "            # grad_ts = tn_left.contract()\n",
    "\n",
    "            # Back propagate through the final contraction\n",
    "            ts0.apply_to_arrays(lambda x: x.clone().detach().requires_grad_(True))\n",
    "            grad_ts.apply_to_arrays(lambda x: x.clone().detach().requires_grad_(False))\n",
    "            amp_temp = (ts0 | grad_ts).contract() * 10 ** (self.skeleton.exponent)\n",
    "            amp_temp.backward()\n",
    "\n",
    "            grad_ts_backprop_params = ts0.get_params().copy()  # correct gradients\n",
    "            for blk, ts_temp in ts0.get_params().items():\n",
    "                grad_ts_backprop_params[blk] = ts_temp.grad\n",
    "\n",
    "            # select the remaining sectors in model_ts_params\n",
    "            remaining_sliced_sectors = [sector for sector in sorted(ts_params)]\n",
    "            remaining_sectors = [\n",
    "                sector + (on_site_config_parity,) for sector in sorted(ts_params)\n",
    "            ]\n",
    "\n",
    "            for sliced_blk, blk in zip(remaining_sliced_sectors, remaining_sectors):\n",
    "                data = model_ts_params[str(blk)]\n",
    "                select_index = torch.argmax(input_vec).item()\n",
    "                slicer = [slice(None)] * data.ndim\n",
    "                slicer[p_ind_order] = select_index\n",
    "                reconstructed_grad_tensor = torch.zeros_like(data)\n",
    "                reconstructed_grad_tensor[tuple(slicer)] = grad_ts_backprop_params[\n",
    "                    sliced_blk\n",
    "                ]\n",
    "                data.grad = reconstructed_grad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b660425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "import pickle\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "pwd = \"/home/sijingdu/TNVMC/VMC_code/vmc_torch/data\"\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import autoray as ar\n",
    "from vmc_torch.experiment.tn_model import *\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "from vmc_torch.optimizer import SGD, SR, DecayScheduler\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful_2D_reusable\n",
    "from vmc_torch.torch_utils import QR, SVD\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.VMC import VMC\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function(\"torch\", \"linalg.svd\", SVD.apply)\n",
    "ar.register_function(\"torch\", \"linalg.qr\", QR.apply)\n",
    "\n",
    "from vmc_torch.utils import closest_divisible\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "Lx = int(8)\n",
    "Ly = int(8)\n",
    "symmetry = \"Z2\"\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx * Ly)\n",
    "n_fermions_per_spin = (N_f // 2, N_f // 2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(\n",
    "    Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin\n",
    ")\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 14\n",
    "chi = 96\n",
    "dtype = torch.float64\n",
    "\n",
    "if symmetry == \"U1_Z2\":\n",
    "    su_skeleton = \"peps_skeleton_U1.pkl\"\n",
    "    su_params = \"peps_su_params_U1.pkl\"\n",
    "    symmetry = \"Z2\"\n",
    "else:\n",
    "    su_skeleton = \"peps_skeleton.pkl\"\n",
    "    su_params = \"peps_su_params.pkl\"\n",
    "\n",
    "# Load PEPS\n",
    "skeleton = pickle.load(\n",
    "    open(pwd + f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{su_skeleton}\", \"rb\")\n",
    ")\n",
    "peps_params = pickle.load(\n",
    "    open(pwd + f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{su_params}\", \"rb\")\n",
    ")\n",
    "peps = qtn.unpack(peps_params, skeleton)\n",
    "# peps = generate_random_fpeps(Lx, Ly, D=D, seed=42, symmetry=symmetry, Nf=N_f, spinless=False)[0] # random peps\n",
    "# Precondition the fPEPS !!! Important for proper gradient calculation of fermionic TNS\n",
    "## 1. Sync the stored fermionic phases\n",
    "for ts in peps.tensors:\n",
    "    ts.data.phase_sync(inplace=True)\n",
    "## 2. Scale the tensor elements\n",
    "scale = 4.0\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(scale * x, dtype=dtype))\n",
    "## 3. Set the exponent to 0.0\n",
    "peps.exponent = 0.0\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = int(100)\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples / SIZE) % 2 != 0:\n",
    "    N_samples += SIZE\n",
    "\n",
    "# Set up variational model\n",
    "contraction_kawrgs = {\n",
    "    \"mode\": \"fit\",\n",
    "    \"bsz\": 2,\n",
    "    \"max_iterations\": 50,\n",
    "    \"tn_fit\": \"zipup\",\n",
    "    \"progbar\": True,\n",
    "    \"tol\": 1e-5,\n",
    "}\n",
    "# contraction_kawrgs = {\"mode\": \"direct\"}\n",
    "model = fTNModel_reuse_fitting(\n",
    "    peps, max_bond=chi, dtype=dtype, debug=False, contraction_kwargs=contraction_kawrgs\n",
    ")\n",
    "model_names = {\n",
    "    fTNModel_reuse_fitting: \"fTN_reuse\",\n",
    "}\n",
    "model_name = model_names.get(type(model), \"UnknownModel\")\n",
    "\n",
    "\n",
    "# Set up VMC parameters\n",
    "init_step = 0\n",
    "final_step = 1000\n",
    "total_steps = final_step - init_step\n",
    "if init_step != 0:\n",
    "    saved_model_params = torch.load(\n",
    "        pwd\n",
    "        + f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{model_name}/chi={chi}/model_params_step{init_step}.pth\"\n",
    "    )\n",
    "    saved_model_state_dict = saved_model_params[\"model_state_dict\"]\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params[\"model_params_vec\"])\n",
    "    try:\n",
    "        model.load_state_dict(saved_model_state_dict)\n",
    "    except Exception:\n",
    "        model.load_params(saved_model_params_vec)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "learning_rate = 0.1\n",
    "scheduler = DecayScheduler(\n",
    "    init_lr=learning_rate, decay_rate=0.9, patience=50, min_lr=1e-4\n",
    ")\n",
    "optimizer = SGD(learning_rate=learning_rate)\n",
    "sampler = MetropolisExchangeSamplerSpinful_2D_reusable(\n",
    "    H.hilbert,\n",
    "    graph,\n",
    "    N_samples=N_samples,\n",
    "    burn_in_steps=1,\n",
    "    reset_chain=False,\n",
    "    random_edge=False,\n",
    "    equal_partition=False,\n",
    "    dtype=dtype,\n",
    "    hopping_rate=0.25,\n",
    ")\n",
    "if N_f == int(Lx*Ly):\n",
    "    sampler.current_config = torch.tensor([1,2,1,2,1,2,1,2,\n",
    "                                        2,1,2,1,2,1,2,1,\n",
    "                                        1,2,1,2,1,2,1,2,\n",
    "                                        2,1,2,1,2,1,2,1,\n",
    "                                        1,2,1,2,1,2,1,2,\n",
    "                                        2,1,2,1,2,1,2,1,\n",
    "                                        1,2,1,2,1,2,1,2,\n",
    "                                        2,1,2,1,2,1,2,1])\n",
    "variational_state = Variational_State(model, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "preconditioner = SR(\n",
    "    dense=False,\n",
    "    exact=True if sampler is None else False,\n",
    "    use_MPI4Solver=True,\n",
    "    solver=\"minres\",\n",
    "    diag_eta=1e-3,\n",
    "    iter_step=5e2,\n",
    "    dtype=dtype,\n",
    "    rtol=1e-4,\n",
    ")\n",
    "vmc = VMC(\n",
    "    hamiltonian=H,\n",
    "    variational_state=variational_state,\n",
    "    optimizer=optimizer,\n",
    "    preconditioner=preconditioner,\n",
    "    scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0e3ca6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_tdiff=1.12e-06:   6%|6         | 3/50 [00:00<00:10,  4.38it/s]\n",
      "max_tdiff=6.73e-06:   4%|4         | 2/50 [00:00<00:10,  4.52it/s]\n",
      "max_tdiff=3.79e-06:   4%|4         | 2/50 [00:00<00:09,  4.87it/s]\n",
      "max_tdiff=4.70e-07:   6%|6         | 3/50 [00:00<00:12,  3.92it/s]\n",
      "max_tdiff=4.47e-06:   4%|4         | 2/50 [00:00<00:10,  4.44it/s]\n",
      "max_tdiff=5.71e-07:   6%|6         | 3/50 [00:00<00:11,  4.02it/s]\n",
      "max_tdiff=9.94e-07:   6%|6         | 3/50 [00:00<00:09,  4.91it/s]\n",
      "max_tdiff=6.19e-06:   4%|4         | 2/50 [00:00<00:10,  4.39it/s]\n",
      "max_tdiff=8.34e-07:   6%|6         | 3/50 [00:00<00:11,  4.17it/s]\n",
      "max_tdiff=6.71e-06:   4%|4         | 2/50 [00:00<00:10,  4.45it/s]\n",
      "max_tdiff=6.66e-06:   4%|4         | 2/50 [00:00<00:09,  4.82it/s]\n",
      "max_tdiff=8.12e-06:   4%|4         | 2/50 [00:00<00:10,  4.73it/s]\n"
     ]
    }
   ],
   "source": [
    "config = sampler.current_config\n",
    "amp = model.get_amp_tn(config)\n",
    "model.cache_env_x(amp, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
