{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n",
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from quimb.utils import progbar as Progbar\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.experiment.tn_model import fTNModel, fTN_NN_proj_variable_Model\n",
    "from vmc_torch.experiment.tn_model import fTN_Transformer_Model, fTN_Transformer_Proj_lazy_Model\n",
    "from vmc_torch.experiment.tn_model import init_weights_xavier, init_weights_kaiming, init_weights_to_zero\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinless, MetropolisExchangeSamplerSpinful\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import TrivialPreconditioner, SignedSGD, SGD, SR\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian import spinful_Fermi_Hubbard_square_lattice, spinless_Fermi_Hubbard_square_lattice\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "Lx = int(4)\n",
    "Ly = int(4)\n",
    "symmetry = 'U1'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx*Ly)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 4\n",
    "chi = 8\n",
    "dtype=torch.float64\n",
    "\n",
    "# Load PEPS\n",
    "skeleton = pickle.load(open(f\"../../data/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_skeleton.pkl\", \"rb\"))\n",
    "peps_params = pickle.load(open(f\"../../data/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_su_params.pkl\", \"rb\"))\n",
    "peps = qtn.unpack(peps_params, skeleton)\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = 2000\n",
    "N_samples = N_samples - N_samples % SIZE + SIZE\n",
    "if (N_samples/SIZE)%2 != 0:\n",
    "    N_samples += SIZE\n",
    "\n",
    "# model = fTNModel(peps, max_bond=chi)\n",
    "# model = fTN_NNiso_Model(peps, max_bond=chi, nn_hidden_dim=8, nn_eta=1e-3)\n",
    "# model = fTN_NN_Model(peps, max_bond=chi, nn_hidden_dim=8, nn_eta=1e-3)\n",
    "# model = fTN_Transformer_Model(\n",
    "#     peps, \n",
    "#     max_bond=chi, \n",
    "#     nn_eta=1.0, \n",
    "#     d_model=8, \n",
    "#     nhead=2, \n",
    "#     num_encoder_layers=2, \n",
    "#     num_decoder_layers=2,\n",
    "#     dim_feedforward=32,\n",
    "#     dropout=0.0,\n",
    "# )\n",
    "model = fTN_Transformer_Proj_lazy_Model(\n",
    "    peps,\n",
    "    max_bond=chi,\n",
    "    nn_eta=1.0,\n",
    "    d_model=2**2,\n",
    "    nhead=2,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=2**5,\n",
    "    dropout=0.0,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "import jax\n",
    "dummy_config = H.hilbert.random_state(key=jax.random.PRNGKey(0))\n",
    "model = fTN_NN_proj_variable_Model(peps, max_bond=chi, nn_eta=1.0, nn_hidden_dim=32, dtype=dtype, padded_length=30, dummy_config=dummy_config, lazy=True)\n",
    "model.apply(lambda x: init_weights_to_zero(x, std=2e-2))\n",
    "\n",
    "# model.apply(init_weights_to_zero)\n",
    "# model.apply(init_weights_xavier)\n",
    "\n",
    "model_names = {\n",
    "    fTNModel: 'fTN',\n",
    "    fTN_Transformer_Model: 'fTN_Transformer',\n",
    "    fTN_Transformer_Proj_lazy_Model: 'fTN_Transformer_Proj_lazy',\n",
    "    fTN_NN_proj_variable_Model: 'fTN_NN_proj_variable',\n",
    "    \n",
    "}\n",
    "model_name = model_names.get(type(model), 'UnknownModel')\n",
    "\n",
    "init_step = 0\n",
    "total_steps = 200\n",
    "if init_step != 0:\n",
    "    saved_model_params = torch.load(f'../../data/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{model_name}/chi={chi}/model_params_step{init_step}.pth')\n",
    "    saved_model_state_dict = saved_model_params['model_state_dict']\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params['model_params_vec'])\n",
    "    try:\n",
    "        model.load_state_dict(saved_model_state_dict)\n",
    "    except:\n",
    "        model.load_params(saved_model_params_vec)\n",
    "\n",
    "# optimizer = SignedSGD(learning_rate=0.05)\n",
    "optimizer = SGD(learning_rate=0.05)\n",
    "sampler = MetropolisExchangeSamplerSpinful(H.hilbert, graph, N_samples=N_samples, burn_in_steps=16, reset_chain=False, random_edge=True, dtype=dtype)\n",
    "# sampler = None\n",
    "variational_state = Variational_State(model, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "preconditioner = SR(dense=False, exact=True if sampler is None else False, use_MPI4Solver=True, diag_eta=0.05, iter_step=1e5, dtype=dtype)\n",
    "# preconditioner = TrivialPreconditioner()\n",
    "vmc = VMC(H, variational_state, optimizer, preconditioner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.fermion_utils import insert_proj_peps\n",
    "for _ in range(200):\n",
    "    random_seed = np.random.randint(0, 2**32)\n",
    "    dummy_config = H.hilbert.random_state(key=jax.random.PRNGKey(random_seed))\n",
    "    amp = peps.get_amp(dummy_config)\n",
    "    amp_w_proj = insert_proj_peps(amp, max_bond=6, yrange=[0, peps.Ly-2], lazy=True)\n",
    "# amp_w_proj.draw(color='proj')\n",
    "# amp.contract(), amp_w_proj.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13808, 1648)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_params, model.num_tn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fTN_Transformer_Proj_lazy_Model(\n",
       "  (torch_tn_params): ModuleDict(\n",
       "    (0): ParameterDict(\n",
       "        ((-1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((-1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "        ((0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2]\n",
       "        ((0, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "    )\n",
       "    (1): ParameterDict(\n",
       "        ((-1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((1, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (2): ParameterDict(\n",
       "        ((-1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((1, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (3): ParameterDict(\n",
       "        ((-1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "        ((-1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2]\n",
       "        ((0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "    )\n",
       "    (4): ParameterDict(\n",
       "        ((-1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((-1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((-1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((-1, 1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((-1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((-1, 2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((-1, 2, -2, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((-2, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-2, 0, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-2, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-2, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-2, 1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-2, 2, -2, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((0, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((0, 2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((0, 2, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, 2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (5): ParameterDict(\n",
       "        ((-1, 0, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 0, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 0, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((-1, 1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((-1, 1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((-1, 1, 2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 2, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 2, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 2, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 2, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((-1, 2, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 2, 2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 2, 2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((0, 0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 0, 2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2x2]\n",
       "        ((0, 1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 1, 2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 1, 2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 2, 0, 1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 2, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 2, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 2, 2, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 2, 2, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 2, 2, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((1, 0, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 0, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, 0, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 0, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((1, 0, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 0, 2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, 0, 2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 1, 0, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((1, 1, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((1, 1, 2, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 2, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((1, 1, 2, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 2, 1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 2, 2, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 2, 2, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "    )\n",
       "    (6): ParameterDict(\n",
       "        ((-1, 0, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 0, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 0, 0, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((-1, 1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((-1, 1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((-1, 1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 2, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 2, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 2, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 2, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((-1, 2, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 2, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 2, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((0, 0, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 0, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 0, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 0, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 1, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2x2]\n",
       "        ((0, 1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 2, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 2, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 2, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 2, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 2, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 2, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((1, 0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 0, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, 0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((1, 0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, 0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 1, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((1, 1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((1, 1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((1, 1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 2, 0, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 2, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 2, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "    )\n",
       "    (7): ParameterDict(\n",
       "        ((-1, -1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, 1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (8): ParameterDict(\n",
       "        ((-1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((-1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((-1, 0, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((-1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((-1, 1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((-2, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-2, -1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-2, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-2, 0, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-2, 0, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-2, 1, -2, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((0, 1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (9): ParameterDict(\n",
       "        ((-1, -1, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, -1, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, -1, 0, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 0, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 0, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((-1, 0, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 0, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((-1, 0, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((-1, 0, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 1, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((-1, 1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((0, -1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, -1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, -1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, -1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, -1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, -1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 0, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2x2]\n",
       "        ((0, 0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 1, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((1, -1, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, -1, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, -1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, -1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((1, -1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, -1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, -1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 0, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 0, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((1, 0, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((1, 0, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 0, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((1, 0, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 0, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 1, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 1, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "    )\n",
       "    (10): ParameterDict(\n",
       "        ((-1, -1, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, -1, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, -1, 0, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 0, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 0, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((-1, 0, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 0, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((-1, 0, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((-1, 0, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((-1, 1, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((-1, 1, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((-1, 1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((-1, 1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((-1, 1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((0, -1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, -1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, -1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, -1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, -1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, -1, 1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 0, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 0, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2x2]\n",
       "        ((0, 0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x2x1x1]\n",
       "        ((0, 0, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1x2]\n",
       "        ((0, 0, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x2x1]\n",
       "        ((0, 1, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x2x1]\n",
       "        ((0, 1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1x2]\n",
       "        ((0, 1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((0, 1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2x2]\n",
       "        ((0, 1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1x1x1]\n",
       "        ((1, -1, -1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, -1, -1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, -1, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, -1, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2x2]\n",
       "        ((1, -1, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, -1, 1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "        ((1, -1, 1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 0, -1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 0, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x2x1]\n",
       "        ((1, 0, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1x2]\n",
       "        ((1, 0, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 0, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2x2]\n",
       "        ((1, 0, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1x1x1]\n",
       "        ((1, 1, 0, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x2x1x1]\n",
       "        ((1, 1, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x2x1]\n",
       "        ((1, 1, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1x2]\n",
       "    )\n",
       "    (11): ParameterDict(\n",
       "        ((-1, -1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, 1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, 1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, 0, 1, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, 1, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 1, 1, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, -1, 1, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, 0, 1, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, 1, 1, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (12): ParameterDict(\n",
       "        ((-1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2]\n",
       "        ((-1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((-1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((-2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((-2, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "        ((0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "    )\n",
       "    (13): ParameterDict(\n",
       "        ((-1, -1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, -1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, -2, -2, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, 0, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, -1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, -2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, -2, -2, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, -2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, -2, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, -2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (14): ParameterDict(\n",
       "        ((-1, -1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((-1, -1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((-1, -2, -2, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((-1, 0, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((-1, 0, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((0, -1, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2x2]\n",
       "        ((0, -1, -2, 2)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, -1, 0, 0)): Parameter containing: [torch.DoubleTensor of size 2x2x1x1]\n",
       "        ((0, -2, -1, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, -2, -2, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((0, 0, -1, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x2x1]\n",
       "        ((0, 0, 0, 1)): Parameter containing: [torch.DoubleTensor of size 2x1x1x2]\n",
       "        ((1, -1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x2x1]\n",
       "        ((1, -1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x2x1x2]\n",
       "        ((1, -2, -1, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2x2]\n",
       "        ((1, -2, -2, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, -2, 0, 0)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "        ((1, 0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 1x1x1x1]\n",
       "    )\n",
       "    (15): ParameterDict(\n",
       "        ((-1, -1, 0)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((-1, 0, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "        ((0, -1, 1)): Parameter containing: [torch.DoubleTensor of size 2x2x2]\n",
       "        ((0, -2, 0)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((0, 0, 2)): Parameter containing: [torch.DoubleTensor of size 2x1x1]\n",
       "        ((1, -1, 2)): Parameter containing: [torch.DoubleTensor of size 1x2x1]\n",
       "        ((1, -2, 1)): Parameter containing: [torch.DoubleTensor of size 1x1x2]\n",
       "    )\n",
       "  )\n",
       "  (transformer): TransformerModel(\n",
       "    (embedding): Embedding(4, 4)\n",
       "    (float_embedding): Linear(in_features=1, out_features=4, bias=True)\n",
       "    (pos_encoder): PositionalEncoding()\n",
       "    (transformer): Transformer(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "            (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): TransformerDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TransformerDecoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "            )\n",
       "            (multihead_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=4, out_features=32, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (linear2): Linear(in_features=32, out_features=4, bias=True)\n",
       "            (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm3): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.0, inplace=False)\n",
       "            (dropout2): Dropout(p=0.0, inplace=False)\n",
       "            (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=4, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.5298e-12], dtype=torch.float64, grad_fn=<StackBackward0>),\n",
       " tensor(-2.5298e-12, dtype=torch.float64))"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import pyinstrument\n",
    "from vmc_torch.fermion_utils import insert_proj_peps\n",
    "random_config = H.hilbert.random_state(key=jax.random.PRNGKey(1))\n",
    "random_config = torch.tensor(random_config, dtype=dtype)\n",
    "amp = peps.get_amp(random_config)\n",
    "model(random_config), amp.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "insert_proj_peps(amp, max_bond=4, yrange=[0,peps.Ly-2], lazy=False).draw(color='proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 19:08:56  Samples:  86\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.089     CPU time: 0.089\n",
      "/   _/                      v4.7.3\n",
      "\n",
      "Profile at /tmp/ipykernel_23605/1791513093.py:1\n",
      "\n",
      "0.087 <module>  ../../../../../tmp/ipykernel_23605/1791513093.py:1\n",
      "└─ 0.087 fTNModel._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      "      [0 frames hidden]  \n",
      "         0.087 fTNModel._call_impl  torch/nn/modules/module.py:1555\n",
      "         └─ 0.087 fTNModel.forward  tn_model.py:146\n",
      "            └─ 0.087 fTNModel.amplitude  tn_model.py:118\n",
      "               ├─ 0.055 PEPS.contract_boundary_from_ymin  quimb/tensor/tensor_2d.py:2007\n",
      "               │  └─ 0.055 PEPS.contract_boundary_from  quimb/tensor/tensor_2d.py:1715\n",
      "               │     └─ 0.055 PEPS._contract_boundary_core  quimb/tensor/tensor_2d.py:1316\n",
      "               │        ├─ 0.028 PEPS.canonize_plane  quimb/tensor/tensor_2d.py:914\n",
      "               │        │  └─ 0.028 PEPS.canonize_between  quimb/tensor/tensor_core.py:6250\n",
      "               │        │     └─ 0.028 PEPS._canonize_between_tids  quimb/tensor/tensor_core.py:6225\n",
      "               │        │        └─ 0.028 wrapper  functools.py:883\n",
      "               │        │           └─ 0.028 tensor_canonize_bond  quimb/tensor/tensor_core.py:628\n",
      "               │        │              ├─ 0.011 Tensor.split  quimb/tensor/tensor_core.py:2239\n",
      "               │        │              │  └─ 0.011 wrapper  functools.py:883\n",
      "               │        │              │     └─ 0.011 tensor_split  quimb/tensor/tensor_core.py:435\n",
      "               │        │              │        ├─ 0.009 Composed.__call__  autoray/autoray.py:921\n",
      "               │        │              │        │  └─ 0.009 qr_stabilized  symmray/linalg.py:118\n",
      "               │        │              │        │     └─ 0.009 wrapper  functools.py:883\n",
      "               │        │              │        │        └─ 0.009 qr_fermionic  symmray/linalg.py:107\n",
      "               │        │              │        │           └─ 0.009 qr  symmray/linalg.py:46\n",
      "               │        │              │        │              └─ 0.009 _qr  symmray/linalg.py:26\n",
      "               │        │              │        │                 └─ 0.009 qr_stabilized  quimb/tensor/decomp.py:669\n",
      "               │        │              │        │                    └─ 0.009 do  autoray/autoray.py:30\n",
      "               │        │              │        │                          [0 frames hidden]  \n",
      "               │        │              │        │                             0.009 QR.apply  torch/autograd/function.py:558\n",
      "               │        │              │        │                             └─ 0.009 QRBackward.forward  ../torch_utils.py:158\n",
      "               │        │              │        │                                ├─ 0.006 SVDforward  ../torch_utils.py:39\n",
      "               │        │              │        │                                │  ├─ 0.004 [self]  ../torch_utils.py\n",
      "               │        │              │        │                                │  └─ 0.002 max  <built-in>\n",
      "               │        │              │        │                                ├─ 0.001 linalg_qr  <built-in>\n",
      "               │        │              │        │                                ├─ 0.001 is_one  ../torch_utils.py:34\n",
      "               │        │              │        │                                └─ 0.001 _VariableFunctionsClass.all  <built-in>\n",
      "               │        │              │        └─ 0.002 do  autoray/autoray.py:30\n",
      "               │        │              │           ├─ 0.001 fuse  symmray/interface.py:117\n",
      "               │        │              │           │  └─ 0.001 U1FermionicArray.fuse  symmray/fermionic_core.py:552\n",
      "               │        │              │           │     └─ 0.001 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │              │           │        └─ 0.001 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │        │              │           │           └─ 0.001 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │        │              │           │              └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │        │              │           │                    [1 frames hidden]  <built-in>\n",
      "               │        │              │           └─ 0.001 reshape  symmray/interface.py:58\n",
      "               │        │              │              └─ 0.001 U1FermionicArray.reshape  symmray/abelian_core.py:1685\n",
      "               │        │              │                 └─ 0.001 U1FermionicArray.unfuse  symmray/fermionic_core.py:638\n",
      "               │        │              ├─ 0.008 tensor_make_single_bond  quimb/tensor/tensor_core.py:944\n",
      "               │        │              │  └─ 0.008 tensor_multifuse  quimb/tensor/tensor_core.py:915\n",
      "               │        │              │     ├─ 0.007 Tensor.fuse  quimb/tensor/tensor_core.py:2414\n",
      "               │        │              │     │  └─ 0.007 do  autoray/autoray.py:30\n",
      "               │        │              │     │     └─ 0.007 fuse  symmray/interface.py:117\n",
      "               │        │              │     │        └─ 0.007 U1FermionicArray.fuse  symmray/fermionic_core.py:552\n",
      "               │        │              │     │           ├─ 0.006 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │              │     │           │  ├─ 0.005 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │        │              │     │           │  │  ├─ 0.004 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │        │              │     │           │  │  │  └─ 0.004 translated_function  autoray/autoray.py:1310\n",
      "               │        │              │     │           │  │  │        [1 frames hidden]  <built-in>\n",
      "               │        │              │     │           │  │  └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │        │              │     │           │  └─ 0.001 _VariableFunctionsClass.reshape  <built-in>\n",
      "               │        │              │     │           └─ 0.001 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │              │     │              └─ 0.001 permuted  symmray/abelian_core.py:271\n",
      "               │        │              │     └─ 0.001 do  autoray/autoray.py:30\n",
      "               │        │              │        └─ 0.001 align_axes  symmray/interface.py:109\n",
      "               │        │              │           └─ 0.001 U1FermionicArray.align_axes  symmray/abelian_core.py:1838\n",
      "               │        │              │              └─ 0.001 drop_misaligned_sectors  symmray/abelian_core.py:1963\n",
      "               │        │              │                 └─ 0.001 <dictcomp>  symmray/abelian_core.py:1983\n",
      "               │        │              │                    └─ 0.001 <genexpr>  symmray/abelian_core.py:1984\n",
      "               │        │              ├─ 0.006 Tensor.__matmul__  quimb/tensor/tensor_core.py:2911\n",
      "               │        │              │  └─ 0.006 do  autoray/autoray.py:30\n",
      "               │        │              │        [0 frames hidden]  \n",
      "               │        │              │           0.006 wrapper  functools.py:883\n",
      "               │        │              │           └─ 0.006 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │        │              │              ├─ 0.005 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │        │              │              │  └─ 0.005 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │        │              │              │     ├─ 0.002 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │        │              │              │     │  └─ 0.002 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │              │              │     │     ├─ 0.001 _VariableFunctionsClass.reshape  <built-in>\n",
      "               │        │              │              │     │     └─ 0.001 U1FermionicArray.cached_fuse_block_info  symmray/abelian_core.py:694\n",
      "               │        │              │              │     ├─ 0.001 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │        │              │              │     ├─ 0.001 _tensordot_blockwise  symmray/abelian_core.py:1897\n",
      "               │        │              │              │     │  └─ 0.001 <genexpr>  symmray/abelian_core.py:1950\n",
      "               │        │              │              │     │     └─ 0.001 numpy_like  autoray/autoray.py:2034\n",
      "               │        │              │              │     │           [3 frames hidden]  torch, <built-in>\n",
      "               │        │              │              │     └─ 0.001 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │        │              │              │        └─ 0.001 dict.keys  <built-in>\n",
      "               │        │              │              └─ 0.001 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │              │                 └─ 0.001 <genexpr>  symmray/fermionic_core.py:258\n",
      "               │        │              │                    └─ 0.001 U1FermionicArray.symmetry  symmray/abelian_core.py:797\n",
      "               │        │              └─ 0.003 Tensor.transpose_like  quimb/tensor/tensor_core.py:2010\n",
      "               │        │                 └─ 0.003 Tensor.transpose  quimb/tensor/tensor_core.py:1965\n",
      "               │        │                    └─ 0.003 Tensor.modify  quimb/tensor/tensor_core.py:1549\n",
      "               │        │                       └─ 0.003 Tensor._apply_function  quimb/tensor/tensor_core.py:1546\n",
      "               │        │                          └─ 0.003 <lambda>  quimb/tensor/tensor_core.py:2005\n",
      "               │        │                             └─ 0.003 do  autoray/autoray.py:30\n",
      "               │        │                                └─ 0.003 transpose  symmray/interface.py:86\n",
      "               │        │                                   └─ 0.003 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │                                      ├─ 0.002 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │        │                                      │  ├─ 0.001 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │        │                                      │  │  └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │        │                                      │  │        [1 frames hidden]  <built-in>\n",
      "               │        │                                      │  └─ 0.001 dict.items  <built-in>\n",
      "               │        │                                      └─ 0.001 <genexpr>  symmray/fermionic_core.py:258\n",
      "               │        ├─ 0.016 PEPS.compress_plane  quimb/tensor/tensor_2d.py:1062\n",
      "               │        │  └─ 0.016 PEPS.compress_between  quimb/tensor/tensor_core.py:5927\n",
      "               │        │     └─ 0.016 PEPS._compress_between_tids  quimb/tensor/tensor_core.py:5808\n",
      "               │        │        └─ 0.016 wrapper  functools.py:883\n",
      "               │        │           ├─ 0.010 tensor_canonize_bond  quimb/tensor/tensor_core.py:628\n",
      "               │        │           │  ├─ 0.007 Tensor.split  quimb/tensor/tensor_core.py:2239\n",
      "               │        │           │  │  └─ 0.007 wrapper  functools.py:883\n",
      "               │        │           │  │     └─ 0.007 tensor_split  quimb/tensor/tensor_core.py:435\n",
      "               │        │           │  │        ├─ 0.006 Composed.__call__  autoray/autoray.py:921\n",
      "               │        │           │  │        │  └─ 0.006 qr_stabilized  symmray/linalg.py:118\n",
      "               │        │           │  │        │     └─ 0.006 wrapper  functools.py:883\n",
      "               │        │           │  │        │        └─ 0.006 qr_fermionic  symmray/linalg.py:107\n",
      "               │        │           │  │        │           └─ 0.006 qr  symmray/linalg.py:46\n",
      "               │        │           │  │        │              └─ 0.006 _qr  symmray/linalg.py:26\n",
      "               │        │           │  │        │                 └─ 0.006 qr_stabilized  quimb/tensor/decomp.py:669\n",
      "               │        │           │  │        │                    ├─ 0.004 do  autoray/autoray.py:30\n",
      "               │        │           │  │        │                    │     [1 frames hidden]  torch\n",
      "               │        │           │  │        │                    │        0.004 QR.apply  torch/autograd/function.py:558\n",
      "               │        │           │  │        │                    │        └─ 0.003 QRBackward.forward  ../torch_utils.py:158\n",
      "               │        │           │  │        │                    │           ├─ 0.001 _VariableFunctionsClass.isfinite  <built-in>\n",
      "               │        │           │  │        │                    │           ├─ 0.001 SVDforward  ../torch_utils.py:39\n",
      "               │        │           │  │        │                    │           └─ 0.001 [self]  ../torch_utils.py\n",
      "               │        │           │  │        │                    ├─ 0.001 Composed.__call__  autoray/autoray.py:921\n",
      "               │        │           │  │        │                    │  └─ 0.001 sgn  quimb/tensor/decomp.py:95\n",
      "               │        │           │  │        │                    │     └─ 0.001 do  autoray/autoray.py:30\n",
      "               │        │           │  │        │                    │           [1 frames hidden]  <built-in>\n",
      "               │        │           │  │        │                    └─ 0.001 rdmul  quimb/tensor/decomp.py:46\n",
      "               │        │           │  │        └─ 0.001 do  autoray/autoray.py:30\n",
      "               │        │           │  │           └─ 0.001 reshape  symmray/interface.py:58\n",
      "               │        │           │  │              └─ 0.001 U1FermionicArray.reshape  symmray/abelian_core.py:1685\n",
      "               │        │           │  │                 └─ 0.001 U1FermionicArray.unfuse  symmray/fermionic_core.py:638\n",
      "               │        │           │  └─ 0.003 Tensor.__matmul__  quimb/tensor/tensor_core.py:2911\n",
      "               │        │           │     └─ 0.003 do  autoray/autoray.py:30\n",
      "               │        │           │           [0 frames hidden]  \n",
      "               │        │           │              0.003 wrapper  functools.py:883\n",
      "               │        │           │              └─ 0.003 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │        │           │                 └─ 0.003 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │        │           │                    └─ 0.003 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │        │           │                       ├─ 0.001 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │        │           │                       │  └─ 0.001 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │           │                       ├─ 0.001 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │        │           │                       │  └─ 0.001 U1FermionicArray.backend  symmray/block_core.py:60\n",
      "               │        │           │                       │     └─ 0.001 U1FermionicArray.get_any_array  symmray/block_core.py:49\n",
      "               │        │           │                       └─ 0.001 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │        │           └─ 0.006 tensor_compress_bond  quimb/tensor/tensor_core.py:732\n",
      "               │        │              ├─ 0.003 Tensor.split  quimb/tensor/tensor_core.py:2239\n",
      "               │        │              │  └─ 0.003 wrapper  functools.py:883\n",
      "               │        │              │     └─ 0.003 tensor_split  quimb/tensor/tensor_core.py:435\n",
      "               │        │              │        ├─ 0.002 Composed.__call__  autoray/autoray.py:921\n",
      "               │        │              │        │  └─ 0.002 svd_truncated  symmray/linalg.py:229\n",
      "               │        │              │        │     └─ 0.002 wrapper  functools.py:883\n",
      "               │        │              │        │        └─ 0.002 svd_fermionic  symmray/linalg.py:190\n",
      "               │        │              │        │           └─ 0.002 svd  symmray/linalg.py:137\n",
      "               │        │              │        │              ├─ 0.001 SVD.apply  torch/autograd/function.py:558\n",
      "               │        │              │        │              │  └─ 0.001 SVDBackward.forward  ../torch_utils.py:116\n",
      "               │        │              │        │              │     └─ 0.001 SVDforward  ../torch_utils.py:39\n",
      "               │        │              │        │              │        └─ 0.001 is_one  ../torch_utils.py:34\n",
      "               │        │              │        │              │           └─ 0.001 _VariableFunctionsClass.abs  <built-in>\n",
      "               │        │              │        │              └─ 0.001 Composed.__call__  autoray/autoray.py:921\n",
      "               │        │              │        │                    [1 frames hidden]  autoray\n",
      "               │        │              │        └─ 0.001 do  autoray/autoray.py:30\n",
      "               │        │              │           └─ 0.001 fuse  symmray/interface.py:117\n",
      "               │        │              │              └─ 0.001 U1FermionicArray.fuse  symmray/fermionic_core.py:552\n",
      "               │        │              │                 └─ 0.001 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │              │                    └─ 0.001 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │        │              │                       └─ 0.001 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │        │              │                          └─ 0.001 <genexpr>  symmray/abelian_core.py:1529\n",
      "               │        │              │                             └─ 0.001 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │        │              │                                └─ 0.001 translated_function  autoray/autoray.py:1310\n",
      "               │        │              │                                      [1 frames hidden]  <built-in>\n",
      "               │        │              ├─ 0.002 Tensor.__matmul__  quimb/tensor/tensor_core.py:2911\n",
      "               │        │              │  └─ 0.002 do  autoray/autoray.py:30\n",
      "               │        │              │        [0 frames hidden]  \n",
      "               │        │              │           0.002 wrapper  functools.py:883\n",
      "               │        │              │           └─ 0.002 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │        │              │              ├─ 0.001 U1FermionicArray.phase_transpose  symmray/fermionic_core.py:316\n",
      "               │        │              │              │  └─ 0.001 <genexpr>  symmray/fermionic_core.py:335\n",
      "               │        │              │              │     └─ 0.001 U1FermionicArray.symmetry  symmray/abelian_core.py:797\n",
      "               │        │              │              └─ 0.001 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │        │              │                 └─ 0.001 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │        │              │                    └─ 0.001 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │        │              │                       └─ 0.001 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │              │                          └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │        │              │                                [1 frames hidden]  <built-in>\n",
      "               │        │              └─ 0.001 Tensor.transpose_like  quimb/tensor/tensor_core.py:2010\n",
      "               │        │                 └─ 0.001 Tensor.transpose  quimb/tensor/tensor_core.py:1965\n",
      "               │        │                    └─ 0.001 Tensor.modify  quimb/tensor/tensor_core.py:1549\n",
      "               │        │                       └─ 0.001 Tensor._apply_function  quimb/tensor/tensor_core.py:1546\n",
      "               │        │                          └─ 0.001 <lambda>  quimb/tensor/tensor_core.py:2005\n",
      "               │        │                             └─ 0.001 do  autoray/autoray.py:30\n",
      "               │        │                                └─ 0.001 transpose  symmray/interface.py:86\n",
      "               │        │                                   └─ 0.001 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │                                      └─ 0.001 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │        │                                         └─ 0.001 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │        │                                            └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │        ├─ 0.010 PEPS.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │        │  └─ 0.010 PEPS.contract_tags  quimb/tensor/tensor_core.py:8187\n",
      "               │        │     ├─ 0.009 wrapper  functools.py:883\n",
      "               │        │     │  └─ 0.009 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │        │     │     └─ 0.009 array_contract  quimb/tensor/contraction.py:273\n",
      "               │        │     │        └─ 0.009 array_contract  cotengra/interface.py:735\n",
      "               │        │     │              [1 frames hidden]  cotengra\n",
      "               │        │     │                 0.009 wrapper  functools.py:883\n",
      "               │        │     │                 └─ 0.009 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │        │     │                    ├─ 0.007 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │        │     │                    │  └─ 0.007 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │        │     │                    │     ├─ 0.002 _tensordot_blockwise  symmray/abelian_core.py:1897\n",
      "               │        │     │                    │     │  ├─ 0.001 <genexpr>  symmray/abelian_core.py:1950\n",
      "               │        │     │                    │     │  │  └─ 0.001 numpy_like  autoray/autoray.py:2034\n",
      "               │        │     │                    │     │  │        [2 frames hidden]  torch, <built-in>\n",
      "               │        │     │                    │     │  └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │        │     │                    │     ├─ 0.002 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │        │     │                    │     ├─ 0.002 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │        │     │                    │     │  ├─ 0.001 _VariableFunctionsClass.all  <built-in>\n",
      "               │        │     │                    │     │  └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │        │     │                    │     └─ 0.001 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │        │     │                    │        └─ 0.001 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │        │     │                    └─ 0.002 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │        │     │                       ├─ 0.001 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │        │     │                       │  └─ 0.001 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │        │     │                       │     └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │        │     │                       │           [1 frames hidden]  <built-in>\n",
      "               │        │     │                       └─ 0.001 <genexpr>  symmray/fermionic_core.py:258\n",
      "               │        │     │                          └─ 0.001 U1FermionicArray.symmetry  symmray/abelian_core.py:797\n",
      "               │        │     └─ 0.001 PEPS.partition_tensors  quimb/tensor/tensor_core.py:5035\n",
      "               │        │        └─ 0.001 PEPS.pop_tensor  quimb/tensor/tensor_core.py:4023\n",
      "               │        │           └─ 0.001 PEPS._unlink_tags  quimb/tensor/tensor_core.py:3866\n",
      "               │        └─ 0.001 <lambda>  quimb/tensor/tensor_2d.py:261\n",
      "               │           └─ 0.001 PEPS.site_tag  quimb/tensor/tensor_2d.py:455\n",
      "               │              └─ 0.001 str.format  <built-in>\n",
      "               ├─ 0.021 fPEPS.get_amp  ../fermion_utils.py:220\n",
      "               │  ├─ 0.017 TensorNetwork.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │  │  └─ 0.017 TensorNetwork.contract_tags  quimb/tensor/tensor_core.py:8187\n",
      "               │  │     └─ 0.017 wrapper  functools.py:883\n",
      "               │  │           [1 frames hidden]  functools\n",
      "               │  │              0.016 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │  │              └─ 0.016 array_contract  quimb/tensor/contraction.py:273\n",
      "               │  │                 └─ 0.016 array_contract  cotengra/interface.py:735\n",
      "               │  │                       [1 frames hidden]  cotengra\n",
      "               │  │                          0.016 wrapper  functools.py:883\n",
      "               │  │                          └─ 0.016 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │  │                             ├─ 0.008 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │  │                             │  └─ 0.008 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │  │                             │     ├─ 0.002 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │  │                             │     │  └─ 0.002 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │  │                             │     │     ├─ 0.001 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │  │                             │     │     │  └─ 0.001 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │  │                             │     │     └─ 0.001 _VariableFunctionsClass.reshape  <built-in>\n",
      "               │  │                             │     ├─ 0.002 _tensordot_blockwise  symmray/abelian_core.py:1897\n",
      "               │  │                             │     │  └─ 0.002 <genexpr>  symmray/abelian_core.py:1950\n",
      "               │  │                             │     │     └─ 0.002 numpy_like  autoray/autoray.py:2034\n",
      "               │  │                             │     │           [3 frames hidden]  torch, <built-in>\n",
      "               │  │                             │     ├─ 0.002 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │  │                             │     │  ├─ 0.001 _VariableFunctionsClass.all  <built-in>\n",
      "               │  │                             │     │  └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │  │                             │     ├─ 0.001 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │  │                             │     └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │  │                             ├─ 0.005 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │  │                             │  └─ 0.005 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │  │                             │     └─ 0.005 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │  │                             │        ├─ 0.004 torch_transpose  autoray/autoray.py:1974\n",
      "               │  │                             │        │     [1 frames hidden]  <built-in>\n",
      "               │  │                             │        └─ 0.001 [self]  symmray/abelian_core.py\n",
      "               │  │                             ├─ 0.002 U1FermionicArray.size  symmray/abelian_core.py:832\n",
      "               │  │                             │  ├─ 0.001 prod  <built-in>\n",
      "               │  │                             │  └─ 0.001 FermionicArray.shape  symmray/abelian_core.py:827\n",
      "               │  │                             │     └─ 0.001 <genexpr>  symmray/abelian_core.py:830\n",
      "               │  │                             │        └─ 0.001 BlockIndex.size_total  symmray/abelian_core.py:76\n",
      "               │  │                             └─ 0.001 [self]  symmray/fermionic_core.py\n",
      "               │  ├─ 0.002 fPEPS.product_bra_state  ../fermion_utils.py:157\n",
      "               │  │  ├─ 0.001 [self]  ../fermion_utils.py\n",
      "               │  │  └─ 0.001 FermionicArray.from_blocks  symmray/abelian_core.py:1114\n",
      "               │  └─ 0.001 TensorNetwork.conj  quimb/tensor/tensor_core.py:4216\n",
      "               │     └─ 0.001 FermionicArray.phase_flip  symmray/fermionic_core.py:278\n",
      "               │        └─ 0.001 FermionicArray.copy  symmray/fermionic_core.py:180\n",
      "               ├─ 0.006 <dictcomp>  tn_model.py:120\n",
      "               │  └─ 0.006 <dictcomp>  tn_model.py:121\n",
      "               │     └─ 0.006 literal_eval  ast.py:54\n",
      "               │           [9 frames hidden]  ast, <built-in>, ipykernel, threading\n",
      "               ├─ 0.004 PEPS.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │  └─ 0.004 wrapper  functools.py:883\n",
      "               │     └─ 0.004 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │        └─ 0.004 array_contract  quimb/tensor/contraction.py:273\n",
      "               │           └─ 0.004 array_contract  cotengra/interface.py:735\n",
      "               │                 [1 frames hidden]  cotengra\n",
      "               │                    0.004 wrapper  functools.py:883\n",
      "               │                    └─ 0.004 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │                       ├─ 0.002 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │                       │  └─ 0.002 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │                       │     ├─ 0.001 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │                       │     │  └─ 0.001 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │                       │     │     └─ 0.001 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │                       │     │        └─ 0.001 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │                       │     │           └─ 0.001 <genexpr>  symmray/abelian_core.py:1529\n",
      "               │                       │     │              └─ 0.001 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │                       │     │                 └─ 0.001 translated_function  autoray/autoray.py:1310\n",
      "               │                       │     │                       [1 frames hidden]  <built-in>\n",
      "               │                       │     └─ 0.001 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │                       └─ 0.002 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │                          ├─ 0.001 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │                          │  └─ 0.001 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │                          │     └─ 0.001 torch_transpose  autoray/autoray.py:1974\n",
      "               │                          └─ 0.001 permuted  symmray/abelian_core.py:271\n",
      "               └─ 0.001 Tensor.__iter__  torch/_tensor.py:1033\n",
      "                     [1 frames hidden]  <built-in>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pyinstrument.Profiler() as prof:\n",
    "    model(random_config)\n",
    "prof.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            output_size=1,\n",
    "            phys_dim=2,\n",
    "            d_model=128, \n",
    "            nhead=8, \n",
    "            num_encoder_layers=6, \n",
    "            num_decoder_layers=6, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.1\n",
    "        ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Embedding layer for integer input sequence\n",
    "        self.embedding = nn.Embedding(phys_dim, d_model)\n",
    "        # Embedding layer for floating-point input sequence\n",
    "        self.float_embedding = nn.Linear(1, d_model)\n",
    "        # Positional encoding for fixed-length sequences\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        # Transformer layers\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        # Linear layer for output generation\n",
    "        self.fc_out = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = src.transpose(0, 1) # [seq_len, batch_size]\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "        # Encode source (input) sequence\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.embedding.embedding_dim, dtype=torch.float32))\n",
    "        src = self.pos_encoder(src)\n",
    "        # Encode target (output) sequence\n",
    "        tgt = self.float_embedding(tgt) * torch.sqrt(torch.tensor(self.float_embedding.out_features, dtype=torch.float32))\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        # Apply transformer\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        # Generate output for each position in the sequence\n",
    "        output = self.fc_out(output)\n",
    "        # Transpose output to match the shape [batch_size, seq_len, output_size]\n",
    "        output = output.transpose(0, 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_config = torch.tensor(H.hilbert.random_state(key=jax.random.PRNGKey(1)))\n",
    "transformer_model = TransformerModel(phys_dim=4, num_encoder_layers=1, num_decoder_layers=1, dim_feedforward=2**7, d_model=2**7, nhead=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps.phys_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 32, 1]),\n",
       " torch.Size([1, 16]),\n",
       " torch.Size([1, 32, 1]),\n",
       " tensor([ 0.1063,  0.2415,  0.0898, -0.0838,  0.3662,  0.1214,  0.1259,  0.1299,\n",
       "          0.3637,  0.4221,  0.3693,  0.2376,  0.5999,  0.4905, -0.2253,  0.0418,\n",
       "          0.2510, -0.2236,  0.2471,  0.2663,  0.1220,  0.3799,  0.0450,  0.6811,\n",
       "          0.3537,  0.8089,  0.4712,  0.1701,  0.3547,  0.5257,  0.4483,  0.1982],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amp = peps.get_amp(random_config)\n",
    "# amp_w_proj = insert_proj_peps(amp, max_bond=4, yrange=[0,peps.Ly-2], lazy=True)\n",
    "\n",
    "# tn, proj_tn = amp_w_proj.partition('proj')\n",
    "input = random_config.unsqueeze(0)\n",
    "target = torch.randn(1, 32, 1)\n",
    "output = transformer_model(input, target)\n",
    "output.shape, input.shape, target.shape, output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.fermion_utils import flatten_proj_params, reconstruct_proj_params, insert_compressor\n",
    "import ast\n",
    "from quimb.tensor.tensor_core import bonds, tags_to_oset, rand_uuid\n",
    "from quimb.tensor.tensor_2d import Rotator2D, pairwise\n",
    "class fTN_Transformer_Proj_lazy_Model(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            ftn, \n",
    "            max_bond, \n",
    "            nn_eta=1e-3, \n",
    "            d_model=128, \n",
    "            nhead=8, \n",
    "            num_encoder_layers=6, \n",
    "            num_decoder_layers=6, \n",
    "            dim_feedforward=512, \n",
    "            dropout=0.1,\n",
    "            param_dtype=torch.float32,\n",
    "            lazy=False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.max_bond = max_bond\n",
    "        self.nn_eta = nn_eta\n",
    "        self.phys_dim = ftn.phys_dim()\n",
    "        self.param_dtype = param_dtype\n",
    "        self.lazy = lazy\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "        \n",
    "        self.charge_config = [array.charge for array in ftn.arrays]\n",
    "        self.N_fermion = sum(self.charge_config)\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "        assert self.symmetry == 'Z2' or self.symmetry == 'U1', \"Only Z2 or U1 symmetry fPEPS is supported for Transformer insertion now.\"\n",
    "        if self.symmetry == 'Z2':\n",
    "            assert self.N_fermion %2 == sum(self.charge_config) % 2, \"The number of fermions must match the parity of the Z2-TNS.\"\n",
    "        \n",
    "        # Transformer model\n",
    "        self.d_model = d_model # embedding dimension\n",
    "        self.transformer = TransformerModel(\n",
    "            output_size=1,\n",
    "            phys_dim=self.phys_dim,\n",
    "            d_model=self.d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers, \n",
    "            num_decoder_layers=num_decoder_layers, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS (transformer-two-row)':{'D': ftn.max_bond(), 'chi': self.max_bond, 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry},\n",
    "            'transformer':{'input_size': ftn.nsites, 'output_size': 1}\n",
    "        }\n",
    "\n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    @property\n",
    "    def num_tn_params(self):\n",
    "        num=0\n",
    "        for tid, blk_array in self.torch_tn_params.items():\n",
    "            for sector, data in blk_array.items():\n",
    "                num += data.numel()\n",
    "        return num\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        pointer = 0\n",
    "        for param, shape in zip(self.parameters(), self.param_shapes):\n",
    "            num_param = param.numel()\n",
    "            new_param_values = new_params[pointer:pointer+num_param].view(shape)\n",
    "            with torch.no_grad():\n",
    "                param.copy_(new_param_values)\n",
    "            pointer += num_param\n",
    "    \n",
    "    def add_transformer_values(self, proj_tn, x_i):\n",
    "        \"\"\"Obtain the new amplitude with projectors TN by adding the output of the transformer to the projectors.\"\"\"\n",
    "        proj_params, proj_skeleton = qtn.pack(proj_tn)\n",
    "        proj_params_vec = flatten_proj_params(proj_params)\n",
    "\n",
    "        # Check x_i type\n",
    "        if not type(x_i) == torch.Tensor or x_i.dtype != torch.int32:\n",
    "            x_i = torch.tensor(x_i, dtype=torch.int32)\n",
    "\n",
    "        # Input of the transformer\n",
    "        src = x_i.unsqueeze(0) # Shape: [batch_size==1, seq_len]\n",
    "        # Target of the transformer\n",
    "        tgt = torch.tensor(proj_params_vec, dtype=self.param_dtype).unsqueeze(0) # Shape: [batch_size==1, seq_len]\n",
    "        tgt.unsqueeze_(-1) # Shape: [batch_size==1, seq_len, 1]\n",
    "        # Forward pass\n",
    "        nn_output = self.transformer(src, tgt)\n",
    "        # concatenate the output to get the final vector of length vec_len\n",
    "        nn_output = nn_output.view(-1)\n",
    "        # Add NN output\n",
    "        proj_params_vec = proj_params_vec + self.nn_eta*nn_output\n",
    "        # Reconstruct the proj parameters\n",
    "        new_proj_params = reconstruct_proj_params(proj_params_vec, proj_params)\n",
    "        # Load the new parameters\n",
    "        new_proj_tn = qtn.unpack(new_proj_params, proj_skeleton)\n",
    "\n",
    "        return new_proj_tn\n",
    "\n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "            # Insert projectors\n",
    "            \"\"\"Insert projectors in a PEPS along the x direction towards y direction.\"\"\"\n",
    "            r = Rotator2D(amp, xrange=None, yrange=[0, psi.Ly-2], from_which='ymin')\n",
    "            tn_calc = amp.copy()\n",
    "            tn_body_nn_value = None\n",
    "            for i, inext in pairwise(r.sweep):\n",
    "                i_passed = [x for x in range(i)]\n",
    "                for j in r.sweep_other:\n",
    "                    # this handles cyclic boundary conditions\n",
    "                    jnext = r.get_jnext(j)\n",
    "                    if jnext is not None:\n",
    "                        ltags = tuple([r.site_tag(ip, j) for ip in i_passed])+(r.site_tag(i, j), r.site_tag(inext, j))\n",
    "                        rtags = tuple([r.site_tag(ip, jnext) for ip in i_passed])+(r.site_tag(i, jnext), r.site_tag(inext, jnext))\n",
    "                        new_ltags = (r.site_tag(inext, j),)\n",
    "                        new_rtags = (r.site_tag(inext, jnext),)\n",
    "                        #      │         │\n",
    "                        #    ──O─┐ chi ┌─O──  j+1\n",
    "                        #      │ └─▷═◁─┘│\n",
    "                        #      │ ┌┘   └┐ │\n",
    "                        #    ──O─┘     └─O──  j\n",
    "                        #     i+1        i\n",
    "                        tn_calc = insert_compressor(\n",
    "                            tn_calc,\n",
    "                            ltags,\n",
    "                            rtags,\n",
    "                            new_ltags=new_ltags,\n",
    "                            new_rtags=new_rtags,\n",
    "                            max_bond=self.max_bond,\n",
    "                        )\n",
    "                \n",
    "                tn_body, proj_tn = tn_calc.partition('proj') # projectors computed from untouched TN\n",
    "                _, tn_body_skeleton = qtn.pack(tn_body)\n",
    "                new_proj_tn = self.add_transformer_values(proj_tn, x_i) # Add transformer values to the projectors\n",
    "                \n",
    "                if tn_body_nn_value is None:\n",
    "                    tn_body_nn = tn_body.copy()\n",
    "                else:\n",
    "                    tn_body_nn = reload_tn(tn_body_skeleton, tn_body_nn_value)\n",
    "                \n",
    "                tn_calc_nn = tn_body_nn | new_proj_tn\n",
    "                \n",
    "                # contract each pair of boundary tensors with their projectors\n",
    "                for j in r.sweep_other:\n",
    "                    tn_calc.contract_tags_(\n",
    "                        (r.site_tag(i, j), r.site_tag(inext, j)),\n",
    "                    )\n",
    "                    tn_calc_nn.contract_tags_(\n",
    "                        (r.site_tag(i, j), r.site_tag(inext, j)),\n",
    "                    )\n",
    "                tn_body_nn_value = tn_calc_nn.copy()\n",
    "\n",
    "            amp_value = tn_calc_nn.contract()\n",
    "            if amp_value == 0:\n",
    "                amp_value = torch.tensor(0.0, dtype=self.param_dtype)\n",
    "            batch_amps.append(amp_value)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)\n",
    "    \n",
    "def reload_tn(old_tn, new_value_tn):\n",
    "    \"\"\"Reload the data of the tensors in old_tn with the data of the tensors with the same tags in new_value_tn.\"\"\"\n",
    "    for old_ts in old_tn:\n",
    "        tags = old_ts.tags\n",
    "        new_ts_ = new_value_tn.select_tensors(tags)\n",
    "        assert len(new_ts_) == 1, 'Tensor must be unique.'\n",
    "        new_ts = new_ts_[0]\n",
    "        # print(old_ts.inds, new_ts.inds, old_ts.data.shape, new_ts.data.shape)\n",
    "        old_ts.modify(data=new_ts.data)\n",
    "    return old_tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/tmp/ipykernel_23605/3718476108.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_i = torch.tensor(x_i, dtype=torch.int32)\n",
      "/tmp/ipykernel_23605/3718476108.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt = torch.tensor(proj_params_vec, dtype=self.param_dtype).unsqueeze(0) # Shape: [batch_size==1, seq_len]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2.6328e-06], dtype=torch.float64, grad_fn=<StackBackward0>),\n",
       " tensor(-2.5298e-12, dtype=torch.float64))"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_model = fTN_Transformer_Proj_lazy_Model(\n",
    "    peps,\n",
    "    max_bond=chi,\n",
    "    nn_eta=1.0,\n",
    "    d_model=32,\n",
    "    nhead=2,\n",
    "    num_encoder_layers=1,\n",
    "    num_decoder_layers=1,\n",
    "    dim_feedforward=32,\n",
    "    dropout=0.0,\n",
    ")\n",
    "peps_model(random_config), peps.get_amp(random_config).contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23605/3718476108.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_i = torch.tensor(x_i, dtype=torch.int32)\n",
      "/tmp/ipykernel_23605/3718476108.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt = torch.tensor(proj_params_vec, dtype=self.param_dtype).unsqueeze(0) # Shape: [batch_size==1, seq_len]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.1678e-04,  3.6532e-04,  0.0000e+00,  ...,  2.4260e-05,\n",
       "        -6.7946e-05, -2.5571e-05], dtype=torch.float64)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_model.zero_grad()\n",
    "peps_model(random_config).backward()\n",
    "peps_model.params_grad_to_vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19217])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_model.from_params_to_vec().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 19:08:59  Samples:  163\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.165     CPU time: 0.165\n",
      "/   _/                      v4.7.3\n",
      "\n",
      "Profile at /tmp/ipykernel_23605/817398746.py:1\n",
      "\n",
      "0.165 <module>  ../../../../../tmp/ipykernel_23605/817398746.py:1\n",
      "└─ 0.165 fTN_Transformer_Proj_lazy_Model._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      "      [0 frames hidden]  \n",
      "         0.165 fTN_Transformer_Proj_lazy_Model._call_impl  torch/nn/modules/module.py:1555\n",
      "         └─ 0.165 fTN_Transformer_Proj_lazy_Model.forward  ../../../../../tmp/ipykernel_23605/3718476108.py:202\n",
      "            └─ 0.163 fTN_Transformer_Proj_lazy_Model.amplitude  ../../../../../tmp/ipykernel_23605/3718476108.py:127\n",
      "               ├─ 0.080 insert_compressor  ../fermion_utils.py:567\n",
      "               │  ├─ 0.036 Tensor.split  quimb/tensor/tensor_core.py:2239\n",
      "               │  │  └─ 0.036 wrapper  functools.py:883\n",
      "               │  │     └─ 0.036 tensor_split  quimb/tensor/tensor_core.py:435\n",
      "               │  │        ├─ 0.023 Composed.__call__  autoray/autoray.py:921\n",
      "               │  │        │  ├─ 0.010 qr_stabilized  symmray/linalg.py:118\n",
      "               │  │        │  │  └─ 0.010 wrapper  functools.py:883\n",
      "               │  │        │  │     └─ 0.010 qr_fermionic  symmray/linalg.py:107\n",
      "               │  │        │  │        └─ 0.010 qr  symmray/linalg.py:46\n",
      "               │  │        │  │           └─ 0.010 _qr  symmray/linalg.py:26\n",
      "               │  │        │  │              └─ 0.010 qr_stabilized  quimb/tensor/decomp.py:669\n",
      "               │  │        │  │                 └─ 0.010 do  autoray/autoray.py:30\n",
      "               │  │        │  │                       [0 frames hidden]  \n",
      "               │  │        │  │                          0.010 QR.apply  torch/autograd/function.py:558\n",
      "               │  │        │  │                          └─ 0.010 QRBackward.forward  ../torch_utils.py:158\n",
      "               │  │        │  │                             └─ 0.006 SVDforward  ../torch_utils.py:39\n",
      "               │  │        │  │                                ├─ 0.003 linalg_svd  <built-in>\n",
      "               │  │        │  │                                └─ 0.003 [self]  ../torch_utils.py\n",
      "               │  │        │  ├─ 0.008 lq_stabilized  quimb/tensor/decomp.py:710\n",
      "               │  │        │  │  └─ 0.008 Composed.__call__  autoray/autoray.py:921\n",
      "               │  │        │  │     └─ 0.008 qr_stabilized  symmray/linalg.py:118\n",
      "               │  │        │  │        └─ 0.008 wrapper  functools.py:883\n",
      "               │  │        │  │           └─ 0.008 qr_fermionic  symmray/linalg.py:107\n",
      "               │  │        │  │              └─ 0.007 qr  symmray/linalg.py:46\n",
      "               │  │        │  │                 └─ 0.007 _qr  symmray/linalg.py:26\n",
      "               │  │        │  │                    └─ 0.007 qr_stabilized  quimb/tensor/decomp.py:669\n",
      "               │  │        │  │                       └─ 0.006 do  autoray/autoray.py:30\n",
      "               │  │        │  │                             [0 frames hidden]  \n",
      "               │  │        │  │                                0.005 QR.apply  torch/autograd/function.py:558\n",
      "               │  │        │  │                                └─ 0.004 QRBackward.forward  ../torch_utils.py:158\n",
      "               │  │        │  │                                   └─ 0.003 SVDforward  ../torch_utils.py:39\n",
      "               │  │        │  │                                      └─ 0.002 _VariableFunctionsClass.isfinite  <built-in>\n",
      "               │  │        │  └─ 0.005 svd_truncated  symmray/linalg.py:229\n",
      "               │  │        │     └─ 0.005 wrapper  functools.py:883\n",
      "               │  │        │        └─ 0.005 svd_fermionic  symmray/linalg.py:190\n",
      "               │  │        │           └─ 0.005 svd  symmray/linalg.py:137\n",
      "               │  │        │              └─ 0.004 SVD.apply  torch/autograd/function.py:558\n",
      "               │  │        │                 └─ 0.004 SVDBackward.forward  ../torch_utils.py:116\n",
      "               │  │        │                    └─ 0.004 SVDforward  ../torch_utils.py:39\n",
      "               │  │        ├─ 0.009 do  autoray/autoray.py:30\n",
      "               │  │        │  ├─ 0.006 fuse  symmray/interface.py:117\n",
      "               │  │        │  │  └─ 0.006 U1FermionicArray.fuse  symmray/fermionic_core.py:552\n",
      "               │  │        │  │     ├─ 0.004 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │  │        │  │     │  └─ 0.002 torch_transpose  autoray/autoray.py:1974\n",
      "               │  │        │  │     │        [1 frames hidden]  <built-in>\n",
      "               │  │        │  │     └─ 0.002 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │  │        │  └─ 0.003 reshape  symmray/interface.py:58\n",
      "               │  │        │     └─ 0.003 U1FermionicArray.reshape  symmray/abelian_core.py:1685\n",
      "               │  │        │        └─ 0.002 U1FermionicArray.unfuse  symmray/fermionic_core.py:638\n",
      "               │  │        │           └─ 0.002 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │  │        └─ 0.002 Tensor.transpose  quimb/tensor/tensor_core.py:1965\n",
      "               │  │           └─ 0.002 Tensor.modify  quimb/tensor/tensor_core.py:1549\n",
      "               │  ├─ 0.034 TensorNetwork.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │  │  └─ 0.034 wrapper  functools.py:883\n",
      "               │  │     └─ 0.034 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │  │        ├─ 0.028 array_contract  quimb/tensor/contraction.py:273\n",
      "               │  │        │  └─ 0.028 array_contract  cotengra/interface.py:735\n",
      "               │  │        │        [1 frames hidden]  cotengra\n",
      "               │  │        │           0.028 wrapper  functools.py:883\n",
      "               │  │        │           └─ 0.028 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │  │        │              ├─ 0.021 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │  │        │              │  └─ 0.021 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │  │        │              │     ├─ 0.006 _tensordot_blockwise  symmray/abelian_core.py:1897\n",
      "               │  │        │              │     │  └─ 0.005 <genexpr>  symmray/abelian_core.py:1950\n",
      "               │  │        │              │     │     └─ 0.005 numpy_like  autoray/autoray.py:2034\n",
      "               │  │        │              │     │           [2 frames hidden]  torch, <built-in>\n",
      "               │  │        │              │     ├─ 0.005 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │  │        │              │     │  ├─ 0.003 [self]  symmray/abelian_core.py\n",
      "               │  │        │              │     │  └─ 0.002 _VariableFunctionsClass.reshape  <built-in>\n",
      "               │  │        │              │     ├─ 0.005 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │  │        │              │     │  └─ 0.005 _VariableFunctionsClass.all  <built-in>\n",
      "               │  │        │              │     ├─ 0.003 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │  │        │              │     │  └─ 0.003 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │  │        │              │     └─ 0.002 drop_misaligned_sectors  symmray/abelian_core.py:1963\n",
      "               │  │        │              ├─ 0.004 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │  │        │              │  └─ 0.004 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │  │        │              │     └─ 0.003 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │  │        │              │        └─ 0.002 torch_transpose  autoray/autoray.py:1974\n",
      "               │  │        │              │              [1 frames hidden]  <built-in>\n",
      "               │  │        │              └─ 0.002 U1FermionicArray.phase_flip  symmray/fermionic_core.py:278\n",
      "               │  │        │                 └─ 0.002 <genexpr>  symmray/fermionic_core.py:302\n",
      "               │  │        └─ 0.003 oset_union  quimb/tensor/tensor_core.py:115\n",
      "               │  │           └─ 0.003 oset.__init__  quimb/utils.py:338\n",
      "               │  ├─ 0.002 PEPS.partition  quimb/tensor/tensor_core.py:5067\n",
      "               │  └─ 0.002 TensorNetwork.__or__  quimb/tensor/tensor_core.py:3747\n",
      "               ├─ 0.039 TensorNetwork.contract_tags  quimb/tensor/tensor_core.py:8187\n",
      "               │  └─ 0.037 wrapper  functools.py:883\n",
      "               │     └─ 0.037 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │        └─ 0.037 array_contract  quimb/tensor/contraction.py:273\n",
      "               │           └─ 0.037 array_contract  cotengra/interface.py:735\n",
      "               │                 [1 frames hidden]  cotengra\n",
      "               │                    0.035 wrapper  functools.py:883\n",
      "               │                    └─ 0.035 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │                       ├─ 0.029 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │                       │  └─ 0.028 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │                       │     ├─ 0.014 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │                       │     │  └─ 0.014 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │                       │     │     ├─ 0.006 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │                       │     │     │  └─ 0.006 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │                       │     │     │     ├─ 0.003 <genexpr>  symmray/abelian_core.py:1529\n",
      "               │                       │     │     │     │  └─ 0.003 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │                       │     │     │     │     └─ 0.003 translated_function  autoray/autoray.py:1310\n",
      "               │                       │     │     │     │           [1 frames hidden]  <built-in>\n",
      "               │                       │     │     │     └─ 0.002 [self]  symmray/abelian_core.py\n",
      "               │                       │     │     ├─ 0.003 _VariableFunctionsClass.reshape  <built-in>\n",
      "               │                       │     │     └─ 0.002 [self]  symmray/abelian_core.py\n",
      "               │                       │     ├─ 0.006 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │                       │     │  ├─ 0.004 [self]  symmray/abelian_core.py\n",
      "               │                       │     │  └─ 0.002 _VariableFunctionsClass.all  <built-in>\n",
      "               │                       │     ├─ 0.005 U1FermionicArray.unfuse  symmray/abelian_core.py:1602\n",
      "               │                       │     │  ├─ 0.002 [self]  symmray/abelian_core.py\n",
      "               │                       │     │  └─ 0.002 <genexpr>  symmray/abelian_core.py:1648\n",
      "               │                       │     │     └─ 0.002 BlockIndex.size_of  symmray/abelian_core.py:121\n",
      "               │                       │     └─ 0.002 _tensordot_blockwise  symmray/abelian_core.py:1897\n",
      "               │                       └─ 0.005 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │                          ├─ 0.002 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               │                          │  └─ 0.002 <dictcomp>  symmray/abelian_core.py:1307\n",
      "               │                          │     └─ 0.002 torch_transpose  autoray/autoray.py:1974\n",
      "               │                          │           [1 frames hidden]  <built-in>\n",
      "               │                          └─ 0.002 [self]  symmray/fermionic_core.py\n",
      "               ├─ 0.018 fPEPS.get_amp  ../fermion_utils.py:220\n",
      "               │  └─ 0.017 TensorNetwork.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │     └─ 0.017 TensorNetwork.contract_tags  quimb/tensor/tensor_core.py:8187\n",
      "               │        └─ 0.017 wrapper  functools.py:883\n",
      "               │           └─ 0.017 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │              └─ 0.016 array_contract  quimb/tensor/contraction.py:273\n",
      "               │                 └─ 0.016 array_contract  cotengra/interface.py:735\n",
      "               │                       [1 frames hidden]  cotengra\n",
      "               │                          0.015 wrapper  functools.py:883\n",
      "               │                          └─ 0.015 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │                             ├─ 0.011 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │                             │  └─ 0.011 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               │                             │     ├─ 0.006 U1FermionicArray.fuse  symmray/abelian_core.py:1548\n",
      "               │                             │     │  └─ 0.005 U1FermionicArray._fuse_core  symmray/abelian_core.py:1444\n",
      "               │                             │     │     └─ 0.002 <dictcomp>  symmray/abelian_core.py:1536\n",
      "               │                             │     │        └─ 0.002 _recurse_concat  symmray/abelian_core.py:1496\n",
      "               │                             │     └─ 0.003 U1FermionicArray.drop_missing_blocks  symmray/abelian_core.py:916\n",
      "               │                             └─ 0.004 U1FermionicArray.transpose  symmray/fermionic_core.py:228\n",
      "               │                                └─ 0.002 U1FermionicArray.transpose  symmray/abelian_core.py:1296\n",
      "               ├─ 0.007 fTN_Transformer_Proj_lazy_Model.add_transformer_values  ../../../../../tmp/ipykernel_23605/3718476108.py:100\n",
      "               │  └─ 0.004 TransformerModel._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      "               │        [0 frames hidden]  \n",
      "               │           0.004 TransformerModel._call_impl  torch/nn/modules/module.py:1555\n",
      "               │           └─ 0.004 TransformerModel.forward  ../../../../../tmp/ipykernel_23605/4182114079.py:41\n",
      "               │              └─ 0.004 Transformer._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      "               │                    [14 frames hidden]  torch, <built-in>\n",
      "               ├─ 0.006 <dictcomp>  ../../../../../tmp/ipykernel_23605/3718476108.py:129\n",
      "               │  └─ 0.006 <dictcomp>  ../../../../../tmp/ipykernel_23605/3718476108.py:130\n",
      "               │     └─ 0.006 literal_eval  ast.py:54\n",
      "               │           [3 frames hidden]  ast, <built-in>\n",
      "               ├─ 0.004 TensorNetwork.contract  quimb/tensor/tensor_core.py:8297\n",
      "               │  └─ 0.004 wrapper  functools.py:883\n",
      "               │     └─ 0.004 tensor_contract  quimb/tensor/tensor_core.py:207\n",
      "               │        └─ 0.004 array_contract  quimb/tensor/contraction.py:273\n",
      "               │           └─ 0.004 array_contract  cotengra/interface.py:735\n",
      "               │                 [1 frames hidden]  cotengra\n",
      "               │                    0.004 wrapper  functools.py:883\n",
      "               │                    └─ 0.004 tensordot_fermionic  symmray/fermionic_core.py:725\n",
      "               │                       └─ 0.002 tensordot_abelian  symmray/abelian_core.py:2063\n",
      "               │                          └─ 0.002 _tensordot_via_fused  symmray/abelian_core.py:2005\n",
      "               ├─ 0.002 [self]  ../../../../../tmp/ipykernel_23605/3718476108.py\n",
      "               └─ 0.002 pack  quimb/tensor/interface.py:21\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23605/3718476108.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_i = torch.tensor(x_i, dtype=torch.int32)\n",
      "/tmp/ipykernel_23605/3718476108.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tgt = torch.tensor(proj_params_vec, dtype=self.param_dtype).unsqueeze(0) # Shape: [batch_size==1, seq_len]\n"
     ]
    }
   ],
   "source": [
    "with pyinstrument.Profiler() as prof:\n",
    "    peps_model(random_config)\n",
    "prof.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
