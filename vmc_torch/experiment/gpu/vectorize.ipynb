{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "\n",
    "from vmc_torch.experiment.tn_model import fMPSModel, fMPS_backflow_Model, fMPS_backflow_attn_Tensorwise_Model_v1\n",
    "from vmc_torch.experiment.tn_model import init_weights_to_zero\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, SR, Adam, SGD_momentum, DecayScheduler\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian import spinful_Fermi_Hubbard_chain, spinful_Fermi_Hubbard_chain_quimb\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "from vmc_torch.utils import closest_divisible\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/data'\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "L = int(10)\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(L-2)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_chain(L, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "quimb_ham = spinful_Fermi_Hubbard_chain_quimb(L, t, U, mu=0.0, pbc=False, symmetry=symmetry)\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 8\n",
    "chi = -2\n",
    "dtype=torch.float64\n",
    "\n",
    "# Load mps\n",
    "skeleton = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_skeleton.pkl\", \"rb\"))\n",
    "mps_params = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_su_params.pkl\", \"rb\"))\n",
    "mps = qtn.unpack(mps_params, skeleton)\n",
    "# fmps_tnf = form_gated_fmps_tnf(fmps=mps, ham=quimb_ham, depth=2)\n",
    "mps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "\n",
    "# # randomize the mps tensors\n",
    "# mps.apply_to_arrays(lambda x: torch.randn_like(torch.tensor(x, dtype=dtype), dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = int(15000)\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples/SIZE)%2 != 0:\n",
    "    N_samples += SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from vmc_torch.experiment.tn_model import wavefunctionModel, fMPSModel\n",
    "\n",
    "class fMPSModel_GPU(wavefunctionModel):\n",
    "    def __init__(self, ftn, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fMPS (exact contraction)':{'D': ftn.max_bond(), 'L': ftn.L, 'symmetry': self.symmetry, 'cyclic': ftn.cyclic, 'skeleton': self.skeleton},\n",
    "        }\n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        \n",
    "        # Ensure x is a tensor of the correct dtype and move to GPU\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=self.param_dtype)\n",
    "        elif x.dtype != self.param_dtype:\n",
    "            x = x.to(self.param_dtype)\n",
    "        \n",
    "        # Move x to GPU and enable gradient computation\n",
    "        x = x.to('cuda')\n",
    "\n",
    "        # Get model parameters list\n",
    "        params_list = list(self.parameters())\n",
    "\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Get the amplitude\n",
    "            with torch.no_grad():\n",
    "                amp = psi.get_amp(x_i, conj=True)\n",
    "                amp_val = amp.contract()\n",
    "            if amp_val == 0.0:\n",
    "                amp_val = torch.tensor(0.0, device='cuda')\n",
    "            batch_amps.append(amp_val)\n",
    "        \n",
    "        # Stack the amplitudes into a tensor\n",
    "        batch_amps = torch.stack(batch_amps).to('cuda')\n",
    "        return batch_amps\n",
    "\n",
    "    def amplitude_grad(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        \n",
    "        # Ensure x is a tensor of the correct dtype and move to GPU\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=self.param_dtype)\n",
    "        elif x.dtype != self.param_dtype:\n",
    "            x = x.to(self.param_dtype)\n",
    "        \n",
    "        # # Move x to GPU and enable gradient computation\n",
    "        # x = x.to('cuda')\n",
    "\n",
    "        # Get model parameters list\n",
    "        params_list = list(self.parameters())\n",
    "\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Get the amplitude\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "            amp_val = amp.contract()\n",
    "            if amp_val == 0.0:\n",
    "                amp_val = torch.tensor(0.0, device='cuda')\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Stack the amplitudes into a tensor\n",
    "        batch_amps = torch.stack(batch_amps).to('cuda')\n",
    "\n",
    "        # Compute gradients with respect to the parameters\n",
    "        gradients = []\n",
    "        for amp in batch_amps:\n",
    "            grad = torch.autograd.grad(amp, self.parameters(), retain_graph=True, allow_unused=True)\n",
    "            flatten_grad = []\n",
    "            for i in range(len(grad)):\n",
    "                if grad[i] is None:\n",
    "                    flatten_grad.append(torch.zeros_like(params_list[i]))\n",
    "                else:\n",
    "                    flatten_grad.append(grad[i])\n",
    "            gradients.append(torch.cat([g.flatten() for g in flatten_grad]))\n",
    "        # Stack the gradients into a tensor\n",
    "        gradients = torch.stack(gradients)\n",
    "\n",
    "        return batch_amps, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fMPSModel_GPU(\n",
       "  (torch_tn_params): ModuleDict(\n",
       "    (0): ParameterDict(\n",
       "        ((0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 2x2 (cuda:0)]\n",
       "        ((1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 2x2 (cuda:0)]\n",
       "    )\n",
       "    (1): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 2x4x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 2x4x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 2x4x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 2x4x2 (cuda:0)]\n",
       "    )\n",
       "    (2): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x4x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x4x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x4x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x4x2 (cuda:0)]\n",
       "    )\n",
       "    (3): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x3x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x5x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x3x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x5x2 (cuda:0)]\n",
       "    )\n",
       "    (4): ParameterDict(\n",
       "        ((0, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 3x3x2 (cuda:0)]\n",
       "        ((0, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 3x5x2 (cuda:0)]\n",
       "        ((1, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 5x3x2 (cuda:0)]\n",
       "        ((1, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 5x5x2 (cuda:0)]\n",
       "    )\n",
       "    (5): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 3x4x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 3x4x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 5x4x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 5x4x2 (cuda:0)]\n",
       "    )\n",
       "    (6): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x5x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x3x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 4x5x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 4x3x2 (cuda:0)]\n",
       "    )\n",
       "    (7): ParameterDict(\n",
       "        ((0, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 5x3x2 (cuda:0)]\n",
       "        ((0, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 5x5x2 (cuda:0)]\n",
       "        ((1, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 3x3x2 (cuda:0)]\n",
       "        ((1, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 3x5x2 (cuda:0)]\n",
       "    )\n",
       "    (8): ParameterDict(\n",
       "        ((0, 0, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 3x2x2 (cuda:0)]\n",
       "        ((0, 1, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 3x2x2 (cuda:0)]\n",
       "        ((1, 0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 5x2x2 (cuda:0)]\n",
       "        ((1, 1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 5x2x2 (cuda:0)]\n",
       "    )\n",
       "    (9): ParameterDict(\n",
       "        ((0, 1)): Parameter containing: [torch.cuda.DoubleTensor of size 2x2 (cuda:0)]\n",
       "        ((1, 0)): Parameter containing: [torch.cuda.DoubleTensor of size 2x2 (cuda:0)]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fMPSModel_GPU(mps, dtype=dtype)\n",
    "model1 = fMPSModel(mps, dtype=dtype)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 22:43:56  Samples:  43\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.056     CPU time: 0.056\n",
      "/   _/                      v4.7.3\n",
      "\n",
      "Profile at /tmp/ipykernel_12453/2548696919.py:6\n",
      "\n",
      "\u001b[31m0.055\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<module>\u001b[0m  \u001b[2m../../../../../tmp/ipykernel_12453/2548696919.py:6\u001b[0m\n",
      "└─ \u001b[31m0.055\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPSModel_GPU.amplitude_grad\u001b[0m  \u001b[2m../../../../../tmp/ipykernel_12453/269258344.py:69\u001b[0m\n",
      "   ├─ \u001b[33m0.032\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.get_amp\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py:471\u001b[0m\n",
      "   │  ├─ \u001b[33m0.027\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8438\u001b[0m\n",
      "   │  │  └─ \u001b[33m0.027\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.contract_tags\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8328\u001b[0m\n",
      "   │  │     ├─ \u001b[33m0.025\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "   │  │     │  └─ \u001b[33m0.025\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensor_contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:207\u001b[0m\n",
      "   │  │     │     └─ \u001b[33m0.025\u001b[0m \u001b[48;5;24m\u001b[38;5;15marray_contract\u001b[0m  \u001b[2mquimb/tensor/contraction.py:273\u001b[0m\n",
      "   │  │     │        └─ \u001b[33m0.025\u001b[0m array_contract\u001b[0m  \u001b[2mcotengra/interface.py:735\u001b[0m\n",
      "   │  │     │              [1 frames hidden]  \u001b[2mcotengra\u001b[0m\n",
      "   │  │     │                 \u001b[33m0.025\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "   │  │     │                 └─ \u001b[33m0.025\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_fermionic\u001b[0m  \u001b[2msymmray/fermionic_core.py:762\u001b[0m\n",
      "   │  │     │                    ├─ \u001b[33m0.024\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_abelian\u001b[0m  \u001b[2msymmray/abelian_core.py:2347\u001b[0m\n",
      "   │  │     │                    │  └─ \u001b[33m0.024\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_via_fused\u001b[0m  \u001b[2msymmray/abelian_core.py:2289\u001b[0m\n",
      "   │  │     │                    │     ├─ \u001b[33m0.022\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.drop_missing_blocks\u001b[0m  \u001b[2msymmray/abelian_core.py:1091\u001b[0m\n",
      "   │  │     │                    │     │  ├─ \u001b[33m0.019\u001b[0m [self]\u001b[0m  \u001b[2msymmray/abelian_core.py\u001b[0m\n",
      "   │  │     │                    │     │  └─ \u001b[32m0.003\u001b[0m _VariableFunctionsClass.all\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "   │  │     │                    │     └─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_blockwise\u001b[0m  \u001b[2msymmray/abelian_core.py:2137\u001b[0m\n",
      "   │  │     │                    │        ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2msymmray/abelian_core.py:2194\u001b[0m\n",
      "   │  │     │                    │        │  └─ \u001b[92m\u001b[2m0.001\u001b[0m numpy_like\u001b[0m  \u001b[2mautoray/autoray.py:1967\u001b[0m\n",
      "   │  │     │                    │        │        [2 frames hidden]  \u001b[2mtorch, <built-in>\u001b[0m\n",
      "   │  │     │                    │        └─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2msymmray/abelian_core.py\u001b[0m\n",
      "   │  │     │                    └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/fermionic_core.py:261\u001b[0m\n",
      "   │  │     │                       └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/abelian_core.py:1518\u001b[0m\n",
      "   │  │     │                          └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<dictcomp>\u001b[0m  \u001b[2msymmray/abelian_core.py:1530\u001b[0m\n",
      "   │  │     │                             └─ \u001b[92m\u001b[2m0.001\u001b[0m torch_transpose\u001b[0m  \u001b[2mautoray/autoray.py:1907\u001b[0m\n",
      "   │  │     │                                   [1 frames hidden]  \u001b[2m<built-in>\u001b[0m\n",
      "   │  │     ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.partition_tensors\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:5087\u001b[0m\n",
      "   │  │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.pop_tensor\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:4070\u001b[0m\n",
      "   │  │     │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.inds\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1492\u001b[0m\n",
      "   │  │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.add_tensor\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:3971\u001b[0m\n",
      "   │  │        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.tags\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1496\u001b[0m\n",
      "   │  ├─ \u001b[32m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.product_bra_state\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py:405\u001b[0m\n",
      "   │  │  ├─ \u001b[92m\u001b[2m0.002\u001b[0m [self]\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py\u001b[0m\n",
      "   │  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m do\u001b[0m  \u001b[2mautoray/autoray.py:30\u001b[0m\n",
      "   │  │  │     [1 frames hidden]  \u001b[2m<built-in>\u001b[0m\n",
      "   │  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.__init__\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1378\u001b[0m\n",
      "   │  │     └─ \u001b[92m\u001b[2m0.001\u001b[0m do\u001b[0m  \u001b[2mautoray/autoray.py:30\u001b[0m\n",
      "   │  │           [3 frames hidden]  \u001b[2mautoray\u001b[0m\n",
      "   │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.conj\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:4263\u001b[0m\n",
      "   │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.conj\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1912\u001b[0m\n",
      "   │        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.modify\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1549\u001b[0m\n",
      "   │           └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor._apply_function\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1546\u001b[0m\n",
      "   │              └─ \u001b[92m\u001b[2m0.001\u001b[0m conj\u001b[0m  \u001b[2mautoray/autoray.py:1049\u001b[0m\n",
      "   │                    [0 frames hidden]  \u001b[2m\u001b[0m\n",
      "   │                       \u001b[92m\u001b[2m0.001\u001b[0m do\u001b[0m  \u001b[2mautoray/autoray.py:30\u001b[0m\n",
      "   │                       └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mconj\u001b[0m  \u001b[2msymmray/interface.py:8\u001b[0m\n",
      "   │                          └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mFermionicArray.conj\u001b[0m  \u001b[2msymmray/fermionic_core.py:451\u001b[0m\n",
      "   │                             └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mFermionicArray.parity\u001b[0m  \u001b[2msymmray/fermionic_core.py:153\u001b[0m\n",
      "   │                                └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mFermionicArray.symmetry\u001b[0m  \u001b[2msymmray/abelian_core.py:963\u001b[0m\n",
      "   ├─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8438\u001b[0m\n",
      "   │  └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_structured\u001b[0m  \u001b[2mquimb/tensor/tensor_1d.py:510\u001b[0m\n",
      "   │     └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_cumulative\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8552\u001b[0m\n",
      "   │        └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_tags\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8328\u001b[0m\n",
      "   │           └─ \u001b[33m0.016\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "   │              └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensor_contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:207\u001b[0m\n",
      "   │                 └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15marray_contract\u001b[0m  \u001b[2mquimb/tensor/contraction.py:273\u001b[0m\n",
      "   │                    └─ \u001b[33m0.016\u001b[0m array_contract\u001b[0m  \u001b[2mcotengra/interface.py:735\u001b[0m\n",
      "   │                          [1 frames hidden]  \u001b[2mcotengra\u001b[0m\n",
      "   │                             \u001b[33m0.016\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "   │                             └─ \u001b[33m0.016\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_fermionic\u001b[0m  \u001b[2msymmray/fermionic_core.py:762\u001b[0m\n",
      "   │                                ├─ \u001b[33m0.013\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_abelian\u001b[0m  \u001b[2msymmray/abelian_core.py:2347\u001b[0m\n",
      "   │                                │  └─ \u001b[33m0.013\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_via_fused\u001b[0m  \u001b[2msymmray/abelian_core.py:2289\u001b[0m\n",
      "   │                                │     ├─ \u001b[32m0.009\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.drop_missing_blocks\u001b[0m  \u001b[2msymmray/abelian_core.py:1091\u001b[0m\n",
      "   │                                │     │  ├─ \u001b[32m0.008\u001b[0m [self]\u001b[0m  \u001b[2msymmray/abelian_core.py\u001b[0m\n",
      "   │                                │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m _VariableFunctionsClass.all\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "   │                                │     ├─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_blockwise\u001b[0m  \u001b[2msymmray/abelian_core.py:2137\u001b[0m\n",
      "   │                                │     │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2msymmray/abelian_core.py:2194\u001b[0m\n",
      "   │                                │     │  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m numpy_like\u001b[0m  \u001b[2mautoray/autoray.py:1967\u001b[0m\n",
      "   │                                │     │  │        [2 frames hidden]  \u001b[2mtorch, <built-in>\u001b[0m\n",
      "   │                                │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2msymmray/abelian_core.py\u001b[0m\n",
      "   │                                │     └─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.fuse\u001b[0m  \u001b[2msymmray/abelian_core.py:1775\u001b[0m\n",
      "   │                                │        └─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray._fuse_core\u001b[0m  \u001b[2msymmray/abelian_core.py:1663\u001b[0m\n",
      "   │                                │           └─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.cached_fuse_block_info\u001b[0m  \u001b[2msymmray/abelian_core.py:821\u001b[0m\n",
      "   │                                │              └─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mhasher\u001b[0m  \u001b[2msymmray/abelian_core.py:21\u001b[0m\n",
      "   │                                │                 └─ \u001b[92m\u001b[2m0.002\u001b[0m dumps\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "   │                                ├─ \u001b[92m\u001b[2m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.phase_sync\u001b[0m  \u001b[2msymmray/fermionic_core.py:422\u001b[0m\n",
      "   │                                └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/fermionic_core.py:261\u001b[0m\n",
      "   │                                   └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/abelian_core.py:1518\u001b[0m\n",
      "   └─ \u001b[32m0.007\u001b[0m grad\u001b[0m  \u001b[2mtorch/autograd/__init__.py:358\u001b[0m\n",
      "         [2 frames hidden]  \u001b[2mtorch, <built-in>\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import pyinstrument\n",
    "random_config = [H.hilbert.random_state(key=jax.random.PRNGKey(1)), H.hilbert.random_state(key=jax.random.PRNGKey(2))]\n",
    "random_config = torch.tensor(random_config, dtype=dtype)\n",
    "random_config_gpu = random_config.to(device)\n",
    "with pyinstrument.Profiler() as prof:\n",
    "    model.amplitude_grad(random_config_gpu)\n",
    "print(prof.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 22:44:01  Samples:  20\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 0.023     CPU time: 0.023\n",
      "/   _/                      v4.7.3\n",
      "\n",
      "Profile at /tmp/ipykernel_12453/3278166090.py:1\n",
      "\n",
      "\u001b[31m0.022\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<module>\u001b[0m  \u001b[2m../../../../../tmp/ipykernel_12453/3278166090.py:1\u001b[0m\n",
      "├─ \u001b[31m0.018\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPSModel.amplitude\u001b[0m  \u001b[2m../tn_model.py:566\u001b[0m\n",
      "│  ├─ \u001b[33m0.012\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.get_amp\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py:471\u001b[0m\n",
      "│  │  ├─ \u001b[33m0.009\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8438\u001b[0m\n",
      "│  │  │  └─ \u001b[33m0.009\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensorNetwork.contract_tags\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8328\u001b[0m\n",
      "│  │  │     └─ \u001b[33m0.009\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "│  │  │        └─ \u001b[33m0.009\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensor_contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:207\u001b[0m\n",
      "│  │  │           ├─ \u001b[33m0.008\u001b[0m \u001b[48;5;24m\u001b[38;5;15marray_contract\u001b[0m  \u001b[2mquimb/tensor/contraction.py:273\u001b[0m\n",
      "│  │  │           │  └─ \u001b[33m0.008\u001b[0m array_contract\u001b[0m  \u001b[2mcotengra/interface.py:735\u001b[0m\n",
      "│  │  │           │        [5 frames hidden]  \u001b[2mcotengra, autoray, importlib\u001b[0m\n",
      "│  │  │           │           \u001b[33m0.006\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "│  │  │           │           └─ \u001b[33m0.006\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_fermionic\u001b[0m  \u001b[2msymmray/fermionic_core.py:762\u001b[0m\n",
      "│  │  │           │              ├─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_abelian\u001b[0m  \u001b[2msymmray/abelian_core.py:2347\u001b[0m\n",
      "│  │  │           │              │  └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_via_fused\u001b[0m  \u001b[2msymmray/abelian_core.py:2289\u001b[0m\n",
      "│  │  │           │              │     ├─ \u001b[32m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.fuse\u001b[0m  \u001b[2msymmray/abelian_core.py:1775\u001b[0m\n",
      "│  │  │           │              │     │  └─ \u001b[32m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray._fuse_core\u001b[0m  \u001b[2msymmray/abelian_core.py:1663\u001b[0m\n",
      "│  │  │           │              │     │     ├─ \u001b[92m\u001b[2m0.001\u001b[0m _VariableFunctionsClass.reshape\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "│  │  │           │              │     │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.cached_fuse_block_info\u001b[0m  \u001b[2msymmray/abelian_core.py:821\u001b[0m\n",
      "│  │  │           │              │     ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mdrop_misaligned_sectors\u001b[0m  \u001b[2msymmray/abelian_core.py:2214\u001b[0m\n",
      "│  │  │           │              │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m dict.items\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "│  │  │           │              │     ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.unfuse\u001b[0m  \u001b[2msymmray/abelian_core.py:1828\u001b[0m\n",
      "│  │  │           │              │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.backend\u001b[0m  \u001b[2msymmray/block_core.py:60\u001b[0m\n",
      "│  │  │           │              │     │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.get_any_array\u001b[0m  \u001b[2msymmray/block_core.py:49\u001b[0m\n",
      "│  │  │           │              │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.drop_missing_blocks\u001b[0m  \u001b[2msymmray/abelian_core.py:1091\u001b[0m\n",
      "│  │  │           │              │        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.blocks\u001b[0m  \u001b[2msymmray/block_core.py:31\u001b[0m\n",
      "│  │  │           │              └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/fermionic_core.py:261\u001b[0m\n",
      "│  │  │           │                 └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.transpose\u001b[0m  \u001b[2msymmray/abelian_core.py:1518\u001b[0m\n",
      "│  │  │           │                    └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<dictcomp>\u001b[0m  \u001b[2msymmray/abelian_core.py:1530\u001b[0m\n",
      "│  │  │           │                       └─ \u001b[92m\u001b[2m0.001\u001b[0m torch_transpose\u001b[0m  \u001b[2mautoray/autoray.py:1907\u001b[0m\n",
      "│  │  │           │                             [1 frames hidden]  \u001b[2m<built-in>\u001b[0m\n",
      "│  │  │           └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.__init__\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1378\u001b[0m\n",
      "│  │  │              └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor._set_data\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1406\u001b[0m\n",
      "│  │  │                 └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15masarray\u001b[0m  \u001b[2mquimb/tensor/array_ops.py:21\u001b[0m\n",
      "│  │  │                    └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.shape\u001b[0m  \u001b[2msymmray/abelian_core.py:993\u001b[0m\n",
      "│  │  │                       └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2msymmray/abelian_core.py:996\u001b[0m\n",
      "│  │  ├─ \u001b[32m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.product_bra_state\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py:405\u001b[0m\n",
      "│  │  │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py\u001b[0m\n",
      "│  │  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.dtype\u001b[0m  \u001b[2msymmray/block_core.py:55\u001b[0m\n",
      "│  │  │     └─ \u001b[92m\u001b[2m0.001\u001b[0m get_dtype_name\u001b[0m  \u001b[2mautoray/autoray.py:1096\u001b[0m\n",
      "│  │  │           [2 frames hidden]  \u001b[2mautoray\u001b[0m\n",
      "│  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.__or__\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:3794\u001b[0m\n",
      "│  │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.combine\u001b[0m  \u001b[2mquimb/tensor/tensor_1d.py:400\u001b[0m\n",
      "│  │        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.combine\u001b[0m  \u001b[2mquimb/tensor/tensor_arbgeom.py:450\u001b[0m\n",
      "│  │           └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.combine\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:3761\u001b[0m\n",
      "│  ├─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8438\u001b[0m\n",
      "│  │  └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_structured\u001b[0m  \u001b[2mquimb/tensor/tensor_1d.py:510\u001b[0m\n",
      "│  │     └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_cumulative\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8552\u001b[0m\n",
      "│  │        └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mMatrixProductState.contract_tags\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:8328\u001b[0m\n",
      "│  │           └─ \u001b[33m0.005\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "│  │              └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensor_contract\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:207\u001b[0m\n",
      "│  │                 └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15marray_contract\u001b[0m  \u001b[2mquimb/tensor/contraction.py:273\u001b[0m\n",
      "│  │                    └─ \u001b[33m0.005\u001b[0m array_contract\u001b[0m  \u001b[2mcotengra/interface.py:735\u001b[0m\n",
      "│  │                          [1 frames hidden]  \u001b[2mcotengra\u001b[0m\n",
      "│  │                             \u001b[33m0.005\u001b[0m wrapper\u001b[0m  \u001b[2mfunctools.py:883\u001b[0m\n",
      "│  │                             └─ \u001b[33m0.005\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_fermionic\u001b[0m  \u001b[2msymmray/fermionic_core.py:762\u001b[0m\n",
      "│  │                                ├─ \u001b[32m0.004\u001b[0m \u001b[48;5;24m\u001b[38;5;15mtensordot_abelian\u001b[0m  \u001b[2msymmray/abelian_core.py:2347\u001b[0m\n",
      "│  │                                │  └─ \u001b[32m0.004\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_via_fused\u001b[0m  \u001b[2msymmray/abelian_core.py:2289\u001b[0m\n",
      "│  │                                │     ├─ \u001b[32m0.002\u001b[0m \u001b[48;5;24m\u001b[38;5;15m_tensordot_blockwise\u001b[0m  \u001b[2msymmray/abelian_core.py:2137\u001b[0m\n",
      "│  │                                │     │  ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15m<genexpr>\u001b[0m  \u001b[2msymmray/abelian_core.py:2194\u001b[0m\n",
      "│  │                                │     │  │  └─ \u001b[92m\u001b[2m0.001\u001b[0m numpy_like\u001b[0m  \u001b[2mautoray/autoray.py:1967\u001b[0m\n",
      "│  │                                │     │  │        [2 frames hidden]  \u001b[2mtorch, <built-in>\u001b[0m\n",
      "│  │                                │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m [self]\u001b[0m  \u001b[2msymmray/abelian_core.py\u001b[0m\n",
      "│  │                                │     ├─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.drop_missing_blocks\u001b[0m  \u001b[2msymmray/abelian_core.py:1091\u001b[0m\n",
      "│  │                                │     │  └─ \u001b[92m\u001b[2m0.001\u001b[0m _VariableFunctionsClass.all\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "│  │                                │     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.fuse\u001b[0m  \u001b[2msymmray/abelian_core.py:1775\u001b[0m\n",
      "│  │                                │        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray._fuse_core\u001b[0m  \u001b[2msymmray/abelian_core.py:1663\u001b[0m\n",
      "│  │                                │           └─ \u001b[92m\u001b[2m0.001\u001b[0m _VariableFunctionsClass.reshape\u001b[0m  \u001b[2m<built-in>\u001b[0m\n",
      "│  │                                └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.phase_flip\u001b[0m  \u001b[2msymmray/fermionic_core.py:311\u001b[0m\n",
      "│  │                                   └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mZ2FermionicArray.sectors\u001b[0m  \u001b[2msymmray/block_core.py:70\u001b[0m\n",
      "│  └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15munpack\u001b[0m  \u001b[2mquimb/tensor/interface.py:50\u001b[0m\n",
      "│     └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.copy\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:3884\u001b[0m\n",
      "│        └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.__init__\u001b[0m  \u001b[2mvmc_torch/fermion_utils.py:388\u001b[0m\n",
      "│           └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.phys_dim\u001b[0m  \u001b[2mquimb/tensor/tensor_arbgeom.py:891\u001b[0m\n",
      "│              └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mfMPS.ind_size\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:9115\u001b[0m\n",
      "│                 └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.ind_size\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1970\u001b[0m\n",
      "│                    └─ \u001b[92m\u001b[2m0.001\u001b[0m \u001b[48;5;24m\u001b[38;5;15mTensor.shape\u001b[0m  \u001b[2mquimb/tensor/tensor_core.py:1925\u001b[0m\n",
      "│                       └─ \u001b[92m\u001b[2m0.001\u001b[0m Composed.__call__\u001b[0m  \u001b[2mautoray/autoray.py:921\u001b[0m\n",
      "│                             [3 frames hidden]  \u001b[2mautoray\u001b[0m\n",
      "└─ \u001b[32m0.004\u001b[0m Tensor.backward\u001b[0m  \u001b[2mtorch/_tensor.py:525\u001b[0m\n",
      "      [3 frames hidden]  \u001b[2mtorch, <built-in>\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pyinstrument.Profiler() as prof:\n",
    "    for config in random_config:\n",
    "        if config.ndim == 1:\n",
    "            config = config.unsqueeze(0)\n",
    "        amp = model1.amplitude(config)\n",
    "        amp.backward()\n",
    "        grad = model1.params_grad_to_vec()\n",
    "print(prof.output_text(unicode=True, color=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
