{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "os.environ['DENSE_TENSOR'] = '0'\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import pickle\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/data'\n",
    "# torch\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "\n",
    "from vmc_torch.experiment.tn_model import *\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful, MetropolisMPSSamplerSpinful\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, SR,Adam, SGD_momentum, DecayScheduler, TrivialPreconditioner\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "from vmc_torch.utils import closest_divisible\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "Lx = int(4)\n",
    "Ly = int(4)\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx*Ly)\n",
    "# N_f = int(Lx*Ly)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 4\n",
    "chi = -1\n",
    "dtype=torch.float64\n",
    "torch.random.manual_seed(RANK)\n",
    "np.random.seed(RANK)\n",
    "\n",
    "# Load PEPS\n",
    "skeleton = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_skeleton.pkl\", \"rb\"))\n",
    "peps_params = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_su_params.pkl\", \"rb\"))\n",
    "peps = qtn.unpack(peps_params, skeleton)\n",
    "device = torch.device(\"cpu\")\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype, device=device))\n",
    "peps.exponent = torch.tensor(peps.exponent, dtype=dtype, device=device)\n",
    "\n",
    "# # randomize the PEPS tensors\n",
    "# peps.apply_to_arrays(lambda x: torch.randn_like(torch.tensor(x, dtype=dtype), dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = int(20)\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples/SIZE)%2 != 0:\n",
    "    N_samples += SIZE\n",
    "        \n",
    "# nn_hidden_dim = Lx*Ly\n",
    "# model = fTNModel_vec(peps, max_bond=chi, dtype=dtype, functional=True, device=device)\n",
    "# model1 = fTNModel(peps, max_bond=chi, dtype=dtype, functional=False)\n",
    "# model1.tree = model.tree\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Set up sampler\n",
    "sampler = MetropolisExchangeSamplerSpinful(H.hilbert, graph, N_samples=N_samples, burn_in_steps=2, reset_chain=False, random_edge=False, equal_partition=True, dtype=dtype, subchain_length=10)\n",
    "# mps_dir = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/data'+f'/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/tmp'\n",
    "# sampler = MetropolisMPSSamplerSpinful(H.hilbert, graph, mps_dir=mps_dir, mps_n_sample=1, N_samples=N_samples, burn_in_steps=20, reset_chain=True, equal_partition=True, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.fermion_utils import *\n",
    "from vmc_torch.experiment.tn_model import *\n",
    "\n",
    "class fTNModel_vec_test(wavefunctionModel):\n",
    "\n",
    "    def __init__(self, ftn, max_bond=None, dtype=torch.float32, functional=False, tree=None, device=None):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        self.functional = functional\n",
    "        self.device = device\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            f'fPEPS (chi={max_bond})':{'D': ftn.max_bond(), 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry},\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "\n",
    "        opt = ctg.HyperOptimizer(progbar=True, max_repeats=10, parallel=True)\n",
    "        # Get the amplitude\n",
    "        random_x = torch.randint(0, 3, (ftn.Lx*ftn.Ly,), dtype=torch.int)\n",
    "        amp = ftn.get_amp(random_x, conj=True, functional=self.functional)\n",
    "        self.tree = amp.contraction_tree(optimize=opt)\n",
    "\n",
    "        # Get self parity\n",
    "        self.parity_config = torch.tensor([array.parity for array in ftn.arrays], dtype=torch.int, device=self.device)\n",
    "\n",
    "        # BUG: in tree.traverse(), the tids are not automatically sorted, so we need to sort them manually\n",
    "        self.sorted_tree_traverse_path = {i: tuple(sorted(left_tids))+tuple(sorted(right_tids)) for i, (_, left_tids, right_tids) in enumerate(self.tree.traverse())}\n",
    "\n",
    "        # compute the permutation dict for future global phase computation\n",
    "        self.perm_dict = {i: tuple(torch.argsort(torch.tensor(left_right_tids))) for i, left_right_tids in self.sorted_tree_traverse_path.items()}\n",
    "        self.perm_dict_desc = {i: tuple(torch.argsort(torch.tensor(tuple(sorted(left_tids))[::-1]+tuple(sorted(right_tids))[::-1]), descending=True)) for i, (_, left_tids, right_tids) in enumerate(self.tree.traverse())}\n",
    "        \n",
    "        self.adjacent_transposition_dict_asc = {i: decompose_permutation_into_transpositions(perm, asc=False) for i, perm in self.perm_dict.items()}\n",
    "        self.adjacent_transposition_dict_desc = {i: decompose_permutation_into_transpositions(perm, asc=False) for i, perm in self.perm_dict_desc.items()}\n",
    "\n",
    "    \n",
    "    def compute_global_phase(self, input_config):\n",
    "        \"\"\"Get the global phase of contracting an amplitude of the fPEPS given computational graph.\"\"\"\n",
    "        on_site_parity_tensor = torch.tensor([0,1,1,0], dtype=torch.int, device=input_config.device)\n",
    "        def get_parity(n):\n",
    "            return on_site_parity_tensor[n]\n",
    "        # input_parity_config = input_config % 2\n",
    "        input_config_parity = get_parity(input_config)\n",
    "        amp_parity_config = (self.parity_config + input_config_parity) % 2\n",
    "\n",
    "        phase = 1\n",
    "        phase *= calculate_phase_from_adjacent_trans_dict(\n",
    "            self.tree, \n",
    "            input_config_parity, \n",
    "            self.parity_config, \n",
    "            amp_parity_config, \n",
    "            self.adjacent_transposition_dict_asc, \n",
    "            self.adjacent_transposition_dict_desc,\n",
    "            self.sorted_tree_traverse_path\n",
    "            )\n",
    "            \n",
    "        return phase\n",
    "\n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "\n",
    "        def amplitude_func(psi, x_i):\n",
    "            # Check x_i type\n",
    "            if not type(x_i) == torch.Tensor:\n",
    "                x_i = torch.tensor(x_i, dtype=torch.int if self.functional else self.param_dtype)\n",
    "            else:\n",
    "                if x_i.dtype != self.param_dtype:\n",
    "                    x_i = x_i.to(torch.int if self.functional else self.param_dtype)\n",
    "            # Get the amplitude\n",
    "            amp = psi.get_amp(x_i, conj=True, functional=self.functional)\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "                amp_val = amp.contract(optimize=self.tree)\n",
    "                phase = self.compute_global_phase(x_i.int())\n",
    "                amp_val = phase * amp_val\n",
    "\n",
    "            else:\n",
    "                amp = amp.contract_boundary_from_xmin(max_bond=self.max_bond, cutoff=0.0, xrange=[0, psi.Lx//2-1])\n",
    "                amp = amp.contract_boundary_from_xmax(max_bond=self.max_bond, cutoff=0.0, xrange=[psi.Lx//2, psi.Lx-1])\n",
    "                amp_val = amp.contract()\n",
    "\n",
    "            # if amp_val==0.0:\n",
    "            #     amp_val = torch.tensor(0.0)\n",
    "\n",
    "            # Return the batch of amplitudes stacked as a tensor\n",
    "            return amp_val\n",
    "        \n",
    "        # vec_amplitude_func = vmap(amplitude_func, in_dims=(None, 0), randomness='different')\n",
    "        # # Get the amplitude\n",
    "        # batch_amps = vec_amplitude_func(psi, x)\n",
    "\n",
    "        return amplitude_func(psi, x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.amplitude(x)\n",
    "    \n",
    "class fPEPS_vec(qtn.PEPS):\n",
    "    def __init__(self, arrays, *, shape=\"urdlp\", tags=None, site_ind_id=\"k{},{}\", site_tag_id=\"I{},{}\", x_tag_id=\"X{}\", y_tag_id=\"Y{}\", **tn_opts):\n",
    "        super().__init__(arrays, shape=shape, tags=tags, site_ind_id=site_ind_id, site_tag_id=site_tag_id, x_tag_id=x_tag_id, y_tag_id=y_tag_id, **tn_opts)\n",
    "        self.symmetry = self.arrays[0].symmetry\n",
    "        self.spinless = True if self.phys_dim() == 2 else False\n",
    "    \n",
    "    def product_bra_state(self, config, reverse=1):\n",
    "        product_tn = qtn.TensorNetwork()\n",
    "        backend = self.tensors[0].data.backend\n",
    "        dtype = eval(backend+'.'+self.tensors[0].data.dtype)\n",
    "        if type(config) == numpy.ndarray:\n",
    "            kwargs = {'like':config, 'dtype':dtype}\n",
    "        elif type(config) == torch.Tensor:\n",
    "            device = list(self.tensors[0].data.blocks.values())[0].device\n",
    "            kwargs = {'like':config, 'device':device, 'dtype':dtype}\n",
    "        if self.spinless:\n",
    "            index_map = {0: 0, 1: 1}\n",
    "            array_map = {\n",
    "                0: do('array',[1.0,],**kwargs), \n",
    "                1: do('array',[1.0,],**kwargs)\n",
    "            }\n",
    "        else:\n",
    "            if self.symmetry == 'Z2':\n",
    "                index_map = {0:0, 1:1, 2:1, 3:0}\n",
    "                array_map = {\n",
    "                    0: do('array',[1.0, 0.0],**kwargs), \n",
    "                    1: do('array',[1.0, 0.0],**kwargs), \n",
    "                    2: do('array',[0.0, 1.0],**kwargs), \n",
    "                    3: do('array',[0.0, 1.0],**kwargs)\n",
    "                }\n",
    "            elif self.symmetry == 'U1':\n",
    "                index_map = {0:0, 1:1, 2:1, 3:2}\n",
    "                array_map = {\n",
    "                    0: do('array',[1.0,],**kwargs), \n",
    "                    1: do('array',[1.0, 0.0],**kwargs), \n",
    "                    2: do('array',[0.0, 1.0],**kwargs), \n",
    "                    3: do('array',[1.0,],**kwargs)\n",
    "                }\n",
    "            elif self.symmetry == 'U1U1':\n",
    "                index_map = {0:(0,0), 1:(0,1), 2:(1,0), 3:(1,1)}\n",
    "                array_map = {\n",
    "                    0: do('array',[1.0],**kwargs),\n",
    "                    1: do('array',[1.0],**kwargs), \n",
    "                    2: do('array',[1.0],**kwargs),\n",
    "                    3: do('array',[1.0],**kwargs)\n",
    "                }\n",
    "\n",
    "        for n, site in zip(config, self.sites):\n",
    "            p_ind = self.site_ind_id.format(*site)\n",
    "            p_tag = self.site_tag_id.format(*site)\n",
    "            tid = self.sites.index(site)\n",
    "\n",
    "            n_charge = index_map[int(n)]\n",
    "            n_array = array_map[int(n)]\n",
    "\n",
    "            oddpos = None\n",
    "            if not self.spinless:\n",
    "                # assert self.symmetry == 'U1', \"Only U1 symmetry is supported for spinful fermions for now.\"\n",
    "                if int(n) == 1:\n",
    "                    oddpos = (3*tid+1)*(-1)**reverse\n",
    "                elif int(n) == 2:\n",
    "                    oddpos = (3*tid+2)*(-1)**reverse\n",
    "                elif int(n) == 3:\n",
    "                    # oddpos = ((3*tid+1)*(-1)**reverse, (3*tid+2)*(-1)**reverse)\n",
    "                    oddpos = None\n",
    "            else:\n",
    "                oddpos = (3*tid+1)*(-1)**reverse\n",
    "\n",
    "            tsr_data = sr.FermionicArray.from_blocks(\n",
    "                blocks={(n_charge,):n_array}, \n",
    "                duals=(True,),\n",
    "                symmetry=self.symmetry, \n",
    "                charge=n_charge, \n",
    "                oddpos=oddpos\n",
    "            )\n",
    "            tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, 'bra'))\n",
    "            product_tn |= tsr\n",
    "\n",
    "        return product_tn\n",
    "    \n",
    "    def product_bra_state_functional(self, config, reverse=1):\n",
    "        #XXX remember to comment out the drop_missing_blocks in tensordot_via_fused in line 2304-2305 in symmray abelian_core.py\n",
    "        product_tn = qtn.TensorNetwork()\n",
    "        backend = self.tensors[0].data.backend\n",
    "        dtype = eval(backend+'.'+self.tensors[0].data.dtype)\n",
    "        if type(config) == numpy.ndarray:\n",
    "            kwargs = {'like':config, 'dtype':dtype}\n",
    "        elif type(config) == torch.Tensor:\n",
    "            device = list(self.tensors[0].data.blocks.values())[0].device\n",
    "            kwargs = {'like':config, 'device':device, 'dtype':dtype}\n",
    "        if self.spinless:\n",
    "            raise NotImplementedError(\"Functional bra state is not implemented for spinless fermions.\")\n",
    "        else:\n",
    "            if self.symmetry == 'Z2':\n",
    "                charge_tensor = torch.tensor([0, 1, 1, 0], dtype=torch.int, device=device)\n",
    "                vector_tensor = do('array', [[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]], **kwargs)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Functional bra state is not implemented for spinful fermions.\")\n",
    "\n",
    "        for n, site in zip(config, self.sites):\n",
    "            p_ind = self.site_ind_id.format(*site)\n",
    "            p_tag = self.site_tag_id.format(*site)\n",
    "            tid = self.sites.index(site)\n",
    "            # n_charge = index_map[n.unsqueeze(0).int()].squeeze(0)\n",
    "            n_charge = 0\n",
    "            oddpos = None\n",
    "            if not self.spinless:\n",
    "                phase = 1 #...\n",
    "            else:\n",
    "                raise NotImplementedError(\"Functional bra state is not implemented for spinless fermions.\")\n",
    "            blocks={(int(charge_tensor[n.int()]),): vector_tensor[n.int()]}\n",
    "            tsr_data = sr.FermionicArray.from_blocks(\n",
    "                blocks=blocks, \n",
    "                duals=(True,),\n",
    "                symmetry=self.symmetry, \n",
    "                charge=n_charge, \n",
    "            )\n",
    "            tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, 'bra'))\n",
    "            product_tn |= tsr\n",
    "\n",
    "        return product_tn\n",
    "    \n",
    "    # NOTE: don't use @classmethod here, as we need to access the specific instance attributes\n",
    "    def get_amp(self, config, inplace=False, conj=True, reverse=1, contract=True, functional=False):\n",
    "        \"\"\"Get the amplitude of a configuration in a PEPS.\"\"\"\n",
    "        if functional:\n",
    "            return self.get_amp_functional(config, inplace=inplace)\n",
    "        \n",
    "        peps = self if inplace else self.copy()\n",
    "        product_state = self.product_bra_state(config, reverse=reverse).conj() if conj else self.product_bra_state(config, reverse=reverse)\n",
    "        \n",
    "        amp = peps|product_state # ---T---<---|n>\n",
    "\n",
    "        if not contract:\n",
    "            return amp\n",
    "        \n",
    "        for site in peps.sites:\n",
    "            site_tag = peps.site_tag_id.format(*site)\n",
    "            amp.contract_(tags=site_tag)\n",
    "\n",
    "        amp.view_as_(\n",
    "            qtn.PEPS,\n",
    "            site_ind_id=\"k{},{}\",\n",
    "            site_tag_id=\"I{},{}\",\n",
    "            x_tag_id=\"X{}\",\n",
    "            y_tag_id=\"Y{}\",\n",
    "            Lx=peps.Lx,\n",
    "            Ly=peps.Ly,\n",
    "        )\n",
    "        return amp\n",
    "    \n",
    "    \n",
    "    def get_amp_functional(self, config, inplace=False, conj=True, reverse=1, contract=True):\n",
    "        peps = self if inplace else self.copy()\n",
    "        product_state = self.product_bra_state_functional(config, reverse=reverse).conj() if conj else self.product_bra_state_functional(config, reverse=reverse)\n",
    "        \n",
    "        amp = peps|product_state # ---T---<---|n>\n",
    "\n",
    "        if not contract:\n",
    "            return amp\n",
    "        \n",
    "        for site in peps.sites:\n",
    "            site_tag = peps.site_tag_id.format(*site)\n",
    "            amp.contract_(tags=site_tag)\n",
    "\n",
    "        amp.view_as_(\n",
    "            qtn.PEPS,\n",
    "            site_ind_id=\"k{},{}\",\n",
    "            site_tag_id=\"I{},{}\",\n",
    "            x_tag_id=\"X{}\",\n",
    "            y_tag_id=\"Y{}\",\n",
    "            Lx=peps.Lx,\n",
    "            Ly=peps.Ly,\n",
    "        )\n",
    "        return amp\n",
    "    \n",
    "    \n",
    "    def get_amp_efficient(self, config, inplace=False):\n",
    "        \"\"\"Slicing to get the amplitude, faster than contraction with a tensor product state.\"\"\"\n",
    "        peps = self if inplace else self.copy()\n",
    "        backend = self.tensors[0].data.backend\n",
    "        dtype = eval(backend + '.' + self.tensors[0].data.dtype)\n",
    "        if type(config) == numpy.ndarray:\n",
    "            kwargs = {'like': config, 'dtype': dtype}\n",
    "        elif type(config) == torch.Tensor:\n",
    "            device = list(self.tensors[0].data.blocks.values())[0].device\n",
    "            kwargs = {'like': config, 'device': device, 'dtype': dtype}\n",
    "        \n",
    "        \n",
    "        if self.spinless:\n",
    "            raise NotImplementedError(\"Efficient amplitude calculation is not implemented for spinless fermions.\")\n",
    "        else:\n",
    "            if self.symmetry == 'Z2':\n",
    "                index_map = {0: 0, 1: 1, 2: 1, 3: 0}\n",
    "                array_map = {\n",
    "                    0: do('array', [1.0, 0.0], **kwargs),\n",
    "                    1: do('array', [1.0, 0.0], **kwargs),\n",
    "                    2: do('array', [0.0, 1.0], **kwargs),\n",
    "                    3: do('array', [0.0, 1.0], **kwargs)\n",
    "                }\n",
    "            elif self.symmetry == 'U1':\n",
    "                index_map = {0: 0, 1: 1, 2: 1, 3: 2}\n",
    "                array_map = {\n",
    "                    0: do('array', [1.0], **kwargs),\n",
    "                    1: do('array', [1.0, 0.0], **kwargs),\n",
    "                    2: do('array', [0.0, 1.0], **kwargs),\n",
    "                    3: do('array', [1.0], **kwargs)\n",
    "                }\n",
    "            elif self.symmetry == 'U1U1':\n",
    "                index_map = {0:(0,0), 1:(0,1), 2:(1,0), 3:(1,1)}\n",
    "                array_map = {\n",
    "                    0: do('array',[1.0],**kwargs),\n",
    "                    1: do('array',[1.0],**kwargs), \n",
    "                    2: do('array',[1.0],**kwargs),\n",
    "                    3: do('array',[1.0],**kwargs)\n",
    "                }\n",
    "            \n",
    "\n",
    "            for n, site in zip(config, self.sites):\n",
    "                p_ind = peps.site_ind_id.format(*site)\n",
    "                site_id = peps.sites.index(site)\n",
    "                fts = peps.tensors[site_id]\n",
    "                ftsdata = fts.data\n",
    "                ftsdata.phase_sync(inplace=True) # explicitly apply all lazy phases that are stored and not yet applied\n",
    "                phys_ind_order = fts.inds.index(p_ind)\n",
    "                charge = index_map[int(n)]\n",
    "                input_vec = array_map[int(n)]\n",
    "                charge_sec_data_dict = ftsdata.blocks\n",
    "\n",
    "                new_fts_inds = fts.inds[:phys_ind_order] + fts.inds[phys_ind_order + 1:]\n",
    "                new_charge_sec_data_dict = {}\n",
    "                for charge_blk, data in charge_sec_data_dict.items():\n",
    "                    if charge_blk[phys_ind_order] == charge:\n",
    "                        # new_data = data @ input_vec #BUG: This is not correct, should contract with the correct tensor index\n",
    "                        new_data = do('tensordot', data, input_vec, axes=([phys_ind_order], [0]))\n",
    "                        new_charge_blk = charge_blk[:phys_ind_order] + charge_blk[phys_ind_order + 1:]\n",
    "                        new_charge_sec_data_dict[new_charge_blk] = new_data\n",
    "\n",
    "                new_duals = ftsdata.duals[:phys_ind_order] + ftsdata.duals[phys_ind_order + 1:]\n",
    "\n",
    "                if int(n) == 1:\n",
    "                    new_oddpos = (3 * site_id + 1) * (-1)\n",
    "                elif int(n) == 2:\n",
    "                    new_oddpos = (3 * site_id + 2) * (-1)\n",
    "                elif int(n) == 3 or int(n) == 0:\n",
    "                    new_oddpos = ()\n",
    "\n",
    "                new_oddpos1 = FermionicOperator(new_oddpos, dual=True) if new_oddpos != () else ()\n",
    "                new_oddpos = ftsdata.oddpos + (new_oddpos1,) if new_oddpos1 is not () else ftsdata.oddpos\n",
    "                oddpos = list(new_oddpos)[::-1]\n",
    "                try:\n",
    "                    if ftsdata.symmetry == 'U1':\n",
    "                        new_charge = charge + ftsdata.charge\n",
    "                    elif ftsdata.symmetry == 'Z2':\n",
    "                        new_charge = (charge + ftsdata.charge) % 2 # Z2 symmetry, charge should be 0 or 1\n",
    "                    elif ftsdata.symmetry == 'U1U1':\n",
    "                        new_charge = (charge[0] + ftsdata.charge[0], charge[1] + ftsdata.charge[1]) # U1U1 symmetry, charge should be a tuple of two integers\n",
    "                    new_fts_data = sr.FermionicArray.from_blocks(new_charge_sec_data_dict, duals=new_duals, charge=new_charge, oddpos=oddpos, symmetry=ftsdata.symmetry)\n",
    "                except:\n",
    "                    print(n, site, phys_ind_order, charge_sec_data_dict, new_charge_sec_data_dict)\n",
    "                    \n",
    "                fts.modify(data=new_fts_data, inds=new_fts_inds, left_inds=None)\n",
    "\n",
    "            amp = qtn.PEPS(peps)\n",
    "\n",
    "            return amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The only possible solution I can think of is to make the charge tuples also tensors. Then everything can be done at tensor level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F=4.76 C=5.57 S=10.00 P=11.43: 100%|██████████| 10/10 [00:00<00:00, 622.98it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m model = fTNModel_vec_test(vec_peps, max_bond=chi, dtype=dtype, functional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# model1 = fTNModel(vec_peps, max_bond=chi, dtype=dtype, functional=False)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# func_amp = vec_peps.get_amp(X[0], functional=True)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# func_amp.contract()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/torch/_functorch/apis.py:203\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/torch/_functorch/vmap.py:331\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    321\u001b[39m         func,\n\u001b[32m    322\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    327\u001b[39m         **kwargs,\n\u001b[32m    328\u001b[39m     )\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/torch/_functorch/vmap.py:479\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    476\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    477\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    478\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mfTNModel_vec_test.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mamplitude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mfTNModel_vec_test.amplitude\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m amp_val\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# vec_amplitude_func = vmap(amplitude_func, in_dims=(None, 0), randomness='different')\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# # Get the amplitude\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# batch_amps = vec_amplitude_func(psi, x)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mamplitude_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mfTNModel_vec_test.amplitude.<locals>.amplitude_func\u001b[39m\u001b[34m(psi, x_i)\u001b[39m\n\u001b[32m     97\u001b[39m         x_i = x_i.to(torch.int \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.functional \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_dtype)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Get the amplitude\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m amp = \u001b[43mpsi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_amp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.max_bond \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    101\u001b[39m     amp = amp\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 252\u001b[39m, in \u001b[36mfPEPS_vec.get_amp\u001b[39m\u001b[34m(self, config, inplace, conj, reverse, contract, functional)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the amplitude of a configuration in a PEPS.\"\"\"\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m functional:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_amp_functional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m peps = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    255\u001b[39m product_state = \u001b[38;5;28mself\u001b[39m.product_bra_state(config, reverse=reverse).conj() \u001b[38;5;28;01mif\u001b[39;00m conj \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.product_bra_state(config, reverse=reverse)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 280\u001b[39m, in \u001b[36mfPEPS_vec.get_amp_functional\u001b[39m\u001b[34m(self, config, inplace, conj, reverse, contract)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_amp_functional\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, inplace=\u001b[38;5;28;01mFalse\u001b[39;00m, conj=\u001b[38;5;28;01mTrue\u001b[39;00m, reverse=\u001b[32m1\u001b[39m, contract=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    279\u001b[39m     peps = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     product_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproduct_bra_state_functional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m)\u001b[49m.conj() \u001b[38;5;28;01mif\u001b[39;00m conj \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.product_bra_state_functional(config, reverse=reverse)\n\u001b[32m    282\u001b[39m     amp = peps|product_state \u001b[38;5;66;03m# ---T---<---|n>\u001b[39;00m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contract:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 236\u001b[39m, in \u001b[36mfPEPS_vec.product_bra_state_functional\u001b[39m\u001b[34m(self, config, reverse)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFunctional bra state is not implemented for spinless fermions.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m blocks={(\u001b[38;5;28mint\u001b[39m(\u001b[43mcharge_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m),): vector_tensor[n.int()]}\n\u001b[32m    237\u001b[39m tsr_data = sr.FermionicArray.from_blocks(\n\u001b[32m    238\u001b[39m     blocks=blocks, \n\u001b[32m    239\u001b[39m     duals=(\u001b[38;5;28;01mTrue\u001b[39;00m,),\n\u001b[32m    240\u001b[39m     symmetry=\u001b[38;5;28mself\u001b[39m.symmetry, \n\u001b[32m    241\u001b[39m     charge=n_charge, \n\u001b[32m    242\u001b[39m )\n\u001b[32m    243\u001b[39m tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, \u001b[33m'\u001b[39m\u001b[33mbra\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mRuntimeError\u001b[39m: vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report."
     ]
    }
   ],
   "source": [
    "from torch.func import vmap\n",
    "\n",
    "\n",
    "X = [H.hilbert.random_state(i) for i in range(10)]\n",
    "X = torch.tensor(X, dtype=dtype, device=device)\n",
    "vec_peps = fPEPS_vec(peps, Lx=Lx, Ly=Ly, symmetry=symmetry)\n",
    "model = fTNModel_vec_test(vec_peps, max_bond=chi, dtype=dtype, functional=True)\n",
    "# model1 = fTNModel(vec_peps, max_bond=chi, dtype=dtype, functional=False)\n",
    "vmap(model)(X)\n",
    "# func_amp = vec_peps.get_amp(X[0], functional=True)\n",
    "# func_amp.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(0, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n",
      "(tensor(1, dtype=torch.int32),)\n"
     ]
    }
   ],
   "source": [
    "charge_tensor = torch.tensor([0, 1, 1, 0], dtype=torch.int, device=device)\n",
    "vector_tensor = torch.tensor([[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0]], dtype=dtype, device=device)\n",
    "for n in X[0]:\n",
    "    print((charge_tensor[n.int()],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = {(0,): torch.tensor([1.0], dtype=dtype, device=device)}\n",
    "d1 = {(1,): torch.tensor([2.0], dtype=dtype, device=device)}\n",
    "\n",
    "data = {(0,): torch.tensor([1.0], dtype=dtype, device=device), (1,): torch.tensor([-1.0], dtype=dtype, device=device)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
