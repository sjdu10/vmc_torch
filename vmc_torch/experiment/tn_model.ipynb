{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n=50, tau=0.3000, energy~-0.266968: 100%|##########| 50/50 [00:06<00:00,  7.35it/s]\n",
      "n=100, tau=0.1000, energy~-0.342996: 100%|##########| 50/50 [00:06<00:00,  7.67it/s]\n",
      "n=150, tau=0.0300, energy~-0.345458: 100%|##########| 50/50 [00:06<00:00,  7.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NUMBA_NUM_THREADS\"] = \"20\"\n",
    "\n",
    "import netket as nk\n",
    "import netket.experimental as nkx\n",
    "import netket.nn as nknn\n",
    "\n",
    "from math import pi\n",
    "\n",
    "from netket.experimental.operator.fermion import destroy as c\n",
    "from netket.experimental.operator.fermion import create as cdag\n",
    "from netket.experimental.operator.fermion import number as nc\n",
    "\n",
    "from vmc_torch.fermion_utils import generate_random_fpeps\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import pickle\n",
    "\n",
    "# Define the lattice shape\n",
    "L = 4  # Side of the square\n",
    "Lx = int(L)\n",
    "Ly = int(L)\n",
    "spinless = False\n",
    "# graph = nk.graph.Square(L)\n",
    "graph = nk.graph.Grid([Lx,Ly], pbc=False)\n",
    "N = graph.n_nodes\n",
    "\n",
    "# Define the fermion filling and the Hilbert space\n",
    "N_f = int(Lx*Ly)\n",
    "hi = nkx.hilbert.SpinOrbitalFermions(N, s=1/2, n_fermions=N_f)\n",
    "\n",
    "\n",
    "# Define the Hubbard Hamiltonian\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "mu = 0.0\n",
    "\n",
    "H = 0.0\n",
    "for (i, j) in graph.edges(): # Definition of the Hubbard Hamiltonian\n",
    "    for spin in (1,-1):\n",
    "        H -= t * (cdag(hi,i,spin) * c(hi,j,spin) + cdag(hi,j,spin) * c(hi,i,spin))\n",
    "for i in graph.nodes():\n",
    "    H += U * nc(hi,i,+1) * nc(hi,i,-1)\n",
    "\n",
    "\n",
    "# SU in quimb\n",
    "D = 3\n",
    "seed = 2\n",
    "symmetry = 'U1'\n",
    "spinless = False\n",
    "peps = generate_random_fpeps(Lx, Ly, D=D, seed=2, symmetry=symmetry, Nf=N_f, spinless=spinless)[0]\n",
    "edges = qtn.edges_2d_square(Lx, Ly, cyclic=False)\n",
    "site_info = sr.utils.parse_edges_to_site_info(\n",
    "    edges,\n",
    "    D,\n",
    "    phys_dim=4,\n",
    "    site_ind_id=\"k{},{}\",\n",
    "    site_tag_id=\"I{},{}\",\n",
    ")\n",
    "\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "mu = 0.0\n",
    "\n",
    "terms = {\n",
    "    (sitea, siteb): sr.fermi_hubbard_local_array(\n",
    "        t=t, U=U, mu=mu,\n",
    "        symmetry=symmetry,\n",
    "        coordinations=(\n",
    "            site_info[sitea]['coordination'],\n",
    "            site_info[siteb]['coordination'],\n",
    "        ),\n",
    "    ).fuse((0, 1), (2, 3))\n",
    "    for (sitea, siteb) in peps.gen_bond_coos()\n",
    "}\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "\n",
    "su = qtn.SimpleUpdateGen(peps, ham, compute_energy_per_site=True,D=D, compute_energy_opts={\"max_distance\":1}, gate_opts={'cutoff':1e-12})\n",
    "\n",
    "# cluster energies may not be accuracte yet\n",
    "su.evolve(50, tau=0.3)\n",
    "su.evolve(50, tau=0.1)\n",
    "su.evolve(50, tau=0.03)\n",
    "# su.evolve(100, tau=0.01)\n",
    "# su.evolve(100, tau=0.003)\n",
    "\n",
    "peps = su.get_state()\n",
    "peps.equalize_norms_(value=1)\n",
    "\n",
    "# save the state\n",
    "params, skeleton = qtn.pack(peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import ast\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "from quimb.tensor.tensor_2d import Rotator2D, pairwise\n",
    "import symmray as sr\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.fermion_utils import insert_proj_peps, flatten_proj_params, reconstruct_proj_params, insert_compressor\n",
    "from vmc_torch.global_var import DEBUG, set_debug\n",
    "\n",
    "# Define the custom zero-initialization function\n",
    "def init_weights_to_zero(m, std=1e-3):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d, nn.Embedding, nn.LayerNorm)):\n",
    "        # Set weights and biases to zero\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=std)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "class fTNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ftn, max_bond=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS (exact contraction)':{'D': ftn.max_bond(), 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry},\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        \n",
    "    \n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            if param is not None:\n",
    "                param.grad = None\n",
    "    \n",
    "    def from_vec_to_params(self, vec, quimb_format=False):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        # XXX: useful at all?\n",
    "        params = {}\n",
    "        idx = 0\n",
    "        for tid, blk_array in self.torch_tn_params.items():\n",
    "            params[tid] = {}\n",
    "            for sector, data in blk_array.items():\n",
    "                shape = data.shape\n",
    "                size = data.numel()\n",
    "                if quimb_format:\n",
    "                    params[tid][ast.literal_eval(sector)] = vec[idx:idx+size].view(shape)\n",
    "                else:\n",
    "                    params[tid][sector] = vec[idx:idx+size].view(shape)\n",
    "                idx += size\n",
    "        return params\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        pointer = 0\n",
    "        for param, shape in zip(self.parameters(), self.param_shapes):\n",
    "            num_param = param.numel()\n",
    "            new_param_values = new_params[pointer:pointer+num_param].view(shape)\n",
    "            with torch.no_grad():\n",
    "                param.copy_(new_param_values)\n",
    "            pointer += num_param\n",
    "\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "            else:\n",
    "                amp = amp.contract_boundary_from_ymin(max_bond=self.max_bond, cutoff=0.0, yrange=[0, psi.Ly-2])\n",
    "            amp_val = amp.contract()\n",
    "            if amp_val==0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)\n",
    "\n",
    "class fTN_backflow_Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ftn, max_bond=None, nn_hidden_dim=128, nn_eta=1e-3, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        \n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Define the neural network\n",
    "        input_dim = ftn.Lx * ftn.Ly\n",
    "        tn_params_vec = flatten_proj_params(params)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_dim, nn_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nn_hidden_dim, tn_params_vec.numel())\n",
    "        )\n",
    "        self.nn.to(self.param_dtype)\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS (exact contraction)':{'D': ftn.max_bond(), 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry},\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        self.nn_eta = nn_eta\n",
    "        \n",
    "        \n",
    "    \n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            if param is not None:\n",
    "                param.grad = None\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        pointer = 0\n",
    "        for param, shape in zip(self.parameters(), self.param_shapes):\n",
    "            num_param = param.numel()\n",
    "            new_param_values = new_params[pointer:pointer+num_param].view(shape)\n",
    "            with torch.no_grad():\n",
    "                param.copy_(new_param_values)\n",
    "            pointer += num_param\n",
    "\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        params_vec = flatten_proj_params(params)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Get the NN correction to the parameters\n",
    "            nn_correction = self.nn(x_i.to(self.param_dtype))\n",
    "            # Add the correction to the original parameters\n",
    "            tn_nn_params = reconstruct_proj_params(params_vec + self.nn_eta*nn_correction, params)\n",
    "            # Reconstruct the TN with the new parameters\n",
    "            psi = qtn.unpack(tn_nn_params, self.skeleton)\n",
    "            # Get the amplitude\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "            else:\n",
    "                amp = amp.contract_boundary_from_ymin(max_bond=self.max_bond, cutoff=0.0, yrange=[0, psi.Ly-2])\n",
    "\n",
    "            amp_val = amp.contract()\n",
    "            if amp_val==0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18598, 531)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps.apply_to_arrays(torch.tensor)\n",
    "model = fTN_backflow_Model(peps, max_bond=4, nn_hidden_dim=32, nn_eta=1.0, dtype=torch.float32)\n",
    "model1 = fTNModel(peps, max_bond=4)\n",
    "model.apply(lambda x: init_weights_to_zero(x, std=1e-2))\n",
    "model.num_params, model1.num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7.4627e-09], dtype=torch.float64, grad_fn=<StackBackward0>),\n",
       " tensor([1.0486e-08], dtype=torch.float64, grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vmc_torch.hamiltonian import spinful_Fermi_Hubbard_square_lattice\n",
    "# Hamiltonian parameters\n",
    "Lx = int(4)\n",
    "Ly = int(4)\n",
    "symmetry = 'U1'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx*Ly)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "import jax\n",
    "import numpy as np\n",
    "random_seed = np.random.randint(2**32)\n",
    "random_config = torch.tensor(H.hilbert.random_state(key=jax.random.PRNGKey(random_seed)), dtype=torch.int)\n",
    "model(random_config), model1(random_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
