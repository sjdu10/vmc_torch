{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f518285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "import sys\n",
    "pwd = '/pscratch/sd/s/sijingdu/VMC/fermion/data'\n",
    "# torch\n",
    "import torch\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.experiment.tn_model import *\n",
    "from vmc_torch.experiment.tn_model import init_weights_uniform\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful_1D_reusable\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, SR, Adam, SGD_momentum, DecayScheduler\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_chain_torch\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "from vmc_torch.utils import closest_divisible\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "L = int(24)\n",
    "D = int(4)\n",
    "N_samples = int(4)\n",
    "init_lr = float(1)\n",
    "init_step = int(0)\n",
    "final_step = int(2)\n",
    "\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(20)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_chain_torch(L, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "\n",
    "# TN parameters\n",
    "chi = -1\n",
    "dtype = torch.float64\n",
    "\n",
    "# Load mps\n",
    "skeleton = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_skeleton.pkl\", \"rb\"))\n",
    "mps_params = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_su_params.pkl\", \"rb\"))\n",
    "mps = qtn.unpack(mps_params, skeleton)\n",
    "mps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples / SIZE) % 2 != 0:\n",
    "    N_samples += SIZE\n",
    "\n",
    "# model = fMPSModel(mps, dtype=dtype)\n",
    "# model = fMPS_backflow_attn_Model(mps, nn_eta=1.0, nn_hidden_dim=2*L, embedding_dim=16, attention_heads=4, dtype=dtype)\n",
    "# model = fMPS_backflow_attn_Tensorwise_Model_v1(mps, nn_eta=1.0, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), dtype=dtype)\n",
    "radius = 1\n",
    "# model = fMPS_BFA_cluster_Model(mps, nn_eta=1.0, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), radius=radius, dtype=dtype)\n",
    "model = fMPS_BFA_cluster_Model_reuse(mps, nn_eta=1.0, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), radius=radius, dtype=dtype)\n",
    "init_std = 5e-3\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "model.apply(lambda x: init_weights_uniform(x, a=-init_std, b=init_std))\n",
    "model_names = {\n",
    "    fMPSModel: 'fMPS',\n",
    "    fMPS_backflow_Model: 'fMPS_backflow',\n",
    "    fMPS_backflow_attn_Model: 'fMPS_backflow_attn',\n",
    "    fMPS_backflow_attn_Tensorwise_Model_v1: 'fMPS_backflow_attn_Tensorwise_v1',\n",
    "    fMPS_BFA_cluster_Model: f'fMPS_BFA_cluster_r={radius}',\n",
    "    fMPS_BFA_cluster_Model_reuse: f'fMPS_BFA_cluster_r={radius}_reuse',\n",
    "}\n",
    "model_name = model_names.get(type(model), 'UnknownModel')+f'_test'\n",
    "\n",
    "\n",
    "total_steps = final_step - init_step\n",
    "\n",
    "# Load model parameters\n",
    "optimizer_state = None\n",
    "if init_step != 0:\n",
    "    saved_model_params = torch.load(pwd+f'/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{model_name}/chi={chi}/model_params_step{init_step}.pth', weights_only=False)\n",
    "    saved_model_state_dict = saved_model_params['model_state_dict']\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params['model_params_vec'])\n",
    "    try:\n",
    "        model.load_state_dict(saved_model_state_dict)\n",
    "    except:\n",
    "        model.load_params(saved_model_params_vec)\n",
    "    optimizer_state = saved_model_params.get('optimizer_state', None)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "learning_rate = init_lr\n",
    "scheduler = DecayScheduler(init_lr=learning_rate, decay_rate=0.9, patience=50, min_lr=1e-3)\n",
    "use_prev_opt = False\n",
    "if optimizer_state is not None and use_prev_opt:\n",
    "    optimizer_name = optimizer_state['optimizer']\n",
    "    if optimizer_name == 'SGD_momentum':\n",
    "        optimizer = SGD_momentum(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate, weight_decay=1e-5)\n",
    "    print('Loading optimizer: ', optimizer)\n",
    "    optimizer.lr = learning_rate\n",
    "    if isinstance(optimizer, SGD_momentum):\n",
    "        optimizer.velocity = optimizer_state['velocity']\n",
    "    if isinstance(optimizer, Adam):\n",
    "        optimizer.m = optimizer_state['m']\n",
    "        optimizer.v = optimizer_state['v']\n",
    "        optimizer.t = optimizer_state['t']\n",
    "else:\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "# Set up sampler\n",
    "# sampler = MetropolisExchangeSamplerSpinful(H.hilbert, graph, N_samples=N_samples, burn_in_steps=40, reset_chain=False, random_edge=False, equal_partition=False, dtype=dtype)\n",
    "sampler = MetropolisExchangeSamplerSpinful_1D_reusable(H.hilbert, graph, N_samples=N_samples, burn_in_steps=1, reset_chain=False, random_edge=False, equal_partition=True, dtype=dtype)\n",
    "# Set up variational state\n",
    "variational_state = Variational_State(model, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "# Set up SR preconditioner\n",
    "preconditioner = SR(dense=False, exact=True if sampler is None else False, use_MPI4Solver=True, solver='minres', diag_eta=1e-3, iter_step=1e3, dtype=dtype, rtol=1e-5)\n",
    "# Set up VMC\n",
    "vmc = VMC(hamiltonian=H, variational_state=variational_state, optimizer=optimizer, preconditioner=preconditioner, scheduler=scheduler, SWO=False, beta=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc063502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.get_local_amp_tensors([0,1,2])\n",
    "random_config = torch.tensor(H.hilbert.random_state())\n",
    "# model.get_local_amp_tensors([0,1,2], random_config)\n",
    "# random_config[0,1,2, 5]\n",
    "random_config[[0,1,2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
