{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e93bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "\n",
    "from vmc_torch.experiment.tn_model import fMPSModel, fMPS_backflow_Model, fMPS_backflow_attn_Tensorwise_Model_v1, fMPS_BFA_cluster_Model, fMPS_BFA_cluster_Model_reuse\n",
    "from vmc_torch.experiment.tn_model import init_weights_to_zero\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful, MetropolisExchangeSamplerSpinful_1D_reusable\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, SR, Adam, SGD_momentum, DecayScheduler\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_chain_torch\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "from vmc_torch.fermion_utils import generate_random_fmps, form_gated_fmps_tnf\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "from vmc_torch.utils import closest_divisible\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/data'\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "L = int(100)\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(L)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_chain_torch(L, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 10\n",
    "chi = -1\n",
    "dtype=torch.float64\n",
    "\n",
    "# Load mps\n",
    "skeleton = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_skeleton.pkl\", \"rb\"))\n",
    "mps_params = pickle.load(open(pwd+f\"/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/mps_su_params.pkl\", \"rb\"))\n",
    "mps = qtn.unpack(mps_params, skeleton)\n",
    "# fmps_tnf = form_gated_fmps_tnf(fmps=mps, ham=quimb_ham, depth=2)\n",
    "mps.apply_to_arrays(lambda x: torch.tensor(2*x, dtype=dtype))\n",
    "\n",
    "# # randomize the mps tensors\n",
    "# mps.apply_to_arrays(lambda x: torch.randn_like(torch.tensor(x, dtype=dtype), dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = int(4)\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples/SIZE)%2 != 0:\n",
    "    N_samples += SIZE\n",
    "\n",
    "# model = fMPSModel(mps, dtype=dtype)\n",
    "# model = fMPS_backflow_attn_Tensorwise_Model_v1(mps, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), nn_eta=1.0, dtype=dtype)\n",
    "model0 = fMPS_BFA_cluster_Model(mps, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), nn_eta=1.0, radius=1, dtype=dtype)\n",
    "model = fMPS_BFA_cluster_Model_reuse(mps, embedding_dim=16, attention_heads=4, nn_final_dim=int(L/2), nn_eta=1.0, radius=1, dtype=dtype)\n",
    "# model = fMPS_backflow_Model(mps, nn_eta=1.0, num_hidden_layer=2, nn_hidden_dim=2*L, dtype=dtype)\n",
    "init_std = 5e-2\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "model.apply(lambda x: init_weights_to_zero(x, std=init_std))\n",
    "model0.apply(lambda x: init_weights_to_zero(x, std=init_std))\n",
    "# model.apply(lambda x: init_weights_kaiming(x))\n",
    "\n",
    "model_names = {\n",
    "    fMPSModel: 'fMPS',\n",
    "    fMPS_backflow_Model: 'fMPS_backflow',\n",
    "    fMPS_backflow_attn_Tensorwise_Model_v1: 'fMPS_backflow_attn_Tensorwise_v1',\n",
    "    fMPS_BFA_cluster_Model: 'fMPS_BFA_cluster',\n",
    "    fMPS_BFA_cluster_Model_reuse: 'fMPS_BFA_cluster_reuse',\n",
    "}\n",
    "model_name = model_names.get(type(model), 'UnknownModel')\n",
    "\n",
    "\n",
    "init_step = 0\n",
    "final_step = 250\n",
    "total_steps = final_step - init_step\n",
    "# Load model parameters\n",
    "if init_step != 0:\n",
    "    saved_model_params = torch.load(pwd+f'/L={L}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/{model_name}/chi={chi}/model_params_step{init_step}.pth')\n",
    "    saved_model_state_dict = saved_model_params['model_state_dict']\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params['model_params_vec'])\n",
    "    try:\n",
    "        model.load_state_dict(saved_model_state_dict)\n",
    "    except:\n",
    "        model.load_params(saved_model_params_vec)\n",
    "    optimizer_state = saved_model_params.get('optimizer_state', None)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "learning_rate = 1e-1\n",
    "scheduler = DecayScheduler(init_lr=learning_rate, decay_rate=0.9, patience=50, min_lr=1e-2)\n",
    "optimizer_state = None\n",
    "use_prev_opt = True\n",
    "if optimizer_state is not None and use_prev_opt:\n",
    "    optimizer_name = optimizer_state['optimizer']\n",
    "    if optimizer_name == 'SGD_momentum':\n",
    "        optimizer = SGD_momentum(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate, weight_decay=1e-5)\n",
    "    print('Loading optimizer: ', optimizer)\n",
    "    optimizer.lr = learning_rate\n",
    "    if isinstance(optimizer, SGD_momentum):\n",
    "        optimizer.velocity = optimizer_state['velocity']\n",
    "    if isinstance(optimizer, Adam):\n",
    "        optimizer.m = optimizer_state['m']\n",
    "        optimizer.v = optimizer_state['v']\n",
    "        optimizer.t = optimizer_state['t']\n",
    "else:\n",
    "    # optimizer = SignedSGD(learning_rate=learning_rate)\n",
    "    # optimizer = SignedRandomSGD(learning_rate=learning_rate)\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    # optimizer = SGD_momentum(learning_rate=learning_rate, momentum=0.9)\n",
    "    # optimizer = Adam(learning_rate=learning_rate, t_step=init_step, weight_decay=1e-5)\n",
    "\n",
    "# Set up sampler\n",
    "sampler0 = MetropolisExchangeSamplerSpinful(H.hilbert, graph, N_samples=N_samples, burn_in_steps=10, reset_chain=False, random_edge=False, equal_partition=True, dtype=dtype)\n",
    "sampler = MetropolisExchangeSamplerSpinful_1D_reusable(H.hilbert, graph, N_samples=N_samples, burn_in_steps=10, reset_chain=False, random_edge=False, equal_partition=True, dtype=dtype)\n",
    "# Set up variational state\n",
    "variational_state0 = Variational_State(model0, hi=H.hilbert, sampler=sampler0, dtype=dtype)\n",
    "variational_state = Variational_State(model, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "# Set up SR preconditioner\n",
    "preconditioner = SR(dense=False, exact=True if sampler is None else False, use_MPI4Solver=True, diag_eta=1e-3, iter_step=1e5, dtype=dtype)\n",
    "# preconditioner = TrivialPreconditioner()\n",
    "# Set up VMC\n",
    "vmc = VMC(hamiltonian=H, variational_state=variational_state, optimizer=optimizer, preconditioner=preconditioner, scheduler=scheduler, SWO=False, beta=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640632f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, amp_val = sampler._sample_next(variational_state, burn_in=True)\n",
    "variational_state.set_cache_env_mode(True)\n",
    "variational_state.amplitude_grad(config)\n",
    "variational_state.set_cache_env_mode(False)\n",
    "# model.env_left_cache, model.env_right_cache,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb327ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "etas, _ = H.get_conn(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9488f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 22:59:57  Samples:  1460\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 1.680     CPU time: 1.680\n",
      "/   _/                      v5.0.1\n",
      "\n",
      "Profile at /tmp/ipykernel_12210/4286058046.py:2\n",
      "\n",
      "1.679 <module>  /tmp/ipykernel_12210/4286058046.py:1\n",
      "└─ 1.679 fMPS_BFA_cluster_Model_reuse._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "   └─ 1.679 fMPS_BFA_cluster_Model_reuse._call_impl  torch/nn/modules/module.py:1743\n",
      "      └─ 1.679 fMPS_BFA_cluster_Model_reuse.forward  ../tn_model.py:96\n",
      "         └─ 1.679 fMPS_BFA_cluster_Model_reuse.amplitude  ../tn_model.py:1562\n",
      "            ├─ 0.988 fMPS_BFA_cluster_Model_reuse.get_amp_tn  ../tn_model.py:1410\n",
      "            │  ├─ 0.278 MatrixProductState.select  quimb/tensor/tensor_core.py:5100\n",
      "            │  │     [5 frames hidden]  quimb\n",
      "            │  ├─ 0.209 reconstruct_proj_params  ../../fermion_utils.py:1317\n",
      "            │  │  ├─ 0.138 [self]  ../../fermion_utils.py\n",
      "            │  │  └─ 0.061 Tensor.reshape  <built-in>\n",
      "            │  ├─ 0.169 fMPS_BFA_cluster_Model_reuse.get_local_amp_tensors  ../tn_model.py:1379\n",
      "            │  │  ├─ 0.082 fMPS.fix_phys_inds  ../../fermion_utils.py:802\n",
      "            │  │  ├─ 0.029 unpack  quimb/tensor/interface.py:50\n",
      "            │  │  │  └─ 0.025 fMPS.copy  quimb/tensor/tensor_core.py:4065\n",
      "            │  │  │     └─ 0.023 fMPS.__init__  ../../fermion_utils.py:623\n",
      "            │  │  └─ 0.017 <dictcomp>  ../tn_model.py:1388\n",
      "            │  ├─ 0.111 SelfAttn_FFNN_block._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "            │  │  └─ 0.111 SelfAttn_FFNN_block._call_impl  torch/nn/modules/module.py:1743\n",
      "            │  │     └─ 0.107 SelfAttn_FFNN_block.forward  ../nn_sublayers.py:137\n",
      "            │  │        └─ 0.080 MultiheadAttention._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "            │  │              [4 frames hidden]  torch\n",
      "            │  ├─ 0.107 MatrixProductState.__or__  quimb/tensor/tensor_core.py:3975\n",
      "            │  │     [9 frames hidden]  quimb\n",
      "            │  ├─ 0.076 fMPS_BFA_cluster_Model_reuse.detect_effected_sites  ../tn_model.py:1343\n",
      "            │  └─ 0.019 [self]  ../tn_model.py\n",
      "            ├─ 0.546 TensorNetwork.contract  quimb/tensor/tensor_core.py:8934\n",
      "            │     [24 frames hidden]  functools, quimb, cotengra, symmray\n",
      "            ├─ 0.045 fMPS_BFA_cluster_Model_reuse.detect_effected_sites  ../tn_model.py:1343\n",
      "            ├─ 0.039 fMPS_BFA_cluster_Model_reuse.detect_changed_sites  ../tn_model.py:1361\n",
      "            └─ 0.036 [self]  ../tn_model.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyinstrument\n",
    "with pyinstrument.Profiler() as prof:\n",
    "    with torch.no_grad():\n",
    "        eta_amps = model(etas)\n",
    "prof.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3815f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  _     ._   __/__   _ _  _  _ _/_   Recorded: 22:59:59  Samples:  10045\n",
      " /_//_/// /_\\ / //_// / //_'/ //     Duration: 10.330    CPU time: 10.331\n",
      "/   _/                      v5.0.1\n",
      "\n",
      "Profile at /tmp/ipykernel_12210/2291535159.py:1\n",
      "\n",
      "10.329 <module>  /tmp/ipykernel_12210/2291535159.py:1\n",
      "└─ 10.329 fMPS_BFA_cluster_Model._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "   └─ 10.329 fMPS_BFA_cluster_Model._call_impl  torch/nn/modules/module.py:1743\n",
      "      └─ 10.329 fMPS_BFA_cluster_Model.forward  ../tn_model.py:96\n",
      "         └─ 10.329 fMPS_BFA_cluster_Model.amplitude  ../tn_model.py:1139\n",
      "            ├─ 6.083 MatrixProductState.contract  quimb/tensor/tensor_core.py:8934\n",
      "            │     [47 frames hidden]  quimb, functools, cotengra, symmray, ...\n",
      "            ├─ 2.201 <listcomp>  ../tn_model.py:1164\n",
      "            │  └─ 2.153 SelfAttn_FFNN_block._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "            │     └─ 2.142 SelfAttn_FFNN_block._call_impl  torch/nn/modules/module.py:1743\n",
      "            │        └─ 2.111 SelfAttn_FFNN_block.forward  ../nn_sublayers.py:137\n",
      "            │           ├─ 1.693 MultiheadAttention._wrapped_call_impl  torch/nn/modules/module.py:1735\n",
      "            │           │     [13 frames hidden]  torch, <built-in>\n",
      "            │           ├─ 0.128 [self]  ../nn_sublayers.py\n",
      "            │           └─ 0.104 one_hot  <built-in>\n",
      "            ├─ 1.470 fMPS.get_amp  ../../fermion_utils.py:712\n",
      "            │  └─ 1.467 fMPS.get_amp_efficient  ../../fermion_utils.py:734\n",
      "            │     ├─ 0.336 do  autoray/autoray.py:30\n",
      "            │     │     [3 frames hidden]  autoray, torch, <built-in>\n",
      "            │     ├─ 0.257 FermionicArray.from_blocks  symmray/abelian_core.py:1489\n",
      "            │     ├─ 0.238 Tensor.modify  quimb/tensor/tensor_core.py:1653\n",
      "            │     ├─ 0.153 fMPS.copy  quimb/tensor/tensor_core.py:4065\n",
      "            │     │  └─ 0.152 fMPS.__init__  ../../fermion_utils.py:623\n",
      "            │     │     └─ 0.139 fMPS.__init__  quimb/tensor/tensor_1d.py:1546\n",
      "            │     │        └─ 0.139 fMPS.__init__  quimb/tensor/tensor_core.py:3914\n",
      "            │     ├─ 0.138 [self]  ../../fermion_utils.py\n",
      "            │     ├─ 0.133 fMPS.__getitem__  quimb/tensor/tensor_core.py:5433\n",
      "            │     │     [2 frames hidden]  quimb\n",
      "            │     └─ 0.122 MatrixProductState.__init__  quimb/tensor/tensor_1d.py:1546\n",
      "            │        └─ 0.122 MatrixProductState.__init__  quimb/tensor/tensor_core.py:3914\n",
      "            ├─ 0.252 unpack  quimb/tensor/interface.py:50\n",
      "            │  └─ 0.164 fMPS.copy  quimb/tensor/tensor_core.py:4065\n",
      "            │     └─ 0.163 fMPS.__init__  ../../fermion_utils.py:623\n",
      "            │        └─ 0.155 fMPS.__init__  quimb/tensor/tensor_1d.py:1546\n",
      "            │           └─ 0.155 fMPS.__init__  quimb/tensor/tensor_core.py:3914\n",
      "            └─ 0.181 reconstruct_proj_params  ../../fermion_utils.py:1317\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with pyinstrument.Profiler() as prof0:\n",
    "    with torch.no_grad():\n",
    "        eta_amps = model0(etas)\n",
    "prof0.print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
