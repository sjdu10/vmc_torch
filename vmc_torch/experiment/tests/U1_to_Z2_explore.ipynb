{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mpi4py import MPI\n",
    "import ast\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.fermion_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U1 to Z2 cannot avoid the dynamic shapes in TNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 10/10 [00:02<00:00,  4.12it/s]/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/cotengra/hyperoptimizers/hyper.py:54: UserWarning: Couldn't find `optuna`, `baytune (btb)`, `chocolate`, `nevergrad` or `skopt` so will use completely random sampling in place of hyper-optimization.\n",
      "  warnings.warn(\n",
      "n=10, tau=0.3000, energy~-0.413470: 100%|##########| 10/10 [00:03<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "Lx = 6\n",
    "Ly = 6\n",
    "D = 4\n",
    "symmetry = 'U1'\n",
    "N_f = int(4*4/2)-2\n",
    "# Create a random PEPS\n",
    "peps, parity_config_u1 = generate_random_fpeps(Lx, Ly, D, seed=2, symmetry=symmetry, Nf=N_f)\n",
    "\n",
    "# Create a random configuration\n",
    "random_conf = np.zeros(Lx*Ly)\n",
    "random_conf[:N_f] = 1\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(random_conf)\n",
    "\n",
    "t = 1.0\n",
    "V = 4.0\n",
    "mu = 0.0\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "site_info = sr.utils.parse_edges_to_site_info(\n",
    "    edges,\n",
    "    D,\n",
    "    phys_dim=2,\n",
    "    site_ind_id=\"k{},{}\",\n",
    "    site_tag_id=\"I{},{}\",\n",
    ")\n",
    "terms = {\n",
    "    (sitea, siteb): sr.fermi_hubbard_spinless_local_array(\n",
    "        t=t, V=V, mu=mu,\n",
    "        symmetry=symmetry,\n",
    "        coordinations=(\n",
    "            site_info[sitea]['coordination'],\n",
    "            site_info[siteb]['coordination'],\n",
    "        ),\n",
    "    ).fuse((0, 1), (2, 3))\n",
    "    for (sitea, siteb) in peps.gen_bond_coos()\n",
    "}\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "su_u1 = qtn.SimpleUpdateGen(peps, ham, compute_energy_per_site=True,D=D, compute_energy_opts={\"max_distance\":1}, gate_opts={'cutoff':1e-10})\n",
    "su_u1.evolve(10, 0.3)\n",
    "# su_u1.evolve(50, 0.1)\n",
    "peps_U1 = su_u1.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n=5, tau=0.3000, energy~-0.225428: 100%|##########| 5/5 [00:01<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "Lx = 6\n",
    "Ly = 6\n",
    "D = 4\n",
    "symmetry = 'Z2'\n",
    "N_f = int(4*4/2)-2\n",
    "# Create a random PEPS\n",
    "peps, parity_config_z2 = generate_random_fpeps(Lx, Ly, D, seed=2, symmetry=symmetry, Nf=N_f)\n",
    "\n",
    "# Create a random configuration\n",
    "random_conf = np.zeros(Lx*Ly)\n",
    "random_conf[:N_f] = 1\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(random_conf)\n",
    "\n",
    "t = 1.0\n",
    "V = 4.0\n",
    "mu = 0.0\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "site_info = sr.utils.parse_edges_to_site_info(\n",
    "    edges,\n",
    "    D,\n",
    "    phys_dim=2,\n",
    "    site_ind_id=\"k{},{}\",\n",
    "    site_tag_id=\"I{},{}\",\n",
    ")\n",
    "terms = {\n",
    "    (sitea, siteb): sr.fermi_hubbard_spinless_local_array(\n",
    "        t=t, V=V, mu=mu,\n",
    "        symmetry=symmetry,\n",
    "        coordinations=(\n",
    "            site_info[sitea]['coordination'],\n",
    "            site_info[siteb]['coordination'],\n",
    "        ),\n",
    "    ).fuse((0, 1), (2, 3))\n",
    "    for (sitea, siteb) in peps.gen_bond_coos()\n",
    "}\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "su_z2 = qtn.SimpleUpdateGen(peps, ham, compute_energy_per_site=True,D=D, compute_energy_opts={\"max_distance\":1}, gate_opts={'cutoff':1e-10})\n",
    "su_z2.evolve(5, 0.3)\n",
    "# su_z2.evolve(50, 0.1)\n",
    "peps_Z2 = su_z2.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BlockIndex(chargemap={0: 2, 1: 2}, dual=True),\n",
       " BlockIndex(chargemap={0: 2, 1: 2}, dual=False),\n",
       " BlockIndex(chargemap={0: 2, 1: 2}, dual=False),\n",
       " BlockIndex(chargemap={0: 1, 1: 1}, dual=False))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_Z2.arrays[4].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BlockIndex(chargemap={-1: 1, 0: 2, 1: 1}, dual=True),\n",
       " BlockIndex(chargemap={-1: 1, 0: 2, 1: 1}, dual=False),\n",
       " BlockIndex(chargemap={0: 1, 1: 1}, dual=False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_U1.arrays[5].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8-,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peps_U1.arrays[5].oddpos + peps_U1.arrays[4].oddpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BlockIndex(chargemap={0: 6, 1: 4}, dual=True, subinfo=SubIndexInfo(indices=(BlockIndex(chargemap={-1: 1, 0: 2, 1: 1}, dual=True), BlockIndex(chargemap={-1: 1, 0: 2, 1: 1}, dual=False)), extents={0: {(-1, -1): 1, (0, 0): 4, (1, 1): 1}, 1: {(0, -1): 2, (1, 0): 2}})),\n",
       " BlockIndex(chargemap={0: 1, 1: 1}, dual=False))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quimb.utils import oset\n",
    "test_ts = peps_U1.tensors[5]\n",
    "test_ts_arr = test_ts.data\n",
    "left_inds = oset(test_ts.inds[:2])\n",
    "right_inds = oset(test_ts.inds) - oset(left_inds)\n",
    "TT = test_ts.transpose(*left_inds, *right_inds)\n",
    "# TT.data, TT.data.indices\n",
    "left_dims = TT.shape[: len(left_inds)]\n",
    "right_dims = TT.shape[len(left_inds) :]\n",
    "TT_data_fused = TT.data.reshape((prod(left_dims), prod(right_dims)))\n",
    "TT_data_fused.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), (((0, 1),),), ())"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_reshape_args(shape, newshape, subsizes, squeeze=True):\n",
    "    # tracks position in input shape\n",
    "    i = 0\n",
    "    # tracks position in output shape\n",
    "    j = 0\n",
    "    # tracks position in post-fuse / pre-expand shape\n",
    "    k = 0\n",
    "\n",
    "    ndim_old = len(shape)\n",
    "    ndim_new = len(newshape)\n",
    "\n",
    "    term = []  # dnyamically updated labelled dimensions\n",
    "    axs_squeeze = []\n",
    "    unfuse_sizes = {}\n",
    "    fuse_sizes = {}\n",
    "    axs_expand = []\n",
    "    any_singleton = False\n",
    "    any_fused = False\n",
    "\n",
    "    while i < ndim_old and j < ndim_new:\n",
    "        di = shape[i]\n",
    "        dj = newshape[j]\n",
    "\n",
    "        if subsizes[i] is not None and subsizes[i][0] == dj and j != ndim_new - 1 and subsizes[i][1] == newshape[j + 1]:\n",
    "            # unfuse, check first\n",
    "            label = f\"u{len(unfuse_sizes)}\"\n",
    "            s = 0\n",
    "            for ds in subsizes[i]:\n",
    "                dj = newshape[j]\n",
    "                if ds != dj:\n",
    "                    raise ValueError(\"Shape mismatch for unfuse.\")\n",
    "                s += 1\n",
    "                j += 1\n",
    "                k += 1\n",
    "            unfuse_sizes[label] = s\n",
    "            term.append(label)\n",
    "            i += 1\n",
    "        elif di == dj:\n",
    "            # output dimension already\n",
    "            term.append(\"o\")\n",
    "            i += 1\n",
    "            j += 1\n",
    "            k += 1\n",
    "        elif di == 1:\n",
    "            # have to handle squeezed dimensions after unfusing\n",
    "            term.append(\"s\")\n",
    "            axs_squeeze.append(i)\n",
    "            any_singleton = True\n",
    "            i += 1\n",
    "        elif dj == 1:\n",
    "            # record expansion location relative to *post* fuse shape\n",
    "            axs_expand.append(k)\n",
    "            j += 1\n",
    "        elif di < dj:\n",
    "            # need to fuse\n",
    "            label = f\"g{len(fuse_sizes)}\"\n",
    "            term.append(label)\n",
    "            s = 1\n",
    "            i += 1\n",
    "            while di < dj:\n",
    "                di *= shape[i]\n",
    "                term.append(label)\n",
    "                i += 1\n",
    "                s += 1\n",
    "            if di != dj:\n",
    "                raise ValueError(\"Shape mismatch for fuse.\")\n",
    "            fuse_sizes[label] = s\n",
    "            any_fused = True\n",
    "            j += 1\n",
    "            k += 1\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch.\")\n",
    "\n",
    "    # check trailing dimensions, which should be size 1\n",
    "    for i in range(i, ndim_old):\n",
    "        any_singleton = True\n",
    "        term.append(\"s\")\n",
    "    for j in range(j, ndim_new):\n",
    "        axs_expand.append(k)\n",
    "\n",
    "    # first we handle unfusings\n",
    "    axs_unfuse = []\n",
    "    for label, s in unfuse_sizes.items():\n",
    "        ax = term.index(label)\n",
    "        axs_unfuse.append(ax)\n",
    "        term = term[:ax] + [\"o\"] * s + term[ax + 1 :]\n",
    "\n",
    "    # handle squeezes by converting them into fuse groups\n",
    "    if any_singleton:\n",
    "        i = 0\n",
    "        label = term[i]\n",
    "        if label == \"s\":\n",
    "            # if we have squeeze axes on left, we have to group into right\n",
    "            while label == \"s\":\n",
    "                i += 1\n",
    "                label = term[i]\n",
    "\n",
    "            if label[0] == \"g\":\n",
    "                # adjacent to existing group\n",
    "                g = label\n",
    "            elif label == \"o\":\n",
    "                # or new group\n",
    "                g = f\"g{len(fuse_sizes)}\"\n",
    "                term[i] = g\n",
    "                fuse_sizes[g] = 1\n",
    "\n",
    "            # mark all axs up to this point\n",
    "            for j in range(0, i):\n",
    "                fuse_sizes[g] += 1\n",
    "                term[j] = g\n",
    "\n",
    "        # process rest of term, now preferring grouping into left\n",
    "        i += 1\n",
    "        while i < len(term):\n",
    "            label = term[i]\n",
    "            if label == \"s\":\n",
    "                left = term[i - 1]\n",
    "                if left[0] == \"g\":\n",
    "                    g = left\n",
    "                elif left == \"o\":\n",
    "                    g = f\"g{len(fuse_sizes)}\"\n",
    "                    term[i - 1] = g\n",
    "                    fuse_sizes[g] = 1\n",
    "\n",
    "                # update any right block of squeeze axs to g\n",
    "                while label == \"s\":\n",
    "                    term[i] = g\n",
    "                    fuse_sizes[g] += 1\n",
    "                    i += 1\n",
    "                    if i == len(term):\n",
    "                        break\n",
    "                    label = term[i]\n",
    "            i += 1\n",
    "\n",
    "    # now we handle fusing\n",
    "    axs_fuse = []\n",
    "    if any_fused or any_singleton:\n",
    "        # complexity here is we want to simulteneously fuse adjacent groups for\n",
    "        # efficiency, but also need to handle non-adjacent groups\n",
    "        current_groups = []\n",
    "        i = 0\n",
    "        while i < len(term):\n",
    "            label = term[i]\n",
    "            if label not in fuse_sizes:\n",
    "                if current_groups:\n",
    "                    # start of groups\n",
    "                    i0 = i - sum(map(len, current_groups))\n",
    "                    ng = len(current_groups)\n",
    "                    term = term[:i0] + [\"o\"] * ng + term[i:]\n",
    "                    axs_fuse.append(tuple(current_groups))\n",
    "                    current_groups = []\n",
    "                    # rewind to end of new group(s)\n",
    "                    i = i0 + ng\n",
    "                else:\n",
    "                    i += 1\n",
    "                continue\n",
    "\n",
    "            s = fuse_sizes[label]\n",
    "            current_groups.append(tuple(range(i, i + s)))\n",
    "            i += s\n",
    "        if current_groups:\n",
    "            axs_fuse.append(tuple(current_groups))\n",
    "\n",
    "    # handle expansion\n",
    "    axs_expand.reverse()\n",
    "\n",
    "    return tuple(axs_unfuse), tuple(axs_fuse), tuple(axs_expand)\n",
    "calc_reshape_args((4,4,4), (16,4), (None, None, (4,4)))\n",
    "# calc_reshape_args((4,4,4), (16,4), (None, None, None))\n",
    "# calc_reshape_args((4,4,3), (16,3), ((4, 4), None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symmray.abelian_core import  calc_reshape_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.7040331003839164e-07\n"
     ]
    }
   ],
   "source": [
    "# Create a random configuration\n",
    "for seed in range(1):\n",
    "    random_conf = np.zeros(Lx*Ly)\n",
    "    random_conf[:N_f] = 1\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(random_conf)\n",
    "    print(seed)\n",
    "    amp_U1 = peps_U1.get_amp(random_conf, conj=True)\n",
    "    print(amp_U1.contract())\n",
    "    amp_U1.contract_boundary_from_ymin_(max_bond=4, yrange=(0, amp_U1.Ly//2-2),cutoff=0.0, canonize=False)#, compress_opts={'reduced':'right'})\n",
    "    amp_U1.contract_boundary_from_ymax_(max_bond=4, yrange=(amp_U1.Ly//2, amp_U1.Ly-1),cutoff=0.0, canonize=False)\n",
    "    amp_U1.contract_boundary_from_ymin_(max_bond=4, yrange=(0, amp_U1.Ly//2-1),cutoff=0.0, canonize=False)\n",
    "    # amp_U1_params = amp_U1.get_params()\n",
    "    # amp_U1.draw()\n",
    "# # shape is dynamiclly changing based on the input configuration\n",
    "# for tid, blocks in amp_U1_params.items():\n",
    "#     for blk, data in blocks.items():\n",
    "#         print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "amp_U1.draw(color=['I0,1', 'I1,1', 'I2,1', 'I3,1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BlockIndex(chargemap={-2: 1, -1: 1, 0: 1, 1: 1}, dual=True),\n",
       " BlockIndex(chargemap={-1: 1, 0: 2, 1: 1}, dual=False),\n",
       " BlockIndex(chargemap={-1: 1, 0: 1, 1: 1, 2: 1}, dual=False))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# amp_U1.compress_between('I2,1', 'I3,1', max_bond=4, cutoff=0.0, reduced='left')\n",
    "ts = amp_U1.select(tags=['I2,1']).tensors[0]\n",
    "ts.data.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider dynamic output NN model - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijingdu/anaconda3/envs/vmc_torch/lib/python3.9/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, d_model=128, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Embedding layer for integer input sequence\n",
    "        self.embedding = nn.Embedding(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding for fixed-length sequences\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
    "        \n",
    "        # Linear layer for output generation (predicting floating-point values)\n",
    "        self.fc_out = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \n",
    "        src = src.transpose(0, 1)\n",
    "        tgt = tgt.transpose(0, 1)\n",
    "\n",
    "        # Encode source (input) sequence\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.embedding.embedding_dim, dtype=torch.float32))\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Positional encoding for target (output) sequence\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Apply transformer\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Predict floating-point values for each output token\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        # Transpose output to match the shape [batch_size, seq_len, output_size]\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage for binary string\n",
    "input_size = 2  # Assume input integers range from 0 to 99\n",
    "output_size = 1   # Each output is a single floating-point number\n",
    "\n",
    "# Create the model\n",
    "model = TransformerModel(input_size, output_size)\n",
    "\n",
    "# # Example input: a fixed-length sequence of integers (batch size = 1, seq length = 10)\n",
    "# src = torch.tensor([[1, 0, 1, 1], [1, 0, 1, 1]], dtype=torch.long)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "# # Example target (dynamic length sequence): start with a few steps (2 in this case)\n",
    "# tgt = torch.zeros((2, 2, 128))  # Shape: [batch_size, seq_len,  d_model]\n",
    "\n",
    "# # Forward pass\n",
    "# output = model(src, tgt)\n",
    "\n",
    "# print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.672725148729228e-08\n",
      "379\n",
      "torch.Size([1, 36])\n",
      "-6.62919670252867e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "pyinstrument ........................................\n",
      ".\n",
      ".  Block at /tmp/ipykernel_7419/3205206528.py:7\n",
      ".\n",
      ".  0.231 <module>  ../../../../../tmp/ipykernel_7419/3205206528.py:7\n",
      ".  ├─ 0.065 PEPS.contract  quimb/tensor/tensor_core.py:8396\n",
      ".  │     [5 frames hidden]  functools, quimb, cotengra\n",
      ".  │        0.062 wrapper  functools.py:883\n",
      ".  │        └─ 0.061 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".  │           ├─ 0.041 tensordot_abelian  symmray/abelian_core.py:1997\n",
      ".  │           │  └─ 0.040 _tensordot_via_fused  symmray/abelian_core.py:1942\n",
      ".  │           │     ├─ 0.018 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".  │           │     │  ├─ 0.007 U1FermionicArray.cached_fuse_block_info  symmray/abelian_core.py:670\n",
      ".  │           │     │  │  └─ 0.005 U1FermionicArray.calc_fuse_block_info  symmray/abelian_core.py:534\n",
      ".  │           │     │  ├─ 0.006 <dictcomp>  symmray/abelian_core.py:1525\n",
      ".  │           │     │  │  └─ 0.006 _recurse_concat  symmray/abelian_core.py:1485\n",
      ".  │           │     │  │     └─ 0.004 <genexpr>  symmray/abelian_core.py:1518\n",
      ".  │           │     │  │        └─ 0.003 _recurse_concat  symmray/abelian_core.py:1485\n",
      ".  │           │     │  └─ 0.003 [self]  symmray/abelian_core.py\n",
      ".  │           │     ├─ 0.009 U1FermionicArray.unfuse  symmray/abelian_core.py:1537\n",
      ".  │           │     │  └─ 0.006 [self]  symmray/abelian_core.py\n",
      ".  │           │     ├─ 0.008 _tensordot_blockwise  symmray/abelian_core.py:1832\n",
      ".  │           │     │  └─ 0.008 <genexpr>  symmray/abelian_core.py:1886\n",
      ".  │           │     │     └─ 0.007 tensordot  numpy/_core/numeric.py:978\n",
      ".  │           │     └─ 0.005 drop_misaligned_sectors  symmray/abelian_core.py:1900\n",
      ".  │           ├─ 0.013 U1FermionicArray.transpose  symmray/fermionic_core.py:232\n",
      ".  │           │  └─ 0.007 U1FermionicArray.transpose  symmray/abelian_core.py:1271\n",
      ".  │           │     └─ 0.006 <dictcomp>  symmray/abelian_core.py:1282\n",
      ".  │           │        └─ 0.003 permuted  symmray/abelian_core.py:270\n",
      ".  │           └─ 0.003 U1FermionicArray.phase_flip  symmray/fermionic_core.py:282\n",
      ".  ├─ 0.047 PEPS.contract_boundary_from_ymin  quimb/tensor/tensor_2d.py:2003\n",
      ".  │     [31 frames hidden]  quimb, functools, autoray, cotengra\n",
      ".  │        0.008 wrapper  functools.py:883\n",
      ".  │        └─ 0.008 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".  │           └─ 0.006 tensordot_abelian  symmray/abelian_core.py:1997\n",
      ".  │              └─ 0.006 _tensordot_via_fused  symmray/abelian_core.py:1942\n",
      ".  │        0.006 do  autoray/autoray.py:30\n",
      ".  │        └─ 0.006 fuse  symmray/interface.py:117\n",
      ".  │           └─ 0.006 U1FermionicArray.fuse  symmray/fermionic_core.py:556\n",
      ".  │              └─ 0.003 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".  │        0.003 Composed.__call__  autoray/autoray.py:921\n",
      ".  │        └─ 0.003 svd_truncated  symmray/linalg.py:212\n",
      ".  │           └─ 0.003 wrapper  functools.py:883\n",
      ".  │              └─ 0.003 svd_fermionic  symmray/linalg.py:173\n",
      ".  │                 └─ 0.003 svd  symmray/linalg.py:123\n",
      ".  │                    └─ 0.002 default_not_full_matrices  autoray/autoray.py:1141\n",
      ".  │                          [1 frames hidden]  numpy\n",
      ".  │        0.009 wrapper  functools.py:883\n",
      ".  │        └─ 0.009 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".  │           └─ 0.007 tensordot_abelian  symmray/abelian_core.py:1997\n",
      ".  │              └─ 0.007 _tensordot_via_fused  symmray/abelian_core.py:1942\n",
      ".  │                 └─ 0.004 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".  ├─ 0.045 TransformerModel._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      ".  │     [0 frames hidden]  \n",
      ".  │        0.045 TransformerModel._call_impl  torch/nn/modules/module.py:1555\n",
      ".  │        └─ 0.045 TransformerModel.forward  ../../../../../tmp/ipykernel_7419/953560398.py:35\n",
      ".  │           └─ 0.045 Transformer._wrapped_call_impl  torch/nn/modules/module.py:1549\n",
      ".  │                 [39 frames hidden]  torch, <built-in>\n",
      ".  ├─ 0.044 PEPS.contract_boundary_from_ymax  quimb/tensor/tensor_2d.py:2137\n",
      ".  │     [37 frames hidden]  quimb, functools, autoray, cotengra\n",
      ".  │        0.005 do  autoray/autoray.py:30\n",
      ".  │        └─ 0.005 reshape  symmray/interface.py:58\n",
      ".  │           └─ 0.005 U1FermionicArray.reshape  symmray/abelian_core.py:1620\n",
      ".  │              └─ 0.004 U1FermionicArray.fuse  symmray/fermionic_core.py:556\n",
      ".  │                 └─ 0.003 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".  │        0.005 do  autoray/autoray.py:30\n",
      ".  │        └─ 0.005 fuse  symmray/interface.py:117\n",
      ".  │           └─ 0.005 U1FermionicArray.fuse  symmray/fermionic_core.py:556\n",
      ".  │              └─ 0.003 U1FermionicArray.transpose  symmray/fermionic_core.py:232\n",
      ".  │                 └─ 0.003 U1FermionicArray.transpose  symmray/abelian_core.py:1271\n",
      ".  │                    └─ 0.003 <dictcomp>  symmray/abelian_core.py:1282\n",
      ".  │        0.004 wrapper  functools.py:883\n",
      ".  │        └─ 0.004 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".  │           └─ 0.003 tensordot_abelian  symmray/abelian_core.py:1997\n",
      ".  │              └─ 0.003 _tensordot_via_fused  symmray/abelian_core.py:1942\n",
      ".  │                 └─ 0.003 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".  │        0.004 do  autoray/autoray.py:30\n",
      ".  │        └─ 0.004 reshape  symmray/interface.py:58\n",
      ".  │           └─ 0.004 U1FermionicArray.reshape  symmray/abelian_core.py:1620\n",
      ".  │        0.005 wrapper  functools.py:883\n",
      ".  │        └─ 0.005 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".  └─ 0.026 fPEPS.get_amp  vmc_torch/fermion_utils.py:32\n",
      ".     └─ 0.021 TensorNetwork.contract  quimb/tensor/tensor_core.py:8396\n",
      ".           [6 frames hidden]  quimb, functools, cotengra\n",
      ".              0.014 wrapper  functools.py:883\n",
      ".              └─ 0.014 tensordot_fermionic  symmray/fermionic_core.py:711\n",
      ".                 ├─ 0.010 tensordot_abelian  symmray/abelian_core.py:1997\n",
      ".                 │  └─ 0.009 _tensordot_via_fused  symmray/abelian_core.py:1942\n",
      ".                 │     ├─ 0.004 drop_misaligned_sectors  symmray/abelian_core.py:1900\n",
      ".                 │     │  └─ 0.003 <dictcomp>  symmray/abelian_core.py:1917\n",
      ".                 │     └─ 0.003 U1FermionicArray.fuse  symmray/abelian_core.py:1419\n",
      ".                 └─ 0.003 U1FermionicArray.transpose  symmray/fermionic_core.py:232\n",
      ".  \n",
      ".....................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyinstrument\n",
    "# Create a random configuration\n",
    "random_conf = np.zeros(Lx*Ly)\n",
    "random_conf[:N_f] = 1\n",
    "# np.random.seed(1)\n",
    "np.random.shuffle(random_conf)\n",
    "with pyinstrument.profile():\n",
    "    amp_U1 = peps_U1.get_amp(random_conf, conj=True)\n",
    "    print(amp_U1.contract())\n",
    "    amp_U1.contract_boundary_from_ymin_(max_bond=8, yrange=(0, amp_U1.Ly//2-1),cutoff=0.0)\n",
    "    amp_U1.contract_boundary_from_ymax_(max_bond=8, yrange=(amp_U1.Ly//2, amp_U1.Ly-1),cutoff=0.0)\n",
    "    amp_U1_params, amp_U1_skeleton = qtn.pack(amp_U1)\n",
    "    flattend_params = flatten_proj_params(amp_U1_params)\n",
    "    vec_len = len(flattend_params)\n",
    "    print(vec_len)\n",
    "    random_conf = torch.tensor(random_conf, dtype=torch.int32)\n",
    "    # input is the random configuration\n",
    "    # src = torch.tensor([random_conf], dtype=torch.long) # Shape: [batch_size, seq_len]\n",
    "    src = random_conf.unsqueeze(0)\n",
    "    # target output is a vector of the shape of the flattened parameters\n",
    "    tgt = torch.zeros((1, vec_len, 128)) # Shape: [batch_size, seq_len,  d_model]\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(src, tgt) # shape [batch_size, seq_len, output_size]\n",
    "\n",
    "    # concatenate the output to get the final vector of length vec_len\n",
    "    output = output.view(-1)\n",
    "\n",
    "    new_flattend_params = flattend_params + 0.01*output.detach().numpy()\n",
    "    new_params = reconstruct_proj_params(new_flattend_params, amp_U1_params)\n",
    "\n",
    "    new_amp_U1 = qtn.unpack(new_params, amp_U1_skeleton)\n",
    "    print(new_amp_U1.contract())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
