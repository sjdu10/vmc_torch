{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cd0c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "\n",
    "\n",
    "# torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "# from vmc_torch.experiment.tn_model import *\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, SignedSGD, SignedRandomSGD, SR, TrivialPreconditioner, Adam, SGD_momentum, DecayScheduler\n",
    "from vmc_torch.VMC import VMC\n",
    "# from vmc_torch.hamiltonian import spinful_Fermi_Hubbard_square_lattice\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "from vmc_torch.fermion_utils import generate_random_fpeps\n",
    "from vmc_torch.utils import closest_divisible\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "Lx = int(4)\n",
    "Ly = int(4)\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx*Ly-2)\n",
    "# N_f=12\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "# TN parameters\n",
    "D = 4\n",
    "chi = 16\n",
    "dtype=torch.float64\n",
    "\n",
    "# Load PEPS\n",
    "try:\n",
    "    skeleton = pickle.load(open(f\"../../data/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_skeleton.pkl\", \"rb\"))\n",
    "    peps_params = pickle.load(open(f\"../../data/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D}/peps_su_params.pkl\", \"rb\"))\n",
    "    peps = qtn.unpack(peps_params, skeleton)\n",
    "except:\n",
    "    peps = generate_random_fpeps(Lx, Ly, D=D, seed=2, symmetry=symmetry, Nf=N_f, spinless=False)[0]\n",
    "peps_np = peps.copy()\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "\n",
    "# VMC sample size\n",
    "N_samples = 2\n",
    "N_samples = closest_divisible(N_samples, SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b3ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.experiment.tn_model import wavefunctionModel\n",
    "import ast\n",
    "import cotengra as ctg\n",
    "class fTNModel_reuse(wavefunctionModel):\n",
    "    def __init__(self, ftn, max_bond=None, dtype=torch.float32, functional=False, debug=False):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        self.functional = functional\n",
    "        self.debug = debug\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "        self.skeleton.exponent = 0\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            f'fPEPS (chi={max_bond})':{'D': ftn.max_bond(), 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry},\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        self.tree = None\n",
    "        self.Lx = ftn.Lx\n",
    "        self.Ly = ftn.Ly\n",
    "        self._env_x_cache = None\n",
    "        self._env_y_cache = None\n",
    "        self.config_ref = None\n",
    "        self.amp_ref = None\n",
    "    \n",
    "    def from_1d_to_2d(self, config, ordering='snake'):\n",
    "        if ordering == 'snake':\n",
    "            config_2d = config.reshape((self.Lx, self.Ly))\n",
    "            return config_2d\n",
    "        else:\n",
    "            raise NotImplementedError(f'Ordering {ordering} is not implemented.')\n",
    "        \n",
    "    def from_1dsite_to_2dsite(self, site, ordering='snake'):\n",
    "        \"\"\"\n",
    "            Convert a 1d site index to a 2d site index.\n",
    "            site: 1d site index\n",
    "        \"\"\"\n",
    "        if ordering == 'snake':\n",
    "            return (site // self.Ly, site % self.Ly)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ordering: {ordering}\")\n",
    "    \n",
    "    def transform_quimb_env_x_key_to_config_key(self, env_x, config):\n",
    "        \"\"\"\n",
    "            Return a dictionary with the keys of of the config rows\n",
    "        \"\"\"\n",
    "        config_2d = self.from_1d_to_2d(config)\n",
    "        env_x_row_config = {}\n",
    "        for key in env_x.keys():\n",
    "            if key[0] == 'xmax': # from bottom to top\n",
    "                row_n = key[1]\n",
    "                if row_n != self.Lx-1:\n",
    "                    rows_config = tuple(torch.cat(tuple(config_2d[row_n+1:].to(torch.int))).tolist())\n",
    "                    env_x_row_config[('xmax', rows_config)] = env_x[key]\n",
    "            elif key[0] == 'xmin': # from top to bottom\n",
    "                row_n = key[1]\n",
    "                if row_n != 0:\n",
    "                    rows_config = tuple(torch.cat(tuple(config_2d[:row_n].to(torch.int))).tolist())\n",
    "                    env_x_row_config[('xmin', rows_config)] = env_x[key]\n",
    "        return env_x_row_config\n",
    "    \n",
    "    def transform_quimb_env_y_key_to_config_key(self, env_y, config):\n",
    "        \"\"\"\n",
    "            Return a dictionary with the keys of of the config rows\n",
    "        \"\"\"\n",
    "        config_2d = self.from_1d_to_2d(config)\n",
    "        env_y_row_config = {}\n",
    "        for key in env_y.keys():\n",
    "            if key[0] == 'ymax':\n",
    "                col_n = key[1]\n",
    "                if col_n != self.Ly-1:\n",
    "                    cols_config = tuple(torch.cat(tuple(config_2d[:, col_n+1:].to(torch.int))).tolist())\n",
    "                    env_y_row_config[('ymax', cols_config)] = env_y[key]\n",
    "            elif key[0] == 'ymin':\n",
    "                col_n = key[1]\n",
    "                if col_n != 0:\n",
    "                    cols_config = tuple(torch.cat(tuple(config_2d[:, :col_n].to(torch.int))).tolist())\n",
    "                    env_y_row_config[('ymin', cols_config)] = env_y[key]\n",
    "        return env_y_row_config\n",
    "\n",
    "    def cache_env_x(self, amp, config):\n",
    "        \"\"\"\n",
    "            Cache the environment x for the given configuration\n",
    "        \"\"\"\n",
    "        env_x = amp.compute_x_environments(max_bond=self.max_bond, cutoff=0.0)\n",
    "        env_x_cache = self.transform_quimb_env_x_key_to_config_key(env_x, config)\n",
    "        self._env_x_cache = env_x_cache\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp\n",
    "    \n",
    "    def cache_env_y(self, amp, config):\n",
    "        env_y = amp.compute_y_environments(max_bond=self.max_bond, cutoff=0.0)\n",
    "        env_y_cache = self.transform_quimb_env_y_key_to_config_key(env_y, config)\n",
    "        self._env_y_cache = env_y_cache\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp\n",
    "    \n",
    "    def cache_env(self, amp, config):\n",
    "        \"\"\"\n",
    "            Cache the environment x and y for the given configuration\n",
    "        \"\"\"\n",
    "        self.cache_env_x(amp, config)\n",
    "        self.cache_env_y(amp, config)\n",
    "        \n",
    "    @property\n",
    "    def env_x_cache(self):\n",
    "        \"\"\"\n",
    "            Return the cached environment x\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_env_x_cache'):\n",
    "            return self._env_x_cache\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    @property\n",
    "    def env_y_cache(self):\n",
    "        \"\"\"\n",
    "            Return the cached environment y\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_env_y_cache'):\n",
    "            return self._env_y_cache\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def clear_env_x_cache(self):\n",
    "        \"\"\"\n",
    "            Clear the cached environment x\n",
    "        \"\"\"\n",
    "        self._env_x_cache = None\n",
    "\n",
    "    def clear_env_y_cache(self):\n",
    "        \"\"\"\n",
    "            Clear the cached environment y\n",
    "        \"\"\"\n",
    "        self._env_y_cache = None\n",
    "    \n",
    "    def clear_wavefunction_env_cache(self):\n",
    "        self.clear_env_x_cache()\n",
    "        self.clear_env_y_cache()\n",
    "        self.config_ref = None\n",
    "        self.amp_ref = None\n",
    "    \n",
    "    def detect_changed_sites(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "            Detect the sites that have changed in the new configuration,\n",
    "            written in 1d coordinate format.\n",
    "        \"\"\"\n",
    "        changed_sites = set()\n",
    "        unchanged_sites = set()\n",
    "        for i in range(self.Lx * self.Ly):\n",
    "            if config_ref[i] != new_config[i]:\n",
    "                changed_sites.add(i)\n",
    "            else:\n",
    "                unchanged_sites.add(i)\n",
    "        changed_sites = sorted(changed_sites)\n",
    "        unchanged_sites = sorted(unchanged_sites)\n",
    "        if len(changed_sites) == 0:\n",
    "            return [], []\n",
    "        return changed_sites, unchanged_sites\n",
    "\n",
    "    def from_1d_sites_to_tids(self, sites):\n",
    "        \"\"\"\n",
    "            Convert a list of 1d site indices to a list of tensor ids.\n",
    "        \"\"\"\n",
    "        tids_list = list(self.skeleton.tensor_map.keys())\n",
    "        return [tids_list[site] for site in sites]\n",
    "    \n",
    "    def detect_changed_rows(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "            Detect the rows that have changed in the new configuration\n",
    "        \"\"\"\n",
    "        config_ref_2d = self.from_1d_to_2d(config_ref)\n",
    "        new_config_2d = self.from_1d_to_2d(new_config)\n",
    "        changed_rows = []\n",
    "        for i in range(self.Lx):\n",
    "            if not torch.equal(config_ref_2d[i], new_config_2d[i]):\n",
    "                changed_rows.append(i)\n",
    "        if len(changed_rows) == 0:\n",
    "            return [], [], []\n",
    "        unchanged_rows_above = list(range(changed_rows[0]))\n",
    "        unchanged_rows_below = list(range(changed_rows[-1]+1, self.Lx))\n",
    "        return changed_rows, unchanged_rows_above, unchanged_rows_below\n",
    "    \n",
    "    def detect_changed_cols(self, config_ref, new_config):\n",
    "        \"\"\"\n",
    "            Detect the columns that have changed in the new configuration\n",
    "        \"\"\"\n",
    "        config_ref_2d = self.from_1d_to_2d(config_ref)\n",
    "        new_config_2d = self.from_1d_to_2d(new_config)\n",
    "        changed_cols = []\n",
    "        for i in range(self.Ly):\n",
    "            if not torch.equal(config_ref_2d[:, i], new_config_2d[:, i]):\n",
    "                changed_cols.append(i)\n",
    "        if len(changed_cols) == 0:\n",
    "            return [], [], []\n",
    "        unchanged_cols_left = list(range(changed_cols[0]))\n",
    "        unchanged_cols_right = list(range(changed_cols[-1]+1, self.Ly))\n",
    "        return changed_cols, unchanged_cols_left, unchanged_cols_right\n",
    "    \n",
    "    def update_env_x_cache(self, config):\n",
    "        \"\"\"\n",
    "            Update the cached environment x for the given configuration\n",
    "        \"\"\"\n",
    "        if self.env_x_cache is not None:\n",
    "            self.clear_env_x_cache()\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.cache_env_x(amp_tn, config)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "    \n",
    "    def update_env_x_cache_to_row(self, config, row_id, from_which='xmin'):\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        new_env_x = amp_tn.compute_environments(max_bond=self.max_bond, cutoff=0.0, xrange=(0, row_id+1) if from_which=='xmin' else (row_id-1, self.Lx-1), from_which=from_which)\n",
    "        new_env_x_cache = self.transform_quimb_env_x_key_to_config_key(new_env_x, config)\n",
    "        # add the new env_x to the cache\n",
    "        if self.env_x_cache is None:\n",
    "            self._env_x_cache = new_env_x_cache\n",
    "        else:\n",
    "            self._env_x_cache.update(new_env_x_cache)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "    \n",
    "    def update_env_y_cache(self, config):\n",
    "        \"\"\"\n",
    "            Update the cached environment y for the given configuration\n",
    "        \"\"\"\n",
    "        if self.env_y_cache is not None:\n",
    "            self.clear_env_y_cache()\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        self.cache_env_y(amp_tn, config)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "    \n",
    "    def update_env_y_cache_to_col(self, config, col_id, from_which='ymin'):\n",
    "        amp_tn = self.get_amp_tn(config)\n",
    "        new_env_y = amp_tn.compute_environments(max_bond=self.max_bond, cutoff=0.0, yrange=(0, col_id+1) if from_which=='ymin' else (col_id-1, self.Ly-1), from_which=from_which)\n",
    "        new_env_y_cache = self.transform_quimb_env_y_key_to_config_key(new_env_y, config)\n",
    "        # add the new env_y to the cache\n",
    "        if self.env_y_cache is None:\n",
    "            self._env_y_cache = new_env_y_cache\n",
    "        else:\n",
    "            self._env_y_cache.update(new_env_y_cache)\n",
    "        self.config_ref = config\n",
    "        self.amp_ref = amp_tn\n",
    "    \n",
    "    def psi(self):\n",
    "        \"\"\"\n",
    "            Return the wavefunction (fPEPS)\n",
    "        \"\"\"\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        return psi\n",
    "\n",
    "    def get_local_amp_tensors(self, sites:list, config:torch.Tensor):\n",
    "        \"\"\"\n",
    "            Get the local tensors for the given tensor ids and configuration.\n",
    "            tids: a list of tensor ids. list of int.\n",
    "            config: the input configuration.\n",
    "        \"\"\"\n",
    "        # first pick out the tensor parameters and form the local tn parameters vector\n",
    "        local_ts_params = {}\n",
    "        tids = self.from_1d_sites_to_tids(sites)\n",
    "        for tid in tids:\n",
    "            local_ts_params[tid] = {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in self.torch_tn_params[str(tid)].items()\n",
    "            }\n",
    "        \n",
    "        # Get sites corresponding to the tids\n",
    "        sites_2d = [self.from_1dsite_to_2dsite(site) for site in sites]\n",
    "\n",
    "        # Select the corresponding tensor skeleton\n",
    "        local_ts_skeleton = self.skeleton.select([self.skeleton.site_tag_id.format(*site) for site in sites_2d], which='any')\n",
    "\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        local_ftn = qtn.unpack(local_ts_params, local_ts_skeleton)\n",
    "\n",
    "        # Fix the physical indices\n",
    "        return local_ftn.fix_phys_inds(sites_2d, config[sites])\n",
    "    \n",
    "    def get_amp_tn(self, config, reconstruct=False):\n",
    "\n",
    "        if self.amp_ref is None or reconstruct:\n",
    "            psi = self.psi()\n",
    "            # Check config type\n",
    "            if not type(config) == torch.Tensor:\n",
    "                config = torch.tensor(config, dtype=torch.int if self.functional else self.param_dtype)\n",
    "            else:\n",
    "                if config.dtype != self.param_dtype:\n",
    "                    config = config.to(torch.int if self.functional else self.param_dtype)\n",
    "            # Get the amplitude\n",
    "            amp_tn = psi.get_amp(config, conj=True, functional=self.functional)\n",
    "            return amp_tn\n",
    "        \n",
    "        else:\n",
    "            # detect the sites that have changed\n",
    "            changed_sites, unchanged_sites = self.detect_changed_sites(self.config_ref, config)\n",
    "\n",
    "            if len(changed_sites) == 0:\n",
    "                return self.amp_ref\n",
    "            else:\n",
    "                # substitute the changed sites tensors\n",
    "                local_amp_tn = self.get_local_amp_tensors(changed_sites, config)\n",
    "                unchanged_sites_2d = [self.from_1dsite_to_2dsite(site) for site in unchanged_sites]\n",
    "                unchanged_sites_tags = [self.skeleton.site_tag_id.format(*site) for site in unchanged_sites_2d]\n",
    "                unchanged_amp_tn = self.amp_ref.select(unchanged_sites_tags, which='any')\n",
    "                # merge the local_amp_tn and unchanged_amp_tn\n",
    "                amp_tn = local_amp_tn | unchanged_amp_tn\n",
    "                return amp_tn\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Check x_i type\n",
    "            if not type(x_i) == torch.Tensor:\n",
    "                x_i = torch.tensor(x_i, dtype=torch.int if self.functional else self.param_dtype)\n",
    "            else:\n",
    "                if x_i.dtype != self.param_dtype:\n",
    "                    x_i = x_i.to(torch.int if self.functional else self.param_dtype)\n",
    "            # Get the amplitude\n",
    "            # amp = psi.get_amp(x_i, conj=True, functional=self.functional)\n",
    "            amp = self.get_amp_tn(x_i)\n",
    "\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "                if self.tree is None:\n",
    "                    opt = ctg.HyperOptimizer(progbar=True, max_repeats=10, parallel=True)\n",
    "                    self.tree = amp.contraction_tree(optimize=opt)\n",
    "                amp_val = amp.contract(optimize=self.tree)\n",
    "\n",
    "            else:\n",
    "                if self.cache_env_mode:\n",
    "                    self.cache_env_x(amp, x_i)\n",
    "                    # self.cache_env_y(amp, x_i)\n",
    "                    self.config_ref = x_i\n",
    "                    config_2d = self.from_1d_to_2d(x_i)\n",
    "                    key_bot = ('xmax', tuple(torch.cat(tuple(config_2d[self.Lx//2:].to(torch.int))).tolist()))\n",
    "                    key_top = ('xmin', tuple(torch.cat(tuple(config_2d[:self.Lx//2].to(torch.int))).tolist()))\n",
    "                    amp_bot = self.env_x_cache[key_bot]\n",
    "                    amp_top = self.env_x_cache[key_top]\n",
    "                    amp_val = (amp_bot|amp_top).contract()\n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    if self.env_x_cache is None and self.env_y_cache is None:\n",
    "                        # check whether we can reuse the cached environment\n",
    "                        amp = amp.contract_boundary_from_ymin(max_bond=self.max_bond, cutoff=0.0, yrange=[0, psi.Ly//2-1])\n",
    "                        amp = amp.contract_boundary_from_ymax(max_bond=self.max_bond, cutoff=0.0, yrange=[psi.Ly//2, psi.Ly-1])\n",
    "                        amp_val = amp.contract()\n",
    "                    else:\n",
    "                        config_2d = self.from_1d_to_2d(x_i)\n",
    "                        # detect the rows that have changed\n",
    "                        changed_rows, unchanged_rows_above, unchanged_rows_below = self.detect_changed_rows(self.config_ref, x_i)\n",
    "                        # detect the columns that have changed\n",
    "                        changed_cols, unchanged_cols_left, unchanged_cols_right = self.detect_changed_cols(self.config_ref, x_i)\n",
    "                        if len(changed_rows) == 0:\n",
    "                            key_bot = ('xmax', tuple(torch.cat(tuple(config_2d[self.Lx//2:].to(torch.int))).tolist()))\n",
    "                            key_top = ('xmin', tuple(torch.cat(tuple(config_2d[:self.Lx//2].to(torch.int))).tolist()))\n",
    "                            amp_bot = self.env_x_cache[key_bot]\n",
    "                            amp_top = self.env_x_cache[key_top]\n",
    "                            amp_val = (amp_bot|amp_top).contract()\n",
    "                        else:\n",
    "                            if len(changed_rows) <= len(changed_cols):\n",
    "                                # for bottom envs, until the last row in the changed rows, we can reuse the env\n",
    "                                # for top envs, until the first row in the changed rows, we can reuse the env\n",
    "                                amp_changed_rows = qtn.TensorNetwork([amp.select(amp.x_tag_id.format(row_n)) for row_n in changed_rows])\n",
    "                                amp_unchanged_bottom_env = qtn.TensorNetwork()\n",
    "                                amp_unchanged_top_env = qtn.TensorNetwork()\n",
    "                                if len(unchanged_rows_below) != 0:\n",
    "                                    amp_unchanged_bottom_env = self.env_x_cache[('xmax', tuple(torch.cat(tuple(config_2d[unchanged_rows_below].to(torch.int))).tolist()))]\n",
    "                                if len(unchanged_rows_above) != 0:\n",
    "                                    amp_unchanged_top_env = self.env_x_cache[('xmin', tuple(torch.cat(tuple(config_2d[unchanged_rows_above].to(torch.int))).tolist()))]\n",
    "                                amp_val = (amp_unchanged_bottom_env|amp_unchanged_top_env|amp_changed_rows).contract()\n",
    "                                # print(f'changed rows: {changed_rows}', self.from_1d_to_2d(x_i), self.from_1d_to_2d(self.config_ref))\n",
    "                            else:\n",
    "                                # for left envs, until the first column in the changed columns, we can reuse the env\n",
    "                                # for right envs, until the last column in the changed columns, we can reuse the env\n",
    "                                amp_changed_cols = qtn.TensorNetwork([amp.select(amp.y_tag_id.format(col_n)) for col_n in changed_cols])\n",
    "                                amp_unchanged_left_env = qtn.TensorNetwork()\n",
    "                                amp_unchanged_right_env = qtn.TensorNetwork()\n",
    "                                if len(unchanged_cols_left) != 0:\n",
    "                                    amp_unchanged_left_env = self.env_y_cache[('ymin', tuple(torch.cat(tuple(config_2d[:, unchanged_cols_left].to(torch.int))).tolist()))]\n",
    "                                if len(unchanged_cols_right) != 0:\n",
    "                                    amp_unchanged_right_env = self.env_y_cache[('ymax', tuple(torch.cat(tuple(config_2d[:, unchanged_cols_right].to(torch.int))).tolist()))]\n",
    "                                amp_val = (amp_unchanged_left_env|amp_unchanged_right_env|amp_changed_cols).contract()\n",
    "                                \n",
    "            if amp_val==0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "            \n",
    "            if self.debug:\n",
    "                print(f'Reused Amp val: {amp_val}, Exact Amp val: {self.get_amp_tn(x_i).contract()}')\n",
    "            \n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f2eb416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 1, 3, 0, 3, 2, 0, 1, 1, 2, 0, 0, 3, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "random_x = torch.tensor(H.hilbert.random_state(1))\n",
    "random_x1 = random_x.clone()\n",
    "print(random_x)\n",
    "random_x1[1] = 2\n",
    "random_x1[5] = 1\n",
    "model = fTNModel_reuse(peps, max_bond=chi, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35f0a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 5], [0, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_sites = model.detect_changed_sites(random_x, random_x1)\n",
    "changed_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0617047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fTNModel_reuse(peps, max_bond=chi, dtype=dtype, functional=False)\n",
    "model.cache_env_mode = True\n",
    "model(random_x)\n",
    "model.cache_env_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8992aab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.002435922622680664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(37485.4894, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "amp0 = model.get_amp_tn(random_x1)\n",
    "t1 = time.time()\n",
    "print(\"Time taken:\", t1 - t0)\n",
    "amp0.contract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b009c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.006429195404052734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(37485.4894, dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "amp1 = model.get_amp_tn(random_x1, reconstruct=True)\n",
    "t1 = time.time()\n",
    "print(\"Time taken:\", t1 - t0)\n",
    "amp1.contract()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
