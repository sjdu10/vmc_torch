{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 6, 6])\n",
      "torch.Size([1, 20, 4, 4])\n",
      "1820\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "H, W = 6, 6\n",
    "C_in = 10\n",
    "C_out = 20\n",
    "kernel_size = 3\n",
    "\n",
    "# initial falttened vector\n",
    "x = torch.randn(1, C_in*H*W)\n",
    "\n",
    "# input tensor\n",
    "x = x.view(-1, C_in, H, W)\n",
    "print(x.size())\n",
    "\n",
    "# Conv2d\n",
    "conv = nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=1, padding=0, bias=True)\n",
    "y = conv(x)\n",
    "\n",
    "# Non-linearity\n",
    "relu = nn.ReLU()\n",
    "y = relu(y)\n",
    "print(y.size())\n",
    "\n",
    "# num of parameters in Conv2d\n",
    "num_params = sum(p.numel() for p in conv.parameters())\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels_map, kernel_size, stride=1, padding=0, nn_hidden_dim=4):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          in_channels: number of input channels.\n",
    "          out_channels_map: 2D torch.Tensor of shape (H_out, W_out) containing, at each (i,j), \n",
    "                            the desired number of output channels.\n",
    "          kernel_size: int or tuple, the size of the kernel.\n",
    "          stride, padding: convolution parameters.\n",
    "        \"\"\"\n",
    "        super(CustomConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Save the desired output dimensions per spatial location.\n",
    "        # Here we assume out_channels_map is a tensor of shape (H_out, W_out) with integer entries.\n",
    "        self.out_channels_map = out_channels_map\n",
    "        H_out, W_out = out_channels_map.shape\n",
    "        self.H_out = H_out\n",
    "        self.W_out = W_out\n",
    "        \n",
    "        # For each output pixel (i, j), create a unique weight and bias.\n",
    "        # Weight for pixel (i,j) is of shape: (d_{ij}, in_channels * kernel_height * kernel_width)\n",
    "        in_dim = in_channels * self.kernel_size[0] * self.kernel_size[1]\n",
    "        self.weights = nn.ParameterDict()\n",
    "        self.biases = nn.ParameterDict()\n",
    "        self.out_weights = nn.ParameterDict()\n",
    "        self.out_biases = nn.ParameterDict()\n",
    "        for i in range(H_out):\n",
    "            for j in range(W_out):\n",
    "                out_channels = int(out_channels_map[i, j])\n",
    "                key = f\"{i}_{j}\"\n",
    "                # Initialize weights (you can choose a different initialization if desired)\n",
    "                self.weights[key] = nn.Parameter(torch.randn(nn_hidden_dim, in_dim))\n",
    "                self.biases[key] = nn.Parameter(torch.randn(nn_hidden_dim))\n",
    "                self.out_weights[key] = nn.Parameter(torch.randn(out_channels, nn_hidden_dim))\n",
    "                self.out_biases[key] = nn.Parameter(torch.randn(out_channels))\n",
    "        \n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (B, in_channels, H, W).\n",
    "        \n",
    "        Returns:\n",
    "          A nested list outputs[i][j] for each spatial position (i,j), where outputs[i][j] has shape (B, d_{ij}).\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        # Extract patches: result shape is (B, in_channels * k^2, L), where L = H_out * W_out.\n",
    "        patches = F.unfold(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        \n",
    "        # Reshape patches to (B, L, in_dim)\n",
    "        patches = patches.transpose(1, 2)\n",
    "        L = patches.size(1)\n",
    "        # Ensure that L equals H_out * W_out (this is true if the parameters are consistent)\n",
    "        assert L == self.H_out * self.W_out, \"Output spatial dimensions do not match out_channels_map shape.\"\n",
    "        \n",
    "        outputs = []\n",
    "        idx = 0  # index over the flattened spatial locations\n",
    "        for i in range(self.H_out):\n",
    "            row_outputs = []\n",
    "            for j in range(self.W_out):\n",
    "                # patch: shape (B, in_dim) for the current spatial location.\n",
    "                patch = patches[:, idx, :]  \n",
    "                key = f\"{i}_{j}\"\n",
    "                weight = self.weights[key]  # shape: (nn_hidden_dim, in_dim)\n",
    "                bias = self.biases[key]     # shape: (nn_hidden_dim)\n",
    "                out_weight = self.out_weights[key]\n",
    "                out_bias = self.out_biases[key]\n",
    "                # Compute the linear mapping: result shape (B, d_{ij})\n",
    "                out_pixel = F.linear(patch, weight, bias)\n",
    "                out_pixel = F.relu(out_pixel)\n",
    "                out_pixel = F.linear(out_pixel, out_weight, out_bias)\n",
    "                row_outputs.append(out_pixel)\n",
    "                idx += 1\n",
    "            outputs.append(row_outputs)\n",
    "        # outputs is a list of lists such that outputs[i][j] is a tensor of shape (B, d_{ij})\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n",
      "torch.Size([1, 6])\n",
      "tensor([[6, 6, 4, 6, 5, 7, 5, 7],\n",
      "        [4, 4, 4, 7, 4, 6, 4, 5],\n",
      "        [6, 4, 5, 6, 5, 7, 4, 4],\n",
      "        [6, 4, 7, 4, 6, 7, 4, 6],\n",
      "        [4, 5, 6, 5, 5, 5, 6, 6],\n",
      "        [7, 6, 4, 5, 6, 5, 7, 5],\n",
      "        [4, 4, 5, 5, 6, 4, 5, 7],\n",
      "        [5, 4, 7, 7, 6, 7, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# Define input tensor of shape (B, in_channels, H, W)\n",
    "B, in_channels, H, W = 1, 4, 8, 8\n",
    "x = torch.randn(B, in_channels, H, W)\n",
    "\n",
    "# Suppose after applying a kernel of size 3 with stride 1 and padding 1,\n",
    "# the output spatial dimensions remain 8x8.\n",
    "# Letâ€™s define a custom output channel map where the number of channels varies with position.\n",
    "out_channels_map = torch.randint(4, 8, (8, 8))  # each pixel gets between 4 and 7 channels\n",
    "\n",
    "# Create the custom convolution layer.\n",
    "custom_conv = CustomConv2d(in_channels, out_channels_map, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "# Forward pass.\n",
    "outputs = custom_conv(x)\n",
    "\n",
    "# Check the output shapes.\n",
    "print(outputs[0][0].shape)\n",
    "print(outputs[0][1].shape)\n",
    "print(out_channels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.experiment.tn_model import *\n",
    "class fTN_backflow_attn_Tensorwise_Model_v1(wavefunctionModel):\n",
    "    \"\"\"\n",
    "        For each on-site fermionic tensor with specific shape, assign a narrow on-site projector MLP with corresponding output dimension.\n",
    "        This is to avoid the large number of parameters in the previous model, where Np = N_neurons * N_TNS.\n",
    "    \"\"\"\n",
    "    ...\n",
    "    def __init__(self, ftn, max_bond=None, embedding_dim=32, attention_heads=4, nn_final_dim=4, nn_eta=1.0, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        \n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Define the neural network\n",
    "        input_dim = ftn.Lx * ftn.Ly\n",
    "        phys_dim = ftn.phys_dim()\n",
    "        \n",
    "        self.nn = SelfAttn_block(\n",
    "            n_site=input_dim,\n",
    "            num_classes=phys_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            attention_heads=attention_heads,\n",
    "            dtype=self.param_dtype\n",
    "        )\n",
    "        # for each tensor (labelled by tid), assign a MLP\n",
    "        self.mlp = nn.ModuleDict()\n",
    "        for tid in self.torch_tn_params.keys():\n",
    "            mlp_input_dim = ftn.Lx * ftn.Ly * embedding_dim\n",
    "            tn_params_dict = {\n",
    "                tid: params[int(tid)]\n",
    "            }\n",
    "            tn_params_vec = flatten_tn_params(tn_params_dict)\n",
    "            self.mlp[tid] = nn.Sequential(\n",
    "                nn.Linear(mlp_input_dim, nn_final_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(nn_final_dim, tn_params_vec.numel()),\n",
    "            )\n",
    "            self.mlp[tid].to(self.param_dtype)\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS_backflow_attn_Tensorwise':\n",
    "            {\n",
    "                'D': ftn.max_bond(), \n",
    "                'Lx': ftn.Lx, 'Ly': ftn.Ly, \n",
    "                'symmetry': self.symmetry, \n",
    "                # 'nn_hidden_dim': nn_hidden_dim, \n",
    "                'nn_final_dim': nn_final_dim,\n",
    "                'nn_eta': nn_eta, \n",
    "                'embedding_dim': embedding_dim,\n",
    "                'attention_heads': attention_heads,\n",
    "                'max_bond': max_bond,\n",
    "            },\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        self.nn_eta = nn_eta\n",
    "        self.tree = None\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        params_vec = flatten_tn_params(params)\n",
    "\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Check x_i type\n",
    "            if not type(x_i) == torch.Tensor:\n",
    "                x_i = torch.tensor(x_i, dtype=self.param_dtype)\n",
    "            else:\n",
    "                if x_i.dtype != self.param_dtype:\n",
    "                    x_i = x_i.to(self.param_dtype)\n",
    "        \n",
    "            # Get the NN correction to the parameters, concatenate the results for each tensor\n",
    "            nn_features = self.nn(x_i)\n",
    "            nn_features_vec = nn_features.view(-1)\n",
    "            nn_correction = torch.cat([self.mlp[tid](nn_features_vec) for tid in self.torch_tn_params.keys()])\n",
    "            # Add the correction to the original parameters\n",
    "            tn_nn_params = reconstruct_proj_params(params_vec + self.nn_eta*nn_correction, params)\n",
    "            # Reconstruct the TN with the new parameters\n",
    "            psi = qtn.unpack(tn_nn_params, self.skeleton)\n",
    "            # Get the amplitude\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "                if self.tree is None:\n",
    "                    opt = ctg.ReusableHyperOptimizer()\n",
    "                    self.tree = amp.contraction_tree(optimize=opt)\n",
    "                amp_val = amp.contract(optimize=self.tree)\n",
    "            else:\n",
    "                amp = amp.contract_boundary_from_ymin(max_bond=self.max_bond, cutoff=0.0, yrange=[0, psi.Ly//2-1])\n",
    "                amp = amp.contract_boundary_from_ymax(max_bond=self.max_bond, cutoff=0.0, yrange=[psi.Ly//2, psi.Ly-1])\n",
    "                amp_val = amp.contract()\n",
    "                \n",
    "            if amp_val==0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "class fTN_backflow_attn_conv_Model(wavefunctionModel):\n",
    "    def __init__(self, ftn, max_bond=None, embedding_dim=32, attention_heads=4, nn_final_dim=128, nn_eta=1e-3, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.param_dtype = dtype\n",
    "        \n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter as a part of a ModuleDict\n",
    "        self.torch_tn_params = nn.ModuleDict({\n",
    "            str(tid): nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        })\n",
    "\n",
    "        # Define the neural network\n",
    "        input_dim = ftn.Lx * ftn.Ly\n",
    "        self.Lx = ftn.Lx\n",
    "        self.Ly = ftn.Ly\n",
    "        phys_dim = ftn.phys_dim()\n",
    "        \n",
    "        self.attn_block = SelfAttn_block(\n",
    "            n_site=input_dim,\n",
    "            num_classes=phys_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            attention_heads=attention_heads,\n",
    "            dtype=self.param_dtype\n",
    "        )\n",
    "        \n",
    "        self.ts_dim_list = []\n",
    "        for tid in self.torch_tn_params.keys():\n",
    "            input_dim = ftn.Lx * ftn.Ly\n",
    "            phys_dim = ftn.phys_dim()\n",
    "            ts_params_dict ={\n",
    "                tid: params[int(tid)]\n",
    "            }\n",
    "            ts_params_vec = flatten_tn_params(ts_params_dict)\n",
    "            self.ts_dim_list.append(ts_params_vec.numel())\n",
    "        self.out_channels_map = torch.tensor(self.ts_dim_list).view(ftn.Lx, ftn.Ly)\n",
    "\n",
    "        # define the convolutional layer\n",
    "        self.conv = CustomConv2d(embedding_dim, self.out_channels_map, kernel_size=5, stride=1, padding=2, nn_hidden_dim=nn_final_dim)\n",
    "        self.conv.to(self.param_dtype)\n",
    "\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "        # Store the shapes of the parameters\n",
    "        self.param_shapes = [param.shape for param in self.parameters()]\n",
    "\n",
    "        self.model_structure = {\n",
    "            'fPEPS_backflow_attn':{'D': ftn.max_bond(), 'Lx': ftn.Lx, 'Ly': ftn.Ly, 'symmetry': self.symmetry, 'nn_final_dim': nn_final_dim, 'nn_eta': nn_eta, 'max_bond': max_bond, 'embedding_dim': embedding_dim, 'attention_heads': attention_heads},\n",
    "        }\n",
    "        if max_bond is None or max_bond <= 0:\n",
    "            max_bond = None\n",
    "        self.max_bond = max_bond\n",
    "        self.nn_eta = nn_eta\n",
    "        self.tree = None\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            int(tid): {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_tn_params.items()\n",
    "        }\n",
    "        params_vec = flatten_tn_params(params)\n",
    "\n",
    "        # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            # Check x_i type\n",
    "            if not type(x_i) == torch.Tensor:\n",
    "                x_i = torch.tensor(x_i, dtype=self.param_dtype)\n",
    "            else:\n",
    "                if x_i.dtype != self.param_dtype:\n",
    "                    x_i = x_i.to(self.param_dtype)\n",
    "            # Get the NN correction to the parameters\n",
    "            nn_features = self.attn_block(x_i)\n",
    "            nn_features = nn_features.view(1, self.Lx, self.Ly, -1).permute(0, 3, 1, 2)\n",
    "            nn_correction = self.conv(nn_features)\n",
    "            # print(self.ts_dim_list)\n",
    "            # print([len(nn_correction[i][j][0]) for i in range(self.Lx) for j in range(self.Ly)])\n",
    "            nn_correction = torch.cat([nn_correction[i][j][0] for i in range(self.Lx) for j in range(self.Ly)], dim=0)\n",
    "\n",
    "            # Add the correction to the original parameters\n",
    "            tn_nn_params = reconstruct_proj_params(params_vec + self.nn_eta*nn_correction, params)\n",
    "            # Reconstruct the TN with the new parameters\n",
    "            psi = qtn.unpack(tn_nn_params, self.skeleton)\n",
    "            # Get the amplitude\n",
    "            amp = psi.get_amp(x_i, conj=True)\n",
    "\n",
    "            if self.max_bond is None:\n",
    "                amp = amp\n",
    "                if self.tree is None:\n",
    "                    opt = ctg.ReusableHyperOptimizer()\n",
    "                    self.tree = amp.contraction_tree(optimize=opt)\n",
    "                amp_val = amp.contract(optimize=self.tree)\n",
    "            else:\n",
    "                amp = amp.contract_boundary_from_ymin(max_bond=self.max_bond, cutoff=0.0, yrange=[0, psi.Ly//2-1])\n",
    "                amp = amp.contract_boundary_from_ymax(max_bond=self.max_bond, cutoff=0.0, yrange=[psi.Ly//2, psi.Ly-1])\n",
    "                amp_val = amp.contract()\n",
    "\n",
    "            if amp_val==0.0:\n",
    "                amp_val = torch.tensor(0.0)\n",
    "\n",
    "            batch_amps.append(amp_val)\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmc_torch.fermion_utils import generate_random_fpeps\n",
    "from vmc_torch.hamiltonian import spinful_Fermi_Hubbard_square_lattice\n",
    "import jax\n",
    "\n",
    "Lx = Ly = 6\n",
    "D = 4\n",
    "symmetry = 'Z2'\n",
    "N_f = int(Lx*Ly)\n",
    "chi = -1\n",
    "dtype=torch.float64\n",
    "peps = generate_random_fpeps(Lx, Ly, D=D, seed=2, symmetry=symmetry, Nf=N_f, spinless=False)[0]\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "Ham = spinful_Fermi_Hubbard_square_lattice(Lx, Ly, 1, 8, N_f, pbc=False, n_fermions_per_spin=(N_f//2, N_f//2))\n",
    "random_config = Ham.hilbert.random_state(key=jax.random.PRNGKey(2))\n",
    "random_config = torch.tensor(random_config, dtype=dtype)\n",
    "model = fTN_backflow_attn_conv_Model(peps, max_bond=chi, embedding_dim=16, attention_heads=4, nn_final_dim=4, nn_eta=1e-3, dtype=dtype)\n",
    "model1 = fTN_backflow_attn_Tensorwise_Model_v1(peps, max_bond=chi, embedding_dim=16, attention_heads=4, nn_final_dim=4, nn_eta=1.0, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121120, 146464)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(random_config), model1(random_config)\n",
    "model.num_params, model1.num_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmc_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
