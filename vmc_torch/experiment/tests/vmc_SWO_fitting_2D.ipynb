{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10587/4183139505.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  peps2.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
    "from mpi4py import MPI\n",
    "import pickle\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "# quimb\n",
    "import quimb.tensor as qtn\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "from vmc_torch.experiment.tn_model import fTNModel, fTN_backflow_attn_Tensorwise_Model_v1\n",
    "from vmc_torch.experiment.tn_model import init_weights_to_zero\n",
    "from vmc_torch.sampler import MetropolisExchangeSamplerSpinful\n",
    "from vmc_torch.variational_state import Variational_State\n",
    "from vmc_torch.optimizer import SGD, TrivialPreconditioner, SGD_momentum, DecayScheduler, Adam\n",
    "from vmc_torch.VMC import VMC\n",
    "from vmc_torch.hamiltonian_torch import spinful_Fermi_Hubbard_square_lattice_torch\n",
    "from vmc_torch.torch_utils import SVD,QR\n",
    "\n",
    "# Register safe SVD and QR functions to torch\n",
    "ar.register_function('torch','linalg.svd',SVD.apply)\n",
    "ar.register_function('torch','linalg.qr',QR.apply)\n",
    "\n",
    "from vmc_torch.global_var import DEBUG\n",
    "from vmc_torch.utils import closest_divisible\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "# Hamiltonian parameters\n",
    "Lx = int(4)\n",
    "Ly = int(2)\n",
    "symmetry = 'Z2'\n",
    "t = 1.0\n",
    "U = 8.0\n",
    "N_f = int(Lx*Ly-2)\n",
    "# N_f = int(Lx*Ly)\n",
    "n_fermions_per_spin = (N_f//2, N_f//2)\n",
    "H = spinful_Fermi_Hubbard_square_lattice_torch(Lx, Ly, t, U, N_f, pbc=False, n_fermions_per_spin=n_fermions_per_spin)\n",
    "graph = H.graph\n",
    "\n",
    "# TN parameters\n",
    "D1 = 4\n",
    "D2 = 8\n",
    "chi1 = -1\n",
    "chi2 = -1\n",
    "dtype=torch.float64\n",
    "\n",
    "pwd = '/home/sijingdu/TNVMC/VMC_code/vmc_torch/data'\n",
    "# Load peps1\n",
    "skeleton = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D1}/peps_skeleton.pkl\", \"rb\"))\n",
    "peps_params = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D1}/peps_su_params.pkl\", \"rb\"))\n",
    "peps1 = qtn.unpack(peps_params, skeleton)\n",
    "peps1.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "# Load peps2\n",
    "skeleton = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D2}/peps_skeleton_U1.pkl\", \"rb\"))\n",
    "peps_params = pickle.load(open(pwd+f\"/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D2}/peps_su_params_U1.pkl\", \"rb\"))\n",
    "peps2 = qtn.unpack(peps_params, skeleton)\n",
    "peps2.apply_to_arrays(lambda x: torch.tensor(x, dtype=dtype))\n",
    "\n",
    "model_names = {\n",
    "    fTNModel: 'fTN',\n",
    "    fTN_backflow_attn_Tensorwise_Model_v1: 'fTN_backflow_attn_Tensorwise_v1',\n",
    "}\n",
    "\n",
    "# Learning model\n",
    "model1 = fTN_backflow_attn_Tensorwise_Model_v1(\n",
    "    peps1,\n",
    "    max_bond=chi1,\n",
    "    embedding_dim=16,\n",
    "    attention_heads=4,\n",
    "    nn_final_dim=4,\n",
    "    nn_eta=1.0,\n",
    "    dtype=dtype,\n",
    ")\n",
    "# model1 = fTNModel(peps1, max_bond=chi1, dtype=dtype)\n",
    "init_std = 5e-2\n",
    "model1.apply(lambda x: init_weights_to_zero(x, std=init_std))\n",
    "# model1 = fpepsModel(peps1, dtype=dtype)\n",
    "model_name = model_names.get(type(model1), 'UnknownModel')\n",
    "\n",
    "# Set learning steps\n",
    "init_step = 0\n",
    "final_step = 20\n",
    "total_steps = final_step - init_step\n",
    "\n",
    "# Target model\n",
    "model2 = fTNModel(peps2, max_bond=chi2, dtype=dtype)\n",
    "target_model_name = model_names.get(type(model2), 'UnknownModel')\n",
    "target_step = 399\n",
    "\n",
    "# SWO sample size\n",
    "N_samples = int(1e4)\n",
    "N_samples = closest_divisible(N_samples, SIZE)\n",
    "if (N_samples/SIZE)%2 != 0:\n",
    "    N_samples += SIZE\n",
    "\n",
    "# Load learning model parameters\n",
    "optimizer_state = None\n",
    "if init_step != 0:\n",
    "    saved_model_params = torch.load(pwd+f'/SWO_fit/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/target_{target_model_name}_D={D2}_chi={chi2}/{model_name}/D={D1}/chi={chi1}/model_params_step{init_step}.pth', weights_only=False)\n",
    "    saved_model_state_dict = saved_model_params['model_state_dict']\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params['model_params_vec'])\n",
    "    try:\n",
    "        model1.load_state_dict(saved_model_state_dict)\n",
    "    except:\n",
    "        model1.load_params(saved_model_params_vec)\n",
    "    optimizer_state = saved_model_params.get('optimizer_state', None)\n",
    "\n",
    "# Load target model parameters\n",
    "if target_step != 0:\n",
    "    saved_model_params = torch.load(pwd+f'/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/D={D2}/{target_model_name}/chi={chi2}/model_params_step{target_step}.pth', weights_only=False)\n",
    "    saved_model_state_dict = saved_model_params['model_state_dict']\n",
    "    saved_model_params_vec = torch.tensor(saved_model_params['model_params_vec'])\n",
    "    try:\n",
    "        model2.load_state_dict(saved_model_state_dict)\n",
    "    except:\n",
    "        model2.load_params(saved_model_params_vec)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "learning_rate = 5e-2\n",
    "scheduler = DecayScheduler(init_lr=learning_rate, decay_rate=0.9, patience=50, min_lr=1e-3)\n",
    "use_prev_opt = True\n",
    "if optimizer_state is not None and use_prev_opt:\n",
    "    optimizer_name = optimizer_state['optimizer']\n",
    "    if optimizer_name == 'SGD_momentum':\n",
    "        optimizer = SGD_momentum(learning_rate=learning_rate, momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate, weight_decay=1e-5)\n",
    "    print('Loading optimizer: ', optimizer)\n",
    "    optimizer.lr = learning_rate\n",
    "    if isinstance(optimizer, SGD_momentum):\n",
    "        optimizer.velocity = optimizer_state['velocity']\n",
    "    if isinstance(optimizer, Adam):\n",
    "        optimizer.m = optimizer_state['m']\n",
    "        optimizer.v = optimizer_state['v']\n",
    "        optimizer.t = optimizer_state['t']\n",
    "else:\n",
    "    # optimizer = SignedSGD(learning_rate=learning_rate)\n",
    "    # optimizer = SignedRandomSGD(learning_rate=learning_rate)\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    # optimizer = SGD_momentum(learning_rate=learning_rate, momentum=0.9)\n",
    "    # optimizer = Adam(learning_rate=learning_rate, t_step=init_step, weight_decay=0.0)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "# Set up sampler\n",
    "sampler = MetropolisExchangeSamplerSpinful(H.hilbert, graph, N_samples=N_samples, burn_in_steps=20, reset_chain=True, random_edge=False, equal_partition=True, dtype=dtype, device=device)\n",
    "# Set up variational state\n",
    "variational_state = Variational_State(model2, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "target_state = Variational_State(model2, hi=H.hilbert, sampler=sampler, dtype=dtype)\n",
    "# Set up preconditioner (Trivial for SWO)\n",
    "preconditioner = TrivialPreconditioner()\n",
    "# Set up VMC\n",
    "vmc = VMC(hamiltonian=H, variational_state=variational_state, optimizer=optimizer, preconditioner=preconditioner, scheduler=scheduler, SWO=True, beta=0.1)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     os.makedirs(pwd+f'/SWO_fit/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/target_{target_model_name}_D={D2}_chi={chi2}/{model_name}/D={D1}/chi={chi1}/', exist_ok=True)\n",
    "#     vmc.run_SWO_state_fitting(target_state=target_state, sample_times=total_steps, compute_energy=True, SWO_max_iter=100, log_fidelity_tol=0.0, tmpdir=pwd+f'/SWO_fit/{Lx}x{Ly}/t={t}_U={U}/N={N_f}/{symmetry}/target_{target_model_name}_D={D2}_chi={chi2}/{model_name}/D={D1}/chi={chi1}/')\n",
    "\n",
    "model1 = model1.to(device)\n",
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?it/s]/home/sijingdu/TNVMC/VMC_code/mpsds/mpsds/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "F=4.24 C=5.00 S=9.00 P=11.43: 100%|██████████| 10/10 [00:00<00:00, 368.58it/s]\n",
      "F=3.07 C=4.13 S=6.00 P=8.52: 100%|██████████| 10/10 [00:00<00:00, 1010.46it/s]\n",
      "Training: 100%|██████████| 500/500 [03:08<00:00,  2.66it/s, loss=2.472e-06]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import trange\n",
    "\n",
    "# assume model1, sampler and variational_state are already defined\n",
    "# e.g. model1 = MyModel(); sampler = MySampler(); variational_state = ...\n",
    "\n",
    "# 1. loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=1e-3)\n",
    "loss_history = []\n",
    "log_dir = \"runs/exp4\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# 2. training loop\n",
    "num_epochs = 500\n",
    "pbar = trange(1, num_epochs + 1, desc=\"Training\")\n",
    "for epoch in pbar:\n",
    "    # sample a batch of 10 configurations and targets\n",
    "    with torch.no_grad():\n",
    "        X, Y_target = sampler.sample_configs(variational_state, chain_length=50)\n",
    "    # # ensure tensors are on correct device\n",
    "    # X = X.to(device)\n",
    "    # Y_target = Y_target.to(device)\n",
    "    \n",
    "    # forward pass\n",
    "    Y_pred = model1(X)           # shape: [10]\n",
    "    loss = criterion(Y_pred, Y_target)\n",
    "    \n",
    "    # backward + step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # update bar with latest loss\n",
    "    pbar.set_postfix({\"loss\": f\"{loss.item():.3e}\"})\n",
    "    loss_history.append(loss.item())\n",
    "    # log scalar\n",
    "    writer.add_scalar(\"train/loss\", loss.item(), epoch)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpsds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
