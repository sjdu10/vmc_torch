{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijingdu/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time,scipy,functools, gc\n",
    "import numpy as np\n",
    "import scipy.sparse.linalg as spla\n",
    "from tfqmr import tfqmr\n",
    "from quimb.utils import progbar as Progbar\n",
    "from mpi4py import MPI\n",
    "import ast\n",
    "\n",
    "import scipy\n",
    "\n",
    "# torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# quimb\n",
    "import quimb as qu\n",
    "import quimb.tensor as qtn\n",
    "import symmray as sr\n",
    "import autoray as ar\n",
    "from autoray import do\n",
    "\n",
    "\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "np.set_printoptions(suppress=True,precision=6,linewidth=100)\n",
    "\n",
    "\n",
    "class fTNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ftn):\n",
    "        super().__init__()\n",
    "        # extract the raw arrays and a skeleton of the TN\n",
    "        params, self.skeleton = qtn.pack(ftn)\n",
    "\n",
    "        # Flatten the dictionary structure and assign each parameter\n",
    "        self.torch_params = {\n",
    "            tid: nn.ParameterDict({\n",
    "                str(sector): nn.Parameter(data)\n",
    "                for sector, data in blk_array.items()\n",
    "            })\n",
    "            for tid, blk_array in params.items()\n",
    "        }\n",
    "\n",
    "        # Get symmetry\n",
    "        self.symmetry = ftn.arrays[0].symmetry\n",
    "\n",
    "    \n",
    "\n",
    "    def product_bra_state(self, config, peps, symmetry='Z2'):\n",
    "        \"\"\"Spinless fermion product bra state.\"\"\"\n",
    "        product_tn = qtn.TensorNetwork()\n",
    "        backend = peps.tensors[0].data.backend\n",
    "        iterable_oddpos = iter(range(2*peps.nsites+1))\n",
    "        for n, site in zip(config, peps.sites):\n",
    "            p_ind = peps.site_ind_id.format(*site)\n",
    "            p_tag = peps.site_tag_id.format(*site)\n",
    "            tid = peps.sites.index(site)\n",
    "            nsites = peps.nsites\n",
    "            # use autoray to ensure the correct backend is used\n",
    "            with ar.backend_like(backend):\n",
    "                if symmetry == 'Z2':\n",
    "                    data = [sr.Z2FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2', charge=0, oddpos=2*tid+1), # It doesn't matter if oddpos is None for even parity tensor.\n",
    "                            sr.Z2FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='Z2',charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "                elif symmetry == 'U1':\n",
    "                    data = [sr.U1FermionicArray.from_blocks(blocks={(0,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=0, oddpos=2*tid+1),\n",
    "                            sr.U1FermionicArray.from_blocks(blocks={(1,):do('array', [1.0,], like=backend)}, duals=(True,),symmetry='U1', charge=1, oddpos=2*tid+1)\n",
    "                        ]\n",
    "            tsr_data = data[int(n)] # BUG: does not fit in jax compilation, a concrete value is needed for traced arrays\n",
    "            tsr = qtn.Tensor(data=tsr_data, inds=(p_ind,),tags=(p_tag, 'bra'))\n",
    "            product_tn |= tsr\n",
    "        return product_tn\n",
    "\n",
    "    def get_amp(self, peps, config, inplace=False, symmetry='Z2', conj=True):\n",
    "        \"\"\"Get the amplitude of a configuration in a PEPS.\"\"\"\n",
    "        if not inplace:\n",
    "            peps = peps.copy()\n",
    "        if conj:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry).conj()\n",
    "        else:\n",
    "            amp = peps|self.product_bra_state(config, peps, symmetry)\n",
    "        for site in peps.sites:\n",
    "            site_tag = peps.site_tag_id.format(*site)\n",
    "            amp.contract_(tags=site_tag)\n",
    "\n",
    "        amp.view_as_(\n",
    "            qtn.PEPS,\n",
    "            site_ind_id=\"k{},{}\",\n",
    "            site_tag_id=\"I{},{}\",\n",
    "            x_tag_id=\"X{}\",\n",
    "            y_tag_id=\"Y{}\",\n",
    "            Lx=peps.Lx,\n",
    "            Ly=peps.Ly,\n",
    "        )\n",
    "        return amp\n",
    "        \n",
    "    def parameters(self):\n",
    "        # Manually yield all parameters from the nested structure\n",
    "        for tid_dict in self.torch_params.values():\n",
    "            for param in tid_dict.values():\n",
    "                yield param\n",
    "    \n",
    "    def from_params_to_vec(self):\n",
    "        return torch.cat([param.data.flatten() for param in self.parameters()])\n",
    "    \n",
    "    @property\n",
    "    def num_params(self):\n",
    "        return len(self.from_params_to_vec())\n",
    "    \n",
    "    def params_grad_to_vec(self):\n",
    "        param_grad_vec = torch.cat([param.grad.flatten() if param.grad is not None else torch.zeros_like(param).flatten() for param in self.parameters()])\n",
    "        return param_grad_vec\n",
    "\n",
    "    def clear_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    def from_vec_to_params(self, vec, quimb_format=False):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {}\n",
    "        idx = 0\n",
    "        for tid, blk_array in self.torch_params.items():\n",
    "            params[tid] = {}\n",
    "            for sector, data in blk_array.items():\n",
    "                shape = data.shape\n",
    "                size = data.numel()\n",
    "                if quimb_format:\n",
    "                    params[tid][ast.literal_eval(sector)] = vec[idx:idx+size].view(shape)\n",
    "                else:\n",
    "                    params[tid][sector] = vec[idx:idx+size].view(shape)\n",
    "                idx += size\n",
    "        return params\n",
    "    \n",
    "    def load_params(self, new_params):\n",
    "        if isinstance(new_params, torch.Tensor):\n",
    "            new_params = self.from_vec_to_params(new_params)\n",
    "        # Update the parameters manually\n",
    "        with torch.no_grad():\n",
    "            for tid, blk_array in new_params.items():\n",
    "                for sector, data in blk_array.items():\n",
    "                    self.torch_params[tid][sector].data = data\n",
    "\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        # Reconstruct the original parameter structure (by unpacking from the flattened dict)\n",
    "        params = {\n",
    "            tid: {\n",
    "                ast.literal_eval(sector): data\n",
    "                for sector, data in blk_array.items()\n",
    "            }\n",
    "            for tid, blk_array in self.torch_params.items()\n",
    "        }\n",
    "        # Reconstruct the TN with the new parameters\n",
    "        psi = qtn.unpack(params, self.skeleton)\n",
    "       # `x` is expected to be batched as (batch_size, input_dim)\n",
    "        # Loop through the batch and compute amplitude for each sample\n",
    "        batch_amps = []\n",
    "        for x_i in x:\n",
    "            amp = self.get_amp(psi, x_i, symmetry=self.symmetry, conj=True)\n",
    "            batch_amps.append(amp.contract())\n",
    "\n",
    "        # Return the batch of amplitudes stacked as a tensor\n",
    "        return torch.stack(batch_amps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 1:\n",
    "            # If input is not batched, add a batch dimension\n",
    "            x = x.unsqueeze(0)\n",
    "        return self.amplitude(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMC:\n",
    "    \"\"\"\n",
    "    NOTE: At current stage, we consider only a 2D hamiltonianiltonian defined on a square lattice as the object function.\n",
    "    1. Perform MC sampling from a parameterized probability distribution\n",
    "    2. Compute the object function and quantities of interest (e.g. energy, gradient, etc.)\n",
    "    3. Optimize the parameters of the probability distribution with respect to the object function using certain optimization algorithms\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hamiltonian,\n",
    "        variational_state,\n",
    "        optimizer,\n",
    "        preconditioner=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the driver class.\n",
    "\n",
    "        Args:\n",
    "            hamiltonian: The Hamiltonian of the system.\n",
    "            optimizer: Determines how optimization steps are performed given the\n",
    "                bare energy gradient.\n",
    "            variational_state: The variational state for which the hamiltonian must\n",
    "                be minimised.\n",
    "            preconditioner: Determines which preconditioner to use for the loss gradient.\n",
    "                This must be a tuple of `(object, solver)` as documented in the section\n",
    "                `preconditioners` in the documentation. The standard preconditioner\n",
    "                included with NetKet is Stochastic Reconfiguration. By default, no\n",
    "                preconditioner is used and the bare gradient is passed to the optimizer.\n",
    "        \"\"\"\n",
    "        self._hamiltonian = hamiltonian # Use NetKet Hamiltonian object!\n",
    "        \n",
    "        self._state = variational_state # A variational state object: a torch function + sampler\n",
    "\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "        self._preconditioner = preconditioner\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def preconditioner(self):\n",
    "        \"\"\"\n",
    "        The preconditioner used to modify the gradient.\n",
    "\n",
    "        This is a function with the following signature\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            precondtioner(vstate: VariationalState,\n",
    "                          grad: PyTree/vector,\n",
    "                          step: Optional[Scalar] = None)\n",
    "\n",
    "        Where the first argument is a variational state, the second argument\n",
    "        is the PyTree/vector of the gradient to precondition and the last optional\n",
    "        argument is the step, used to change some parameters along the\n",
    "        optimisation.\n",
    "        \"\"\"\n",
    "        return self._preconditioner\n",
    "    \n",
    "\n",
    "    def _forward_and_backward(self):\n",
    "        \"\"\"\n",
    "        Performs a number of VMC optimization steps.\n",
    "\n",
    "        Args:\n",
    "            n_steps (int): Number of steps to perform.\n",
    "        \"\"\"\n",
    "\n",
    "        self._state.reset() # Clear out the gradient of the state parameters\n",
    "\n",
    "        # Compute the local energy estimator and average Energy\n",
    "        self._loss_energy, self._loss_grad = self._state.expect_and_grad(self._ham)\n",
    "\n",
    "        # if it's the identity it does\n",
    "        # self._dp = self._loss_grad\n",
    "        self._dp = self.preconditioner(self._state, self._loss_grad, self.step_count)\n",
    "        # self._dp is the preconditioned gradient\n",
    "\n",
    "        return self._dp\n",
    "\n",
    "    @property\n",
    "    def energy(self):\n",
    "        \"\"\"\n",
    "        Aim: Return MCMC statistics for the expectation value of observables in the\n",
    "        current state of the driver.\n",
    "\n",
    "        Now: Return the energy of the current state.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._loss_energy\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"Vmc(\"\n",
    "            + f\"\\n  step_count = {self.step_count},\"\n",
    "            + f\"\\n  state = {self._state})\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def run(self, start, stop, tmpdir=None): # Now naive implementation\n",
    "        \"\"\"Run the VMC optimization loop.\"\"\"\n",
    "        self.Einit = 0.\n",
    "        Energy_stats_list = []\n",
    "        for step in range(start, stop):\n",
    "            # print('Variational step {}'.format(step))\n",
    "            self.step = step\n",
    "            # Compute the average energy and estimated energy gradient, meanwhile also record the amplitude_grad matrix\n",
    "            # Use MPI for the sampling\n",
    "            state_MC_energy, state_MC_energy_grad = self._state.expect_and_grad(self._hamiltonian)\n",
    "            # Only rank 0 collects the energy statistics\n",
    "            if RANK == 0:\n",
    "                Energy_stats_list.append(state_MC_energy)\n",
    "                print('Energy gradient vec: {}'.format(state_MC_energy_grad[:10]))\n",
    "                # Precondition the gradient through SR\n",
    "                preconditioned_grad = self.preconditioner(self._state, state_MC_energy_grad)\n",
    "                # Compute the new parameter vector\n",
    "                new_param_vec = self._optimizer.compute_update_params(self._state.params_vec, preconditioned_grad) # Subroutine: rank 0 computes new parameter vector based on the gradient\n",
    "                # Broadcast the new parameter vector to all ranks\n",
    "                self._state.reset() # Clear out the gradient of the state parameters\n",
    "                print(state_MC_energy)\n",
    "                print('Energy: {}'.format(state_MC_energy['mean']))\n",
    "                new_param_vec = new_param_vec.detach().numpy()\n",
    "            else:\n",
    "                new_param_vec = None\n",
    "\n",
    "            COMM.Bcast(new_param_vec,root=0)\n",
    "            # Update the quantum state with the new parameter vector\n",
    "            self._state.update_state(new_param_vec) # Reload the new parameter vector into the quantum state\n",
    "            \n",
    "        return Energy_stats_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "from mpi4py import MPI\n",
    "COMM = MPI.COMM_WORLD\n",
    "SIZE = COMM.Get_size()\n",
    "RANK = COMM.Get_rank()\n",
    "\n",
    "class Variational_State:\n",
    "\n",
    "    def __init__(self, vstate_func, sampler=None, hi=None):\n",
    "        self.vstate_func = vstate_func\n",
    "        self.sampler = sampler\n",
    "        self.Np = vstate_func.num_params\n",
    "        self.hi = sampler.hi if sampler is not None else hi\n",
    "        self.Ns = sampler.Ns if sampler is not None else self.hi.n_states\n",
    "        self.nsites = self.hi.size\n",
    "        self.amp_grad_matrix = None\n",
    "        assert self.hi is not None, \"Hilbert space must be provided for sampling!\"\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear the gradient of the variational state.\"\"\"\n",
    "        self.vstate_func.clear_grad()\n",
    "\n",
    "    def update_state(self, new_param_vec):\n",
    "        \"\"\"Update the variational state with the new parameter vector.\"\"\"\n",
    "        if not type(new_param_vec) == torch.Tensor:\n",
    "            new_param_vec = torch.tensor(new_param_vec, dtype=torch.float32)\n",
    "        self.vstate_func.load_params(new_param_vec)\n",
    "\n",
    "    @property\n",
    "    def params_vec(self):\n",
    "        return self.vstate_func.from_params_to_vec()\n",
    "    \n",
    "    @property\n",
    "    def params_grad_vec(self):\n",
    "        return self.vstate_func.params_grad_to_vec()\n",
    "    \n",
    "    def amplitude(self, x):\n",
    "        return self.vstate_func(x)\n",
    "    \n",
    "    def amplitude_grad(self, x):\n",
    "        if not type(x) == torch.Tensor:\n",
    "            x = torch.tensor(np.asarray(x), dtype=torch.float32)\n",
    "        amp = self.vstate_func(x)\n",
    "        amp.backward()\n",
    "        vec_grad = self.vstate_func.params_grad_to_vec()\n",
    "        # Clear the gradient\n",
    "        self.reset()\n",
    "        return amp, vec_grad\n",
    "\n",
    "    def full_hi_amp_grad_matrix(self):\n",
    "        \"\"\"Construct the full Np x Ns matrix of amplitude gradients.\"\"\"\n",
    "        parameter_amp_grad = torch.zeros((self.Np, self.hi.n_states), dtype=torch.float32)\n",
    "        all_config = self.hi.all_states()\n",
    "        ampx_arr = torch.zeros((self.hi.n_states,), dtype=torch.float32)\n",
    "\n",
    "        for i, config in enumerate(all_config):\n",
    "            ampx, ampx_dp = self.amplitude_grad(config)\n",
    "            parameter_amp_grad[:, i] = ampx_dp\n",
    "            ampx_arr[i] = ampx\n",
    "\n",
    "        return parameter_amp_grad, ampx_arr\n",
    "    \n",
    "\n",
    "    def get_amp_grad_matrix(self):\n",
    "        \"\"\"Return the amplitude gradient matrix.\"\"\"\n",
    "        if self.amp_grad_matrix is None:\n",
    "            if self.sampler is None and self.hi is not None:\n",
    "                return self.full_hi_amp_grad_matrix()\n",
    "            else:\n",
    "                raise ValueError(\"Sampler must be provided for sampling!\")\n",
    "        else:\n",
    "            # should be computed during sampling\n",
    "            return self.amp_grad_matrix\n",
    "    \n",
    "\n",
    "    def full_hi_expect_and_grad(self, op):\n",
    "        \"\"\"Full Hilbert space expectation value and gradient calculation.\n",
    "        Only for sanity check on small systems.\n",
    "        \"\"\"\n",
    "        hi = op.hilbert # netket hilbert object\n",
    "        N = hi.size\n",
    "        all_config = hi.all_states()\n",
    "        psi = self.vstate_func(all_config)\n",
    "        print(f'Psi: {psi[:10]}')\n",
    "        psi = psi/do('linalg.norm', psi)\n",
    "        \n",
    "        op_dense = torch.tensor(op.to_dense(), dtype=torch.float32)\n",
    "        expect_op = psi.conj().T@(op_dense@psi)\n",
    "\n",
    "        expect_op.backward()\n",
    "        vec_grad = self.vstate_func.params_grad_to_vec()\n",
    "        # Clear the gradient\n",
    "        self.reset()\n",
    "        return {'mean': expect_op.detach().numpy()}, vec_grad\n",
    "        \n",
    "    \n",
    "    def expect_and_grad(self, op, full_hi=False):\n",
    "        \"\"\"\n",
    "        Compute the expectation value of the operator `op` and its gradient.\n",
    "\n",
    "        Args:\n",
    "            op (netket operator object): The operator for which the expectation value and gradient are calculated.\n",
    "            full_hi (bool): Whether to use the full Hilbert space expectation value and gradient calculation.\n",
    "\n",
    "        Returns:\n",
    "            torch.tensor: The expectation value of the operator.\n",
    "            torch.tensor: The gradient of the expectation value with respect to the parameters.\n",
    "        \"\"\"\n",
    "        if full_hi or self.sampler is None:\n",
    "            return self.full_hi_expect_and_grad(op)\n",
    "        \n",
    "        # use MPI for sampling\n",
    "        chain_length = self.Ns//SIZE # Number of samples per rank\n",
    "\n",
    "        op_expect, op_grad, op_var, op_error, config_list, amp_list = self.collect_samples(op, chain_length=chain_length)\n",
    "        \n",
    "        # return statistics of the MC sampling\n",
    "        stats_dict = {'mean': op_expect, 'variance': op_var, 'error': op_error}\n",
    "\n",
    "\n",
    "        return stats_dict, op_grad\n",
    "\n",
    "    \n",
    "    def collect_samples(self, op, chain_length=1):\n",
    "\n",
    "        vstate = self\n",
    "\n",
    "        print('RANK{}, sample size: {}, chain length: {}'.format(RANK, self.Ns, chain_length))\n",
    "        \n",
    "        # Sample on each rank\n",
    "        # this should be a list of samples, where each sample is a tuple of (config, E_loc, amp, amp_grad)\n",
    "        local_samples = self.sampler.sample(op, vstate, chain_length=chain_length)\n",
    "\n",
    "        # Gather all samples to rank 0\n",
    "        all_samples = COMM.gather(local_samples, root=0)\n",
    "        # reset sampler\n",
    "        self.sampler.reset()\n",
    "        self.amp_grad_matrix = None\n",
    "\n",
    "        if RANK == 0:\n",
    "            # Join all samples list from all ranks into a single list\n",
    "            # each sample is a tuple of (config, E_loc, amp, amp_grad)\n",
    "            all_samples = [sample for sublist in all_samples for sample in sublist]\n",
    "\n",
    "            op_loc = [sample[1] for sample in all_samples]\n",
    "            amp_grad = [sample[3] for sample in all_samples]\n",
    "\n",
    "            self.amp_grad_matrix = np.asarray(amp_grad).T\n",
    "\n",
    "            # Compute the expectation value and gradient\n",
    "            op_expect = np.mean(op_loc)\n",
    "            mean_amp_grad = np.mean(amp_grad, axis=0)\n",
    "            op_grad = np.mean([sample[1]*sample[3] for sample in all_samples], axis=0) - op_expect*mean_amp_grad\n",
    "\n",
    "            config_list = [sample[0] for sample in all_samples]\n",
    "            amp_list = [sample[2] for sample in all_samples]\n",
    "            op_var = np.var(op_loc)\n",
    "            op_error = np.sqrt(op_var/len(all_samples))\n",
    "\n",
    "            return op_expect, op_grad, op_var, op_error, config_list, amp_list\n",
    "        \n",
    "        else:\n",
    "            return None, None, None, None, None, None\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "class Sampler:\n",
    "    def __init__(self, hi, graph, N_samples=2**8, burn_in_steps=100):\n",
    "        self.hi = hi\n",
    "        self.Ns = N_samples\n",
    "        self.graph = graph\n",
    "        self.burn_in_steps = burn_in_steps\n",
    "        self.initial_config = torch.tensor(np.asarray(random.choice(hi.all_states())), dtype=torch.float32)\n",
    "        self.current_config = self.initial_config.clone()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.initial_config = torch.tensor(np.asarray(random.choice(self.hi.all_states())), dtype=torch.float32)\n",
    "        self.current_config = self.initial_config.clone()\n",
    "\n",
    "    def _sample_next(self, vstate_func):\n",
    "        \"\"\"Sample the next configuration.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def sample(self, op, vstate, chain_length=1):\n",
    "        \"\"\"Sample the local energy and amplitude gradient for each configuration.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class MetropolisExchangeSampler(Sampler):\n",
    "    def __init__(self, hi, graph, N_samples=2**8, burn_in_steps=100):\n",
    "        super().__init__(hi, graph, N_samples, burn_in_steps)\n",
    "    \n",
    "    def burn_in(self, vstate):\n",
    "        \"\"\"Discard the initial samples. (Burn-in)\"\"\"\n",
    "        for _ in range(self.burn_in_steps):\n",
    "            self._sample_next(vstate)\n",
    "    \n",
    "    def _sample_next(self, vstate):\n",
    "        \"\"\"Sample the next configuration. Change the current configuration in place.\"\"\"\n",
    "        current_prob = abs(vstate.amplitude(self.current_config))**2\n",
    "        proposed_config = self.current_config.clone()\n",
    "        attempts = 0\n",
    "        accepts = 0\n",
    "        for (i, j) in self.graph.edges(): # We always loop over all edges.\n",
    "            if self.current_config[i] == self.current_config[j]:\n",
    "                continue\n",
    "            attempts += 1\n",
    "            proposed_config = self.current_config.clone()\n",
    "            # swap the configuration on site i and j\n",
    "            temp = proposed_config[i].item()\n",
    "            proposed_config[i] = proposed_config[j]\n",
    "            proposed_config[j] = temp\n",
    "            proposed_prob = abs(vstate.amplitude(proposed_config))**2\n",
    "\n",
    "            try:\n",
    "                acceptance_ratio = proposed_prob/current_prob\n",
    "            except ZeroDivisionError:\n",
    "                acceptance_ratio = 1 if proposed_prob > 0 else 0\n",
    "\n",
    "            if random.random() < acceptance_ratio:\n",
    "                self.current_config = proposed_config\n",
    "                current_prob = proposed_prob\n",
    "                accepts += 1\n",
    "        # print('Acceptance rate: {}'.format(accepts/attempts))\n",
    "            \n",
    "        return self.current_config\n",
    "    \n",
    "    def sample(self, op, vstate, chain_length=1):\n",
    "        \"\"\"Sample the local energy and amplitude gradient for each configuration.\"\"\"\n",
    "        self.burn_in(vstate)\n",
    "        samples = []\n",
    "        for _ in range(chain_length):\n",
    "            sigma = self._sample_next(vstate)\n",
    "            # compute local energy and amplitude gradient\n",
    "            eta, O_etasigma = op.get_conn(sigma) # Non-zero matrix elements and corresponding configurations\n",
    "            psi_sigma, psi_sigma_grad = vstate.amplitude_grad(sigma)\n",
    "            psi_eta = vstate.amplitude(eta)\n",
    "\n",
    "            psi_sigma = psi_sigma.detach().numpy()\n",
    "            psi_eta = psi_eta.detach().numpy()\n",
    "            psi_sigma_grad = psi_sigma_grad.detach().numpy()\n",
    "\n",
    "            O_loc = np.sum(O_etasigma * (psi_eta / psi_sigma), axis=-1)\n",
    "            samples.append((sigma.detach().numpy(), O_loc, psi_sigma, psi_sigma_grad))\n",
    "\n",
    "        return samples\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "class Preconditioner:\n",
    "    def __call__(self, state, grad):\n",
    "        \"\"\"Abstract method for preconditioning the gradient.\"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "\n",
    "class TrivialPreconditioner(Preconditioner):\n",
    "    \"\"\"Trivial preconditioner that does nothing.\"\"\"\n",
    "    def __call__(self, state, grad):\n",
    "        return grad\n",
    "\n",
    "class SR(Preconditioner):\n",
    "    \"\"\"\n",
    "    Math: S*dp = g, where S is the QGT and g is the energy gradient. dp is the preconditioned gradient.\n",
    "    g = <E_loc(x)*O(x)> - <E_loc(x)>*<O(x)>, where O(x) = \\nabla_{\\theta} log(psi(x;\\theta)) is the gradient of the log amplitude.\n",
    "    O(x) has shape of (Np,), where Np is the number of parameters.\n",
    "\n",
    "    S = <O^\\dagger(x)*O(x)> - <O^\\dagger(x)>*<O(x)>, which has shape of (Np, Np).\n",
    "    S is a positive definite matrix.\n",
    "    S can be computed from the amp_grad matrix, which has shape of (Np, Ns), where Ns is the number of samples.\n",
    "    S is just the covariance matrix of the amp_grad vectors.\n",
    "\n",
    "    In practice, one does not need to compute the dense S matrix to solve for dp.\n",
    "    One can solve the linear equation S*dp = g iteratively using scipy.sparse.linalg.\n",
    "    \"\"\"\n",
    "    def __init__(self, dense=False, iter_step=None, exact=False):\n",
    "        self.dense = dense\n",
    "        self.iter_step = iter_step\n",
    "        self.exact = exact\n",
    "    def __call__(self, state, energy_grad, eta=1e-3):\n",
    "        \"\"\"iter_step is for iterative solvers.\"\"\"\n",
    "        if self.exact:\n",
    "            parameter_amp_grad, amp_arr = state.get_amp_grad_matrix()\n",
    "            parameter_amp_grad = parameter_amp_grad.detach().numpy()\n",
    "            amp_arr = amp_arr.detach().numpy()\n",
    "            norm_sqr = np.linalg.norm(amp_arr)**2\n",
    "            S = np.sum([np.outer(amp_grad, amp_grad.conj()) for amp_grad in parameter_amp_grad.T], axis=0)/norm_sqr\n",
    "            weighted_amp_grad = np.sum([amp_arr[i]*parameter_amp_grad[:, i] for i in range(amp_arr.shape[0])], axis=0)/norm_sqr\n",
    "            S -= np.outer(weighted_amp_grad, weighted_amp_grad.conj())\n",
    "            R = S + eta*np.eye(S.shape[0])\n",
    "            dp = scipy.linalg.solve(R, energy_grad.detach().numpy())\n",
    "            return torch.tensor(dp, dtype=torch.float32)\n",
    "\n",
    "        amp_grad_matrix_normalized = state.get_amp_grad_matrix()\n",
    "        if type(amp_grad_matrix_normalized) is torch.Tensor:\n",
    "            amp_grad_matrix_normalized = amp_grad_matrix_normalized.detach().numpy()\n",
    "        if self.dense:\n",
    "            # form the dense S matrix\n",
    "            S = np.mean([np.outer(amp_grad, amp_grad.conj()) for amp_grad in amp_grad_matrix_normalized.T], axis=0)\n",
    "            S -= amp_grad_matrix_normalized.mean(axis=1)@amp_grad_matrix_normalized.mean(axis=1).T\n",
    "            R = S + eta*np.eye(S.shape[0])\n",
    "            dp = scipy.linalg.solve(R, energy_grad)\n",
    "            return torch.tensor(dp, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "import torch_optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def compute_update_params(self, params, grad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__(learning_rate)\n",
    "    \n",
    "    def compute_update_params(self, params, grad):\n",
    "        return params - self.lr*grad\n",
    "\n",
    "class SignedSGD(Optimizer):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__(learning_rate)\n",
    "    \n",
    "    def compute_update_params(self, params, grad):\n",
    "        if type(grad) != torch.Tensor:\n",
    "            grad = torch.tensor(grad, dtype=torch.float32)\n",
    "        return params - self.lr*torch.sign(grad)\n",
    "\n",
    "class SignedRandomSGD(Optimizer):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super().__init__(learning_rate)\n",
    "    \n",
    "    def compute_update_params(self, params, grad):\n",
    "        return params - self.lr*torch.sign(grad)*torch.rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact ground state energy per site: -3.1891064523138195\n"
     ]
    }
   ],
   "source": [
    "from fermion_utils import *\n",
    "# import flax.linen as nn\n",
    "# from flax.core import FrozenDict\n",
    "# from flax import traverse_util\n",
    "import netket as nk\n",
    "import netket.experimental as nkx\n",
    "import netket.nn as nknn\n",
    "\n",
    "from math import pi\n",
    "\n",
    "from netket.experimental.operator.fermion import destroy as c\n",
    "from netket.experimental.operator.fermion import create as cdag\n",
    "from netket.experimental.operator.fermion import number as nc\n",
    "\n",
    "# Define the lattice shape\n",
    "L = 4  # Side of the square\n",
    "Lx = int(L)\n",
    "Ly = int(L/2)\n",
    "# graph = nk.graph.Square(L)\n",
    "graph = nk.graph.Grid([Lx,Ly], pbc=False)\n",
    "N = graph.n_nodes\n",
    "\n",
    "\n",
    "# Define the fermion filling and the Hilbert space\n",
    "N_f = int(Lx*Ly/2-1)\n",
    "hi = nkx.hilbert.SpinOrbitalFermions(N, s=None, n_fermions=N_f)\n",
    "\n",
    "\n",
    "# Define the Hubbard Hamiltonian\n",
    "t = 1.0\n",
    "V = 4.0\n",
    "H = 0.0\n",
    "for (i, j) in graph.edges(): # Definition of the Hubbard Hamiltonian\n",
    "    H -= t * (cdag(hi,i) * c(hi,j) + cdag(hi,j) * c(hi,i))\n",
    "    H += V * nc(hi,i) * nc(hi,j)\n",
    "\n",
    "\n",
    "# Exact diagonalization of the Hamiltonian for benchmark\n",
    "sp_h = H.to_sparse() # Convert the Hamiltonian to a sparse matrix\n",
    "from scipy.sparse.linalg import eigsh\n",
    "eig_vals, eig_vecs = eigsh(sp_h, k=2, which=\"SA\")\n",
    "E_gs = eig_vals[0]\n",
    "print(\"Exact ground state energy per site:\", E_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n=5, tau=0.3000, energy~-0.347505: 100%|##########| 5/5 [00:00<00:00, 23.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<samp style='font-size: 12px;'><details><summary><b style=\"color: #da64a1;\">fPEPS</b>(tensors=8, indices=18, Lx=4, Ly=2, max_bond=4)</summary><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #5ee1a8;\">b(0, 0)-(0, 1)</b>, <b style=\"color: #959bdf;\">b(0, 0)-(1, 0)</b>, <b style=\"color: #5fce5e;\">k0,0</b>], tags={<b style=\"color: #8b77de;\">I0,0</b>, <b style=\"color: #d44388;\">X0</b>, <b style=\"color: #b1d42e;\">Y0</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=U1FermionicArray(shape~(4, 4, 2):[+++], charge=0, num_blocks=5)</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #5ee1a8;\">b(0, 0)-(0, 1)</b>, <b style=\"color: #d1e38e;\">b(0, 1)-(1, 1)</b>, <b style=\"color: #de9a98;\">k0,1</b>], tags={<b style=\"color: #92d399;\">I0,1</b>, <b style=\"color: #d44388;\">X0</b>, <b style=\"color: #6c45d0;\">Y1</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=U1FermionicArray(shape~(4, 4, 2):[-++], charge=0, num_blocks=4)</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #959bdf;\">b(0, 0)-(1, 0)</b>, <b style=\"color: #a5e49d;\">b(1, 0)-(1, 1)</b>, <b style=\"color: #46d59d;\">b(1, 0)-(2, 0)</b>, <b style=\"color: #84ce75;\">k1,0</b>], tags={<b style=\"color: #c28de0;\">I1,0</b>, <b style=\"color: #6fe251;\">X1</b>, <b style=\"color: #b1d42e;\">Y0</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=...</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #d1e38e;\">b(0, 1)-(1, 1)</b>, <b style=\"color: #a5e49d;\">b(1, 0)-(1, 1)</b>, <b style=\"color: #ce7957;\">b(1, 1)-(2, 1)</b>, <b style=\"color: #d29586;\">k1,1</b>], tags={<b style=\"color: #d67b97;\">I1,1</b>, <b style=\"color: #6fe251;\">X1</b>, <b style=\"color: #6c45d0;\">Y1</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=...</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #46d59d;\">b(1, 0)-(2, 0)</b>, <b style=\"color: #cd2e7d;\">b(2, 0)-(2, 1)</b>, <b style=\"color: #db39c2;\">b(2, 0)-(3, 0)</b>, <b style=\"color: #aa87d0;\">k2,0</b>], tags={<b style=\"color: #d472d0;\">I2,0</b>, <b style=\"color: #c654d5;\">X2</b>, <b style=\"color: #b1d42e;\">Y0</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=...</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #ce7957;\">b(1, 1)-(2, 1)</b>, <b style=\"color: #cd2e7d;\">b(2, 0)-(2, 1)</b>, <b style=\"color: #83a9d6;\">b(2, 1)-(3, 1)</b>, <b style=\"color: #e1962d;\">k2,1</b>], tags={<b style=\"color: #77df7d;\">I2,1</b>, <b style=\"color: #c654d5;\">X2</b>, <b style=\"color: #6c45d0;\">Y1</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=...</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #db39c2;\">b(2, 0)-(3, 0)</b>, <b style=\"color: #8ede94;\">b(3, 0)-(3, 1)</b>, <b style=\"color: #42d58f;\">k3,0</b>], tags={<b style=\"color: #57ccd3;\">I3,0</b>, <b style=\"color: #e5db50;\">X3</b>, <b style=\"color: #b1d42e;\">Y0</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=U1FermionicArray(shape~(4, 4, 2):[-++], charge=1, num_blocks=5)</details></samp><samp style='font-size: 12px;'><details><summary><b style=\"color: #e55471;\">Tensor</b>(shape=(<b style=\"color: #80d749;\">4</b>, <b style=\"color: #80d749;\">4</b>, <b style=\"color: #828fdd;\">2</b>), inds=[<b style=\"color: #83a9d6;\">b(2, 1)-(3, 1)</b>, <b style=\"color: #8ede94;\">b(3, 0)-(3, 1)</b>, <b style=\"color: #cf3298;\">k3,1</b>], tags={<b style=\"color: #98d482;\">I3,1</b>, <b style=\"color: #e5db50;\">X3</b>, <b style=\"color: #6c45d0;\">Y1</b>}),</summary>backend=<b style=\"color: #5ee466;\">symmray</b>, dtype=<b style=\"color: #75d2c6;\">float64</b>, data=U1FermionicArray(shape~(4, 4, 2):[--+], charge=1, num_blocks=5)</details></samp></details></samp>"
      ],
      "text/plain": [
       "fPEPS(tensors=8, indices=18, Lx=4, Ly=2, max_bond=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SU in quimb\n",
    "Lx = int(L)\n",
    "Ly = int(L/2)\n",
    "D = 4\n",
    "seed = 2\n",
    "symmetry = 'U1'\n",
    "peps, parity_config = generate_random_fpeps(Lx, Ly, D, seed, symmetry, Nf=N_f)\n",
    "\n",
    "edges = qtn.edges_2d_square(Lx, Ly)\n",
    "site_info = sr.utils.parse_edges_to_site_info(\n",
    "    edges,\n",
    "    D,\n",
    "    phys_dim=2,\n",
    "    site_ind_id=\"k{},{}\",\n",
    "    site_tag_id=\"I{},{}\",\n",
    ")\n",
    "\n",
    "t = 1.0\n",
    "V = 4.0\n",
    "mu = 0.0\n",
    "\n",
    "terms = {\n",
    "    (sitea, siteb): sr.fermi_hubbard_spinless_local_array(\n",
    "        t=t, V=V, mu=mu,\n",
    "        symmetry=symmetry,\n",
    "        coordinations=(\n",
    "            site_info[sitea]['coordination'],\n",
    "            site_info[siteb]['coordination'],\n",
    "        ),\n",
    "    ).fuse((0, 1), (2, 3))\n",
    "    for (sitea, siteb) in peps.gen_bond_coos()\n",
    "}\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "su = qtn.SimpleUpdateGen(peps, ham, compute_energy_per_site=True,D=D, compute_energy_opts={\"max_distance\":2}, gate_opts={'cutoff':1e-12})\n",
    "\n",
    "# cluster energies may not be accuracte yet\n",
    "su.evolve(5, tau=0.3)\n",
    "# su.evolve(50, tau=0.1)\n",
    "# su.evolve(100, tau=0.03)\n",
    "# su.evolve(100, tau=0.01)\n",
    "# su.evolve(100, tau=0.003)\n",
    "\n",
    "peps = su.get_state()\n",
    "peps.equalize_norms_(value=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ham = qtn.LocalHam2D(Lx, Ly, terms)\n",
    "def convert_to_torch(blk_arr):\n",
    "    new_blks = { sector: torch.tensor(blk, dtype=torch.float32) for sector, blk in blk_arr.blocks.items() }\n",
    "    new_blk_arr = blk_arr.copy_with(blocks=new_blks)\n",
    "    return new_blk_arr\n",
    "\n",
    "ham.apply_to_arrays(lambda x: convert_to_torch(x))\n",
    "peps.apply_to_arrays(lambda x: torch.tensor(x, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Psi: tensor([ 4.4230e-04,  2.4416e-05, -1.3158e-03, -1.7969e-04,  1.4897e-03,\n",
      "         5.0495e-05, -1.7490e-04,  4.3562e-03,  4.7546e-04,  4.9765e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.3672, -0.4275, -0.0481,  0.3924,  0.1809, -0.1320, -0.2535, -0.3278,\n",
      "        -0.0508,  0.0270])\n",
      "{'mean': array(-2.760063, dtype=float32)}\n",
      "Energy: -2.7600629329681396\n",
      "Psi: tensor([ 3.7488e-04,  2.4436e-05, -1.2140e-03, -2.0670e-04,  1.3459e-03,\n",
      "         1.2477e-04, -1.8591e-04,  4.3067e-03,  5.8222e-04,  5.4728e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.2244, -0.3170,  0.0192,  0.3386,  0.1675, -0.0943, -0.1642, -0.2839,\n",
      "        -0.0350,  0.0244])\n",
      "{'mean': array(-2.888835, dtype=float32)}\n",
      "Energy: -2.8888354301452637\n",
      "Psi: tensor([ 3.1297e-04,  2.1228e-05, -1.1100e-03, -2.2095e-04,  1.1779e-03,\n",
      "         1.9866e-04, -1.7651e-04,  4.1155e-03,  6.6870e-04,  5.5198e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.1227, -0.2343,  0.0314,  0.3292,  0.1524, -0.1009, -0.1033, -0.2179,\n",
      "        -0.0170,  0.0231])\n",
      "{'mean': array(-2.987313, dtype=float32)}\n",
      "Energy: -2.9873125553131104\n",
      "Psi: tensor([ 2.5776e-04,  2.1813e-05, -1.0143e-03, -2.3286e-04,  1.0284e-03,\n",
      "         2.7462e-04, -1.3018e-04,  3.9360e-03,  7.4846e-04,  4.9349e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.0644, -0.1371,  0.0291,  0.2949,  0.1342, -0.0762, -0.0527, -0.1517,\n",
      "        -0.0030,  0.0209])\n",
      "{'mean': array(-3.058428, dtype=float32)}\n",
      "Energy: -3.0584282875061035\n",
      "Psi: tensor([ 2.0597e-04,  2.2182e-05, -9.0043e-04, -2.3943e-04,  9.0373e-04,\n",
      "         3.1584e-04, -1.4360e-04,  3.7886e-03,  8.4034e-04,  5.1992e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([ 0.0420, -0.0443,  0.0308,  0.2126,  0.1016, -0.0735,  0.0160, -0.1179,\n",
      "        -0.0035,  0.0210])\n",
      "{'mean': array(-3.104731, dtype=float32)}\n",
      "Energy: -3.1047306060791016\n",
      "Psi: tensor([ 1.7226e-04,  2.6382e-05, -8.4921e-04, -2.5569e-04,  8.1735e-04,\n",
      "         3.9079e-04, -9.0312e-05,  3.7436e-03,  9.3404e-04,  4.6392e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.0232, -0.0343, -0.0021,  0.1867,  0.0840, -0.0370, -0.0490, -0.0481,\n",
      "        -0.0027,  0.0061])\n",
      "{'mean': array(-3.131014, dtype=float32)}\n",
      "Energy: -3.131014347076416\n",
      "Psi: tensor([ 1.2290e-04,  2.8573e-05, -7.5155e-04, -2.5220e-04,  7.1128e-04,\n",
      "         4.3764e-04, -7.3174e-05,  3.6507e-03,  9.8731e-04,  5.1216e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([ 0.1494,  0.0677,  0.0004,  0.0828,  0.0420, -0.0694,  0.0466, -0.0581,\n",
      "        -0.0032,  0.0145])\n",
      "{'mean': array(-3.142883, dtype=float32)}\n",
      "Energy: -3.142882823944092\n",
      "Psi: tensor([ 1.2070e-04,  2.6059e-05, -7.2016e-04, -2.3290e-04,  8.1181e-04,\n",
      "         4.5058e-04, -1.7691e-05,  3.7775e-03,  9.2315e-04,  4.8571e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.0799, -0.0357, -0.0184,  0.0371,  0.0266,  0.0139, -0.0168,  0.0169,\n",
      "        -0.0006, -0.0051])\n",
      "{'mean': array(-3.152169, dtype=float32)}\n",
      "Energy: -3.1521687507629395\n",
      "Psi: tensor([ 7.5219e-05,  3.1648e-05, -6.4954e-04, -2.4813e-04,  7.0154e-04,\n",
      "         4.8294e-04, -9.0085e-06,  3.7707e-03,  1.0533e-03,  5.5247e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([ 0.1466,  0.0683,  0.0132, -0.0427, -0.0127, -0.0317,  0.0422, -0.0452,\n",
      "         0.0042,  0.0125])\n",
      "{'mean': array(-3.154384, dtype=float32)}\n",
      "Energy: -3.154384136199951\n",
      "Psi: tensor([ 7.7077e-05,  2.8587e-05, -6.1774e-04, -2.2760e-04,  7.9931e-04,\n",
      "         4.6023e-04,  7.7037e-06,  3.8970e-03,  9.7810e-04,  5.1104e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.0805, -0.0456, -0.0048,  0.0371,  0.0211,  0.0153, -0.0445,  0.0248,\n",
      "        -0.0104, -0.0096])\n",
      "{'mean': array(-3.158983, dtype=float32)}\n",
      "Energy: -3.1589834690093994\n",
      "Psi: tensor([ 4.1688e-05,  2.9260e-05, -5.4532e-04, -2.2385e-04,  7.0828e-04,\n",
      "         4.7457e-04, -1.7878e-05,  3.7776e-03,  1.0241e-03,  6.0248e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([ 0.1374,  0.0721,  0.0047, -0.0457, -0.0163, -0.0321,  0.0471, -0.0415,\n",
      "        -0.0025,  0.0105])\n",
      "{'mean': array(-3.159198, dtype=float32)}\n",
      "Energy: -3.1591978073120117\n",
      "Psi: tensor([ 6.3588e-05,  2.7413e-05, -5.1587e-04, -2.1221e-04,  8.0936e-04,\n",
      "         4.5156e-04, -8.6015e-07,  3.9090e-03,  9.8228e-04,  5.1057e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([-0.0868, -0.0447, -0.0064,  0.0333,  0.0192,  0.0178, -0.0404,  0.0297,\n",
      "        -0.0099, -0.0109])\n",
      "{'mean': array(-3.16231, dtype=float32)}\n",
      "Energy: -3.1623096466064453\n",
      "Psi: tensor([ 4.4208e-05,  3.0374e-05, -4.5858e-04, -2.0673e-04,  7.0813e-04,\n",
      "         4.7298e-04, -1.7082e-05,  3.7825e-03,  1.0159e-03,  6.0424e-04],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Energy gradient vec: tensor([ 0.1404,  0.0758,  0.0034, -0.0483, -0.0181, -0.0331,  0.0506, -0.0395,\n",
      "        -0.0021,  0.0107])\n",
      "{'mean': array(-3.161542, dtype=float32)}\n",
      "Energy: -3.1615419387817383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m preconditioner \u001b[38;5;241m=\u001b[39m TrivialPreconditioner()\n\u001b[1;32m     10\u001b[0m vmc \u001b[38;5;241m=\u001b[39m VMC(H, variational_state, optimizer, preconditioner)\n\u001b[0;32m---> 12\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mvmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# import torch_optimizer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# optimizer = torch_optimizer.AdaBelief(model.parameters(), lr=0.01)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# type(H.get_conn(test_sample)[0][0])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# sample0 = sampler.sample(H, variational_state, chain_length=4)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 110\u001b[0m, in \u001b[0;36mVMC.run\u001b[0;34m(self, start, stop, tmpdir)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m step\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Compute the average energy and estimated energy gradient, meanwhile also record the amplitude_grad matrix\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Use MPI for the sampling\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m state_MC_energy, state_MC_energy_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpect_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hamiltonian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Only rank 0 collects the energy statistics\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[56], line 111\u001b[0m, in \u001b[0;36mVariational_State.expect_and_grad\u001b[0;34m(self, op, full_hi)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03mCompute the expectation value of the operator `op` and its gradient.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    torch.tensor: The gradient of the expectation value with respect to the parameters.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_hi \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_hi_expect_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# use MPI for sampling\u001b[39;00m\n\u001b[1;32m    114\u001b[0m chain_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mSIZE \u001b[38;5;66;03m# Number of samples per rank\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[56], line 84\u001b[0m, in \u001b[0;36mVariational_State.full_hi_expect_and_grad\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m     82\u001b[0m N \u001b[38;5;241m=\u001b[39m hi\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     83\u001b[0m all_config \u001b[38;5;241m=\u001b[39m hi\u001b[38;5;241m.\u001b[39mall_states()\n\u001b[0;32m---> 84\u001b[0m psi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPsi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpsi[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m psi \u001b[38;5;241m=\u001b[39m psi\u001b[38;5;241m/\u001b[39mdo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinalg.norm\u001b[39m\u001b[38;5;124m'\u001b[39m, psi)\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 171\u001b[0m, in \u001b[0;36mfTNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# If input is not batched, add a batch dimension\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamplitude\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 161\u001b[0m, in \u001b[0;36mfTNModel.amplitude\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m batch_amps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_i \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 161\u001b[0m     amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_amp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpsi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymmetry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     batch_amps\u001b[38;5;241m.\u001b[39mappend(amp\u001b[38;5;241m.\u001b[39mcontract())\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Return the batch of amplitudes stacked as a tensor\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mfTNModel.get_amp\u001b[0;34m(self, peps, config, inplace, symmetry, conj)\u001b[0m\n\u001b[1;32m     79\u001b[0m     peps \u001b[38;5;241m=\u001b[39m peps\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conj:\n\u001b[0;32m---> 81\u001b[0m     amp \u001b[38;5;241m=\u001b[39m peps\u001b[38;5;241m|\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproduct_bra_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetry\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     amp \u001b[38;5;241m=\u001b[39m peps\u001b[38;5;241m|\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduct_bra_state(config, peps, symmetry)\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/quimb/tensor/tensor_core.py:4182\u001b[0m, in \u001b[0;36mTensorNetwork.conj\u001b[0;34m(self, mangle_inner, inplace)\u001b[0m\n\u001b[1;32m   4179\u001b[0m tn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   4181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tn:\n\u001b[0;32m-> 4182\u001b[0m     \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mangle_inner:\n\u001b[1;32m   4185\u001b[0m     append \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m mangle_inner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(mangle_inner)\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/quimb/tensor/tensor_core.py:1868\u001b[0m, in \u001b[0;36mTensor.conj\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Conjugate this tensors data (does nothing to indices).\"\"\"\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 1868\u001b[0m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_inds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_inds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/quimb/tensor/tensor_core.py:1545\u001b[0m, in \u001b[0;36mTensor.modify\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_left_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m-> 1545\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapply\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_left_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minds\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/quimb/tensor/tensor_core.py:1521\u001b[0m, in \u001b[0;36mTensor._apply_function\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m-> 1521\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_data(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/autoray/autoray.py:1051\u001b[0m, in \u001b[0;36mconj\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconj\u001b[39m(x):\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Array conjugate.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconj\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/symmray_nqs/lib/python3.9/site-packages/autoray/autoray.py:81\u001b[0m, in \u001b[0;36mdo\u001b[0;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m _choose_backend(fn, args, kwargs, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[1;32m     80\u001b[0m func \u001b[38;5;241m=\u001b[39m get_lib_fn(backend, fn)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NQS/Fermion/new_quimb/symmray/symmray/interface.py:10\u001b[0m, in \u001b[0;36mconj\u001b[0;34m(x, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconj\u001b[39m(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Conjugate a `symmray` array.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NQS/Fermion/new_quimb/symmray/symmray/fermionic_core.py:441\u001b[0m, in \u001b[0;36mFermionicArray.conj\u001b[0;34m(self, phase_permutation, phase_dual, inplace)\u001b[0m\n\u001b[1;32m    437\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    439\u001b[0m _conj \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mget_lib_fn(new\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconj\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m new\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m axs_conj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    443\u001b[0m     ax \u001b[38;5;28;01mfor\u001b[39;00m ax, ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new\u001b[38;5;241m.\u001b[39m_indices) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ix\u001b[38;5;241m.\u001b[39mdual\n\u001b[1;32m    444\u001b[0m )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sector, array \u001b[38;5;129;01min\u001b[39;00m new\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# conjugate the actual array\u001b[39;00m\n",
      "File \u001b[0;32m~/NQS/Fermion/new_quimb/symmray/symmray/fermionic_core.py:441\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    437\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    439\u001b[0m _conj \u001b[38;5;241m=\u001b[39m ar\u001b[38;5;241m.\u001b[39mget_lib_fn(new\u001b[38;5;241m.\u001b[39mbackend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconj\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m new\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ix \u001b[38;5;129;01min\u001b[39;00m new\u001b[38;5;241m.\u001b[39mindices)\n\u001b[1;32m    442\u001b[0m axs_conj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    443\u001b[0m     ax \u001b[38;5;28;01mfor\u001b[39;00m ax, ix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new\u001b[38;5;241m.\u001b[39m_indices) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ix\u001b[38;5;241m.\u001b[39mdual\n\u001b[1;32m    444\u001b[0m )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sector, array \u001b[38;5;129;01min\u001b[39;00m new\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# conjugate the actual array\u001b[39;00m\n",
      "File \u001b[0;32m~/NQS/Fermion/new_quimb/symmray/symmray/abelian_core.py:114\u001b[0m, in \u001b[0;36mBlockIndex.conj\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconj\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A copy of this index with the dualness reversed.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     new\u001b[38;5;241m.\u001b[39m_chargemap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chargemap\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    116\u001b[0m     new\u001b[38;5;241m.\u001b[39m_dual \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dual\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = fTNModel(peps)\n",
    "\n",
    "optimizer = SignedSGD(learning_rate=5e-3)\n",
    "sampler = None\n",
    "# sampler = MetropolisExchangeSampler(hi, graph, N_samples=64, burn_in_steps=10)\n",
    "variational_state = Variational_State(model, hi=H.hilbert, sampler=sampler)\n",
    "# preconditioner = SR(dense=True, exact=True if sampler is None else False)\n",
    "preconditioner = TrivialPreconditioner()\n",
    "\n",
    "vmc = VMC(H, variational_state, optimizer, preconditioner)\n",
    "\n",
    "stats = vmc.run(0, 15)\n",
    "\n",
    "# import torch_optimizer\n",
    "\n",
    "# optimizer = torch_optimizer.AdaBelief(model.parameters(), lr=0.01)\n",
    "\n",
    "# test_sample = sampler._sample_next(variational_state)\n",
    "# type(H.get_conn(test_sample)[0][0])\n",
    "# sample0 = sampler.sample(H, variational_state, chain_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([stat['mean']/graph.n_nodes for stat in stats])\n",
    "# plot exact gs energy\n",
    "plt.axhline(y=E_gs/N, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK0, sample size: 20, chain length: 20\n",
      "Total number of samples: 20\n"
     ]
    }
   ],
   "source": [
    "grad_vec = variational_state.expect_and_grad(H)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.000418 -0.000124  0.000083 -0.000016  0.000011 -0.000373 -0.000299  0.000049 -0.000012\n",
      "  0.000023  0.000498 -0.000179 -0.000014 -0.000297 -0.000041  0.000045 -0.000005  0.000217\n",
      " -0.000016  0.000028 -0.000039 -0.000005  0.000305  0.000043 -0.000018 -0.000021 -0.000011\n",
      " -0.000013  0.000007 -0.000003  0.000002 -0.000001 -0.000855  0.000004 -0.000253  0.000077\n",
      " -0.000275  0.000023 -0.000154  0.000036 -0.000234  0.000031  0.000153  0.000012  0.000388\n",
      "  0.000075 -0.000156 -0.000026 -0.000032 -0.000037  0.000006  0.000008  0.000023 -0.000011\n",
      "  0.000014 -0.000007  0.000299 -0.00005   0.000008  0.000004 -0.000017  0.000004  0.000029\n",
      " -0.000004  0.000016 -0.000002 -0.000912  0.00036   0.000027 -0.000018 -0.000261  0.000009\n",
      " -0.000216  0.000049 -0.000034  0.000004 -0.000019  0.000002  0.000106 -0.000006  0.000003\n",
      " -0.        0.       -0.000013  0.000003 -0.000047  0.000056 -0.000012 -0.00054   0.000082\n",
      "  0.000272 -0.00004   0.000042  0.000002  0.        0.000002  0.000001  0.        0.\n",
      " -0.000323  0.000051  0.000612  0.000317 -0.000212 -0.000147 -0.000164 -0.000038  0.000044\n",
      "  0.000021  0.000026  0.000015  0.000012  0.000007  0.000008 -0.000004 -0.000039  0.000005\n",
      " -0.000152 -0.000082  0.000391 -0.000172  0.000144 -0.000051  0.00006  -0.000034  0.000008\n",
      "  0.000005 -0.00001  -0.000006  0.00001  -0.000012 -0.000009  0.000014 -0.000232 -0.000061\n",
      " -0.000018 -0.000001  0.       -0.000057  0.000033  0.000058  0.000011  0.000554 -0.000278\n",
      "  0.000084 -0.000041 -0.001234 -0.000187 -0.000041 -0.000203 -0.0002   -0.000047  0.00004\n",
      " -0.000053 -0.000034 -0.000019 -0.000004 -0.000002 -0.00001  -0.000004 -0.000049  0.000017\n",
      " -0.000004  0.000029  0.000017 -0.000004 -0.000002  0.000003 -0.        0.        0.000419\n",
      "  0.000124  0.000327  0.000214 -0.000111 -0.000085 -0.000069 -0.000016  0.000011 -0.000002\n",
      "  0.000202  0.000013  0.000002 -0.00055   0.000117  0.000526  0.       -0.000026 -0.000009\n",
      "  0.000011]\n"
     ]
    }
   ],
   "source": [
    "# show more precision\n",
    "np.set_printoptions(suppress=True,precision=6,linewidth=100)\n",
    "print(grad_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "symmray_nqs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
